{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca3b5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 20 13:38:19 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0  On |                  N/A |\n",
      "| 30%   40C    P0             46W /  350W |     157MiB /  24576MiB |    100%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     32303      G   /usr/lib/xorg/Xorg                             72MiB |\n",
      "|    0   N/A  N/A     32451      G   /usr/bin/gnome-shell                           72MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7088e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-20 13:38:21 [__init__.py:216] Automatically detected platform cuda.\n",
      "Successfully imported vllm version: 0.10.2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from vllm.distributed.parallel_state import (\n",
    "        destroy_model_parallel,\n",
    "        destroy_distributed_environment,\n",
    "    )\n",
    "    import vllm\n",
    "#    !export VLLM_LOGGING_LEVEL=ERROR\n",
    "\n",
    "    print(f\"Successfully imported vllm version: {vllm.__version__}\")\n",
    "    USE_VLLM = True\n",
    "except ImportError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    USE_VLLM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46155e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.8.0+cu128\n",
      "Using TRL version: 0.23.0\n",
      "Using bitsandbytes version: 0.47.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import trl\n",
    "import bitsandbytes\n",
    "\n",
    "print(f\"Using PyTorch version: {torch.__version__}\")\n",
    "print(f\"Using TRL version: {trl.__version__}\")\n",
    "print(f\"Using bitsandbytes version: {bitsandbytes.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82ddb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import warnings\n",
    "import contextlib\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, Dataset\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOConfig, GRPOTrainer, SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c27e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import userdata\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    # Access the secret\n",
    "    # Make sure you have created a secret named 'HUGGING_FACE_HUB_TOKEN' in your Colab secrets\n",
    "    hf_token = userdata.get(\"HF_TOKEN\")\n",
    "\n",
    "    # Log in to Hugging Face\n",
    "    # The 'token' parameter is used for non-interactive login\n",
    "    login(token=hf_token)\n",
    "\n",
    "    print(\"Hugging Face login successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ada8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silence_annoying_logs():\n",
    "    \"\"\"\n",
    "    Silences INFO and WARNING messages from specific libraries.\n",
    "    This is a robust method that handles placeholder loggers.\n",
    "    \"\"\"\n",
    "    # List of logger prefixes to silence\n",
    "    loggers_to_silence = [\"vllm\", \"numba\"]  # Added numba as it can also be noisy\n",
    "\n",
    "    for name, logger in logging.root.manager.loggerDict.items():\n",
    "        # Check if the logger name starts with any of the prefixes\n",
    "        if any(name.startswith(prefix) for prefix in loggers_to_silence):\n",
    "            # IMPORTANT: Check if it's a real logger and not a placeholder\n",
    "            if isinstance(logger, logging.Logger):\n",
    "                logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Also, let's suppress the UserWarning from numba if it appears\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"numba\")\n",
    "\n",
    "\n",
    "# Call the function to apply the changes\n",
    "silence_annoying_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b2399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System Prompts and Formatting ---\n",
    "\n",
    "R1_STYLE_SYSTEM_PROMPT = \"\"\"You are a financial analyst AI and you need to analyze the sentiment in the piece of news reported in square brackets. First, think in <reasoning> tags. Then, provide your answer in <answer> tags.\"\"\"\n",
    "TASK_SPECIFIC_INSTRUCTIONS = \"\"\"The answer must be a single JSON object with two keys: `sentiment` (\"positive\", \"negative\", or \"neutral\") and `explanation` (a brief justification under 50 words).\"\"\"\n",
    "\n",
    "EXAMPLE = \"\"\"<reasoning>\n",
    "</reasoning>\n",
    "<answer>\n",
    "```json\n",
    "{\n",
    "  \"sentiment\": \"...\",\n",
    "  \"explanation\": \"...\"\n",
    "}\n",
    "</answer>\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\\n\n",
    "{reasoning}\\n\n",
    "</reasoning>\\n\n",
    "<answer>\\n\n",
    "{answer}\\n\n",
    "</answer>\\n\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    R1_STYLE_SYSTEM_PROMPT\n",
    "    + \"\\n\\n\"\n",
    "    + TASK_SPECIFIC_INSTRUCTIONS\n",
    "    + \"\\n\\n\"\n",
    "    + \"Follow this structure:\\n\"\n",
    "    + EXAMPLE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ae213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration Class ---\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters\"\"\"\n",
    "\n",
    "    SIZE = \"3-270m\"\n",
    "    #SIZE = \"3-1b\"\n",
    "    MODEL_NAME = f\"google/gemma-{SIZE}-it\"\n",
    "    OUTPUT_MODEL = f\"gemma-{SIZE}-it-grpo-finsent-tags\"\n",
    "\n",
    "    max_prompt_length = 1024\n",
    "    max_completion_length = 1024\n",
    "\n",
    "\n",
    "# --- Environment and Utility Functions ---\n",
    "\n",
    "\n",
    "def init():\n",
    "    \"\"\"Initialization script\"\"\"\n",
    "    os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "    # It is recommended to set the HF_TOKEN as an environment variable\n",
    "    token = os.environ.get(\"HF_TOKEN\")\n",
    "    if token:\n",
    "        login(token=token)\n",
    "    else:\n",
    "        print(\"HF_TOKEN not set. You might need to log in manually.\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def close(llm=None):\n",
    "    \"\"\"Close vllm and clean up resources\"\"\"\n",
    "    destroy_model_parallel()\n",
    "    destroy_distributed_environment()\n",
    "    if llm:\n",
    "        del llm.llm_engine.model_executor\n",
    "        del llm\n",
    "    with contextlib.suppress(AssertionError):\n",
    "        torch.distributed.destroy_process_group()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def is_bfloat16_supported():\n",
    "    \"\"\"Checks if the current device supports bfloat16.\"\"\"\n",
    "    return torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "\n",
    "def info_device():\n",
    "    \"\"\"Get device for PyTorch\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f50979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "init()\n",
    "params = Config()\n",
    "device = info_device()\n",
    "dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb42bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fpb_questions(split=\"train\"):\n",
    "    \"\"\"Upload FinancialPhraseBank dataset\"\"\"\n",
    "    repo_id = \"lmassaron/FinancialPhraseBank\"\n",
    "    data = load_dataset(repo_id, cache_dir=\"/tmp\")[split]\n",
    "\n",
    "    data = data.map(\n",
    "        lambda x: {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"user\", \"content\": SYSTEM_PROMPT + \"\\n[\" + x[\"sentence\"]+\"]\"},\n",
    "            ],\n",
    "            \"answer\": x[\"sentiment\"],\n",
    "        }\n",
    "    )\n",
    "    data = data.remove_columns([\"sentiment\", \"sentence\", \"label\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24edcd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PROMPT ---\n",
      "You are a financial analyst AI and you need to analyze the sentiment in the piece of news reported in square brackets. First, think in <reasoning> tags. Then, provide your answer in <answer> tags.\n",
      "\n",
      "The answer must be a single JSON object with two keys: `sentiment` (\"positive\", \"negative\", or \"neutral\") and `explanation` (a brief justification under 50 words).\n",
      "\n",
      "Follow this structure:\n",
      "<reasoning>\n",
      "</reasoning>\n",
      "<answer>\n",
      "```json\n",
      "{\n",
      "  \"sentiment\": \"...\",\n",
      "  \"explanation\": \"...\"\n",
      "}\n",
      "</answer>\n",
      "[The new agreement , which expands a long-established cooperation between the companies , involves the transfer of certain engineering and documentation functions from Larox to Etteplan .]\n",
      "\n",
      "--- GROUND TRUTH ANSWER ---\n",
      "positive\n",
      "==============================\n",
      "--- PROMPT ---\n",
      "You are a financial analyst AI and you need to analyze the sentiment in the piece of news reported in square brackets. First, think in <reasoning> tags. Then, provide your answer in <answer> tags.\n",
      "\n",
      "The answer must be a single JSON object with two keys: `sentiment` (\"positive\", \"negative\", or \"neutral\") and `explanation` (a brief justification under 50 words).\n",
      "\n",
      "Follow this structure:\n",
      "<reasoning>\n",
      "</reasoning>\n",
      "<answer>\n",
      "```json\n",
      "{\n",
      "  \"sentiment\": \"...\",\n",
      "  \"explanation\": \"...\"\n",
      "}\n",
      "</answer>\n",
      "[( ADP News ) - Finnish handling systems provider Cargotec Oyj ( HEL : CGCBV ) announced on Friday it won orders worth EUR 10 million ( USD 13.2 m ) to deliver linkspans to Jordan , Morocco and Ireland .]\n",
      "\n",
      "--- GROUND TRUTH ANSWER ---\n",
      "positive\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Let's load a small sample of the GSM8K data to see its structure.\n",
    "fpb_sample = get_fpb_questions(split=\"test\").select(range(2))\n",
    "\n",
    "for response in fpb_sample:\n",
    "    print(\"--- PROMPT ---\")\n",
    "    print(response[\"prompt\"][0][\"content\"])\n",
    "    print(\"\\n--- GROUND TRUTH ANSWER ---\")\n",
    "    print(response[\"answer\"])\n",
    "    print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a9e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text: str, start_tag=\"<answer>\", end_tag=\"</answer>\") -> dict | None:\n",
    "    \"\"\"\n",
    "    Extracts the content from the last occurrence of <answer> tags, cleans it,\n",
    "    \"\"\"\n",
    "    pattern = re.escape(start_tag) + r\"(.*?)\" + re.escape(end_tag)\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "\n",
    "def extract_last_json_from_answer(text: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Tries to extract the last JSON object using a simple greedy regex.\n",
    "    This is unreliable for nested JSON.\n",
    "    \"\"\"\n",
    "    matches = re.findall(r\"\\{[\\s\\S]*?\\}\", text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(matches[-1])\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "def extract_reasoning(\n",
    "    text: str, start_tag=\"<reasoning>\", end_tag=\"</reasoning>\"\n",
    ") -> str:\n",
    "    \"\"\"Extracts the content within the <reasoning> tags.\"\"\"\n",
    "    pattern = re.escape(start_tag) + r\"(.*?)\" + re.escape(end_tag)\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "180a0dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Level 1: Tag Structure ---\n",
    "# Harshest penalty for failing the most basic format requirement.\n",
    "PENALTY_MISSING_TAGS = -2.0\n",
    "# Small reward for just getting the tags right.\n",
    "REWARD_CORRECT_TAGS = 0.2\n",
    "\n",
    "# --- Level 2: JSON Validity ---\n",
    "# A significant penalty for malformed JSON inside the <answer> tag.\n",
    "PENALTY_INVALID_JSON = -1.5\n",
    "# A small reward for valid JSON.\n",
    "REWARD_VALID_JSON = 0.3\n",
    "\n",
    "# --- Level 3: Core Task Correctness ---\n",
    "# The main objective reward.\n",
    "REWARD_SENTIMENT_CORRECT = 1.0\n",
    "PENALTY_SENTIMENT_WRONG = -1.0\n",
    "REWARD_SENTIMENT_NEAR = 0.2  # Encouragement for being close\n",
    "\n",
    "# --- Level 4: Quality/Style ---\n",
    "# A bonus for high-quality reasoning.\n",
    "REWARD_REASONING_QUALITY = 0.8\n",
    "MIN_REASONING_LEN = 20\n",
    "MAX_REASONING_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "301bef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(completion):\n",
    "    if not completion:\n",
    "        return []\n",
    "    if isinstance(completion[0], list):\n",
    "        return [item for sublist in completion for item in sublist]\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb308629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_validity_func(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Checks if the content within the <answer> tag is a valid JSON\n",
    "    with the required keys.\n",
    "    \"\"\"\n",
    "    output = flatten(completions)\n",
    "    rewards = []\n",
    "    for response in output:\n",
    "        answer_content = extract_answer(response[\"content\"])\n",
    "        if (\n",
    "            not answer_content\n",
    "        ):  # If <answer> tag is missing, this function gives no reward/penalty\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        parsed_json = extract_last_json_from_answer(answer_content)\n",
    "        if (\n",
    "            parsed_json != \"\"\n",
    "            and \"sentiment\" in parsed_json\n",
    "            and \"explanation\" in parsed_json\n",
    "        ):\n",
    "            rewards.append(REWARD_VALID_JSON)\n",
    "        else:\n",
    "            rewards.append(PENALTY_INVALID_JSON)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45b3820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Checks if the output contains the required <reasoning> and <answer> tags.\n",
    "    This is the primary structural gate.\n",
    "    \"\"\"\n",
    "    output = flatten(completions)\n",
    "    rewards = []\n",
    "    for response in output:\n",
    "        reasoning_text = extract_reasoning(response[\"content\"])\n",
    "        answer_text = extract_answer(response[\"content\"])\n",
    "\n",
    "        if reasoning_text != \"\" and answer_text != \"\":\n",
    "            rewards.append(REWARD_CORRECT_TAGS)\n",
    "        else:\n",
    "            rewards.append(PENALTY_MISSING_TAGS)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoning_quality_func(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Rewards the model for generating a high-quality explanation within\n",
    "    the <reasoning> tags.\n",
    "    \"\"\"\n",
    "    output = flatten(completions)\n",
    "    rewards = []\n",
    "    for response in output:\n",
    "        reasoning_text = extract_reasoning(response[\"content\"])\n",
    "        if reasoning_text:\n",
    "            exp_len = len(reasoning_text.split())  # Count words\n",
    "            if exp_len >= MIN_REASONING_LEN and exp_len <= MAX_REASONING_LEN:\n",
    "                rewards.append(REWARD_REASONING_QUALITY)\n",
    "            elif exp_len > 0 and exp_len < MIN_REASONING_LEN:\n",
    "                rewards.append(REWARD_REASONING_QUALITY * (exp_len / MIN_REASONING_LEN))\n",
    "            else:\n",
    "                rewards.append(0.0)  # Too short or too long\n",
    "        else:\n",
    "            rewards.append(0.0)  # No penalty here, as format_reward_func handles it\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def explanation_quality_func(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Rewards the model for generating a high-quality explanation within the\n",
    "    \"explanation\" key of the JSON found in the <answer> tag.\n",
    "    \"\"\"\n",
    "    output = flatten(completions)\n",
    "    rewards = []\n",
    "    for response in output:\n",
    "        # Step 1: Extract the content from within the <answer> tag.\n",
    "        answer_content = extract_answer(response[\"content\"])\n",
    "\n",
    "        # If there's no <answer> tag, we can't evaluate the explanation. Reward is 0.\n",
    "        if not answer_content:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Step 2: Parse the JSON from the answer content.\n",
    "        parsed_json = extract_last_json_from_answer(answer_content)\n",
    "\n",
    "        # Step 3: Check if the JSON is valid and contains the \"explanation\" key.\n",
    "        if parsed_json and \"explanation\" in parsed_json:\n",
    "            explanation_text = parsed_json[\"explanation\"]\n",
    "\n",
    "            # Ensure the explanation is a string before processing\n",
    "            if not isinstance(explanation_text, str):\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "\n",
    "            exp_len = len(explanation_text.split())  # Count words\n",
    "\n",
    "            # Step 4: Apply the same length-based reward logic as before.\n",
    "            if exp_len >= MIN_REASONING_LEN and exp_len <= MAX_REASONING_LEN:\n",
    "                # Full reward for an explanation in the ideal length range.\n",
    "                rewards.append(REWARD_REASONING_QUALITY)\n",
    "            elif exp_len > 0 and exp_len < MIN_REASONING_LEN:\n",
    "                # Scaled reward for trying but being too short. This provides a learning gradient.\n",
    "                rewards.append(REWARD_REASONING_QUALITY * (exp_len / MIN_REASONING_LEN))\n",
    "            else:\n",
    "                # No reward if the explanation is empty or too long.\n",
    "                rewards.append(0.0)\n",
    "        else:\n",
    "            # If JSON is invalid or the \"explanation\" key is missing, give no quality reward.\n",
    "            # The other reward functions will handle penalizing the bad format.\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50c1d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sentiment_func(completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Checks for the correct sentiment, but only after parsing the JSON\n",
    "    from within the <answer> tag.\n",
    "    \"\"\"\n",
    "    sentiment_scores = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "    output = flatten(completions)\n",
    "    rewards = []\n",
    "    for response, actual_sentiment in zip(output, answer):\n",
    "        answer_content = extract_answer(response[\"content\"])\n",
    "        if not answer_content:\n",
    "            rewards.append(PENALTY_SENTIMENT_WRONG)  # No answer tag is a wrong answer\n",
    "            continue\n",
    "\n",
    "        parsed_json = extract_last_json_from_answer(answer_content)\n",
    "        if parsed_json and \"sentiment\" in parsed_json:\n",
    "            predicted_sentiment = str(parsed_json[\"sentiment\"]).lower()\n",
    "            if predicted_sentiment in sentiment_scores:\n",
    "                pred_score = sentiment_scores[predicted_sentiment]\n",
    "                actual_score = sentiment_scores[actual_sentiment]\n",
    "                distance = abs(pred_score - actual_score)\n",
    "\n",
    "                if distance == 0:\n",
    "                    rewards.append(REWARD_SENTIMENT_CORRECT)\n",
    "                elif distance == 1:\n",
    "                    rewards.append(REWARD_SENTIMENT_NEAR)\n",
    "                else:  # distance == 2\n",
    "                    rewards.append(PENALTY_SENTIMENT_WRONG)\n",
    "            else:  # Invalid sentiment value\n",
    "                rewards.append(PENALTY_SENTIMENT_WRONG)\n",
    "        else:  # Malformed or missing JSON\n",
    "            rewards.append(PENALTY_SENTIMENT_WRONG)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91c038f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward_function(model, tokenizer, test_data, num_test_samples, device, temperature=1.0, seed=0):\n",
    "    random.seed(seed)\n",
    "    random_indices = random.sample(range(len(test_data)), num_test_samples)\n",
    "    accuracy = []\n",
    "    scored_reward = []\n",
    "    reward_dict = {\n",
    "        \"format_reward_func\": [],\n",
    "        \"json_validity_func\": [],\n",
    "        \"correct_sentiment_func\": [],\n",
    "        \"reasoning_quality_func\": [],\n",
    "    }\n",
    "    for i in tqdm(random_indices):\n",
    "        sample = test_data.select(range(i, i + 1))[0]\n",
    "        full_prompt = sample[\"prompt\"][0][\"content\"]\n",
    "        ground_truth_answer = sample[\"answer\"]\n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # print(\n",
    "        #    f\"\\n==================== Sample {i+1}/{len(test_data)} ====================\"\n",
    "        # )\n",
    "        # print(\n",
    "        #    f\"\\n>>> Input text:\\n{full_prompt[len(SYSTEM_PROMPT):]}...\"\n",
    "        # )\n",
    "        # print(f\"\\n>>> Ground Truth Answer:\\n{json.dumps(ground_truth_answer, indent=2)}\")\n",
    "\n",
    "        # Generate a completion from the base model\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,  # Give it enough space to generate something\n",
    "            temperature=temperature,\n",
    "            do_sample=True if temperature > 0 else False,\n",
    "        )\n",
    "        # Decode only the newly generated tokens\n",
    "        generated_text = tokenizer.decode(\n",
    "            outputs[0, inputs.input_ids.shape[-1] :], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # print(f\"\\n>>> Untrained Model's Raw Output:\\n{generated_text}\")\n",
    "        if ground_truth_answer in generated_text.lower():\n",
    "            accuracy.append(1)\n",
    "        else:\n",
    "            accuracy.append(0)\n",
    "\n",
    "        # --- 4. Calculate Reward ---\n",
    "        # The reward function expects data in a specific batch format, so we wrap our single sample\n",
    "        completions_batch = [[{\"content\": generated_text}]]\n",
    "        ground_truth_batch = [ground_truth_answer]\n",
    "        rewards = [\n",
    "            format_reward_func(completions_batch),\n",
    "            json_validity_func(completions_batch),\n",
    "            correct_sentiment_func(completions_batch, ground_truth_batch),\n",
    "            reasoning_quality_func(completions_batch),\n",
    "        ]\n",
    "        reward_dict[\"format_reward_func\"].append(rewards[0])\n",
    "        reward_dict[\"json_validity_func\"].append(rewards[1])\n",
    "        reward_dict[\"correct_sentiment_func\"].append(rewards[2])\n",
    "        reward_dict[\"reasoning_quality_func\"].append(rewards[3])\n",
    "\n",
    "        calculated_reward = sum([sum(reward) for reward in rewards])  # Get the reward for our single sample\n",
    "        scored_reward.append(calculated_reward)\n",
    "\n",
    "        # print(f\"\\n>>> Calculated Reward: {calculated_reward:.2f}\")\n",
    "        # print(\"========================================================\\n\")\n",
    "\n",
    "    print(f\"\\n>>> Sentiment prediction accuracy: {np.mean(accuracy):0.2f}\")\n",
    "    print(f\">>> Mean scored reward: {np.mean(scored_reward):0.3f}\")\n",
    "    for scorer, scores in reward_dict.items():\n",
    "        print(f\"\\t> {scorer} : {np.mean(scores):0.3f} ({np.mean(np.array(scores)>0):0.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "235a0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False\n",
    "\n",
    "if TEST:\n",
    "    # --- 1. Configuration and Model Loading ---\n",
    "    print(f\"Loading base model: {params.MODEL_NAME} on device: {device}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params.MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        params.MODEL_NAME, torch_dtype=dtype, device_map=device\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # --- 2. Load Dataset ---\n",
    "    print(\"Loading test dataset...\")\n",
    "    fpb_test = get_fpb_questions(split=\"test\")\n",
    "\n",
    "    # --- 3. Run test ----\n",
    "    test_reward_function(model, \n",
    "                         tokenizer, \n",
    "                         test_data=fpb_test, \n",
    "                         num_test_samples=100, \n",
    "                         device=device,\n",
    "                         temperature=1.0,\n",
    "                         seed=0)\n",
    "\n",
    "    # ---4. Clean up ----\n",
    "    try:\n",
    "        del model\n",
    "        del tokenizer\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f648071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grpo_only_pipeline(params, temperature=1.0, debug_mode=False):\n",
    "    \"\"\"\n",
    "    Runs the GRPO fine-tuning pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    if debug_mode:\n",
    "        print(\"--- RUNNING IN DEBUG MODE: Training for a few steps only. ---\")\n",
    "\n",
    "    fpb_train = get_fpb_questions(split=\"train\")\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    training_args = GRPOConfig(\n",
    "        # --- Core Training Hyperparameters ---\n",
    "        learning_rate=2e-5,  # or 3e-5\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        num_train_epochs=24,\n",
    "        max_steps=5 if debug_mode else -1,  # Stop after 5 steps in debug mode\n",
    "        optim=\"adamw_8bit\",\n",
    "        max_grad_norm=1.0,\n",
    "        # --- Batching & Generation Settings ---\n",
    "        per_device_train_batch_size=2,  # Smaller for debug\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_generations=4,\n",
    "        temperature=temperature,\n",
    "        max_prompt_length=params.max_prompt_length,\n",
    "        max_completion_length=params.max_completion_length,\n",
    "        # --- Performance & Precision ---\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        gradient_checkpointing=True,\n",
    "        use_vllm=USE_VLLM,\n",
    "        vllm_mode=\"colocate\",\n",
    "        vllm_gpu_memory_utilization=0.35,\n",
    "        # --- Logging & Saving ---\n",
    "        output_dir=\"grpo_training_output\",\n",
    "        report_to=\"tensorboard\",\n",
    "        logging_dir=\"logs/grpo-tags\",\n",
    "        logging_steps=1 if debug_mode else 100,\n",
    "        save_strategy=\"no\" if debug_mode else \"steps\",\n",
    "        save_steps=500,\n",
    "        # --- Model Initialization ---\n",
    "        # model_init_kwargs={\"attn_implementation\": \"eager\"},\n",
    "        model_init_kwargs={\n",
    "            \"dtype\": torch.bfloat16 if is_bfloat16_supported() else torch.float16,\n",
    "            \"attn_implementation\": \"flash_attention_2\"}, # Faster attention\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params.MODEL_NAME)\n",
    "    # Ensure a padding token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    trainer = GRPOTrainer(\n",
    "        model=params.MODEL_NAME,\n",
    "        args=training_args,\n",
    "        train_dataset=fpb_train,\n",
    "        # The new, hierarchically-designed reward functions\n",
    "        reward_funcs=[\n",
    "            format_reward_func,\n",
    "            json_validity_func,\n",
    "            correct_sentiment_func,\n",
    "            reasoning_quality_func,\n",
    "        ],\n",
    "        peft_config=peft_config,\n",
    "        processing_class=tokenizer,  # Use processing_class instead of tokenizer\n",
    "    )\n",
    "\n",
    "    print(\"--- Starting GRPO training... ---\")\n",
    "    trainer.train()\n",
    "    print(\"--- Training finished. ---\")\n",
    "\n",
    "    print(\"\\nMerging the final model\")\n",
    "    merged_model = trainer.model.merge_and_unload()\n",
    "\n",
    "    # In debug mode, we don't save the final model\n",
    "    if not debug_mode:\n",
    "        lora_output_dir = f\"{params.OUTPUT_MODEL}_lora\"\n",
    "        trainer.model.save_pretrained(lora_output_dir)\n",
    "        print(f\"LoRA adapters saved to {lora_output_dir}\")\n",
    "        # Also save the tokenizer with the LoRA adapters for convenience\n",
    "        tokenizer.save_pretrained(lora_output_dir)\n",
    "\n",
    "        # Save model and tokenizer\n",
    "        output_path = f\"{params.OUTPUT_MODEL}_grpo\"\n",
    "        print(f\"\\nSaving the final model to {output_path}\")\n",
    "        merged_model.save_pretrained(output_path)\n",
    "        tokenizer.save_pretrained(output_path)\n",
    "\n",
    "    close()\n",
    "\n",
    "    return merged_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9979e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[W920 13:38:39.822710303 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005c5e191a2d4f11b4768f7c7e697c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 71.47it/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting GRPO training... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1350' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1350/1350 15:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training finished. ---\n",
      "\n",
      "Merging the final model\n",
      "LoRA adapters saved to gemma-3-270m-it-grpo-finsent_lora\n",
      "\n",
      "Saving the final model to gemma-3-270m-it-grpo-finsent_grpo\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = run_grpo_only_pipeline(params, temperature=1.0, debug_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d86f22f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5978d6d28cf475ea18c5a42bc150cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f61e38697e549b2b253c1b050a60731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6725ecc10a5649db89baba3644c18e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5bfb03f94f468b969ab9471ab66fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer pushed to lmassaron/gemma-3-270m-it-grpo-finsent\n"
     ]
    }
   ],
   "source": [
    "# Define the name for your repository on the Hub\n",
    "repo_name = \"lmassaron/\" + params.OUTPUT_MODEL\n",
    "\n",
    "# Push the model to the Hub\n",
    "model.push_to_hub(repo_name)\n",
    "\n",
    "# Push the tokenizer to the Hub\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Model and tokenizer pushed to {repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "229f05d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:07<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Sentiment prediction accuracy: 0.73\n",
      ">>> Mean scored reward: -2.671\n",
      "\t> format_reward_func : -1.890 (0.05)\n",
      "\t> json_validity_func : 0.039 (0.13)\n",
      "\t> correct_sentiment_func : -0.868 (0.09)\n",
      "\t> reasoning_quality_func : 0.048 (0.06)\n"
     ]
    }
   ],
   "source": [
    "TEST = True\n",
    "\n",
    "if TEST:\n",
    "    # --- 1. Configuration and Model Loading ---\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Set use_cache to True in the model's configuration\n",
    "    model.config.use_cache = True \n",
    "\n",
    "    # --- 2. Load Dataset ---\n",
    "    print(\"Loading test dataset...\")\n",
    "    fpb_test = get_fpb_questions(split=\"test\")\n",
    "\n",
    "    # --- 3. Run test ----\n",
    "    test_reward_function(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        test_data=fpb_test, \n",
    "        num_test_samples=100, \n",
    "        device=device, \n",
    "        temperature=1.0, \n",
    "        seed=0\n",
    "    )\n",
    "\n",
    "    # ---4. Clean up ----\n",
    "    try:\n",
    "        del model\n",
    "        del tokenizer\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b301cdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e8f83fd23849cf80519d9bd0ce591a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe37580aa064fe8a4bbf469051d13f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_name, torch_dtype=dtype, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d97c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
