{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca3b5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 20 09:38:11 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   37C    P8             20W /  350W |     187MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1440      G   /usr/lib/xorg/Xorg                            167MiB |\n",
      "|    0   N/A  N/A      1550      G   /usr/bin/gnome-shell                           11MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7088e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-20 09:38:13 [__init__.py:216] Automatically detected platform cuda.\n",
      "Successfully imported vllm version: 0.10.2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from vllm.distributed.parallel_state import (\n",
    "        destroy_model_parallel,\n",
    "        destroy_distributed_environment,\n",
    "    )\n",
    "    import vllm\n",
    "    !export VLLM_LOGGING_LEVEL=ERROR\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Successfully imported vllm version: {vllm.__version__}\")\n",
    "    USE_VLLM = True\n",
    "except ImportError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    USE_VLLM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46155e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.8.0+cu128\n",
      "Using TRL version: 0.23.0\n",
      "Using bitsandbytes version: 0.47.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import trl\n",
    "import bitsandbytes\n",
    "\n",
    "print(f\"Using PyTorch version: {torch.__version__}\")\n",
    "print(f\"Using TRL version: {trl.__version__}\")\n",
    "print(f\"Using bitsandbytes version: {bitsandbytes.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82ddb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import warnings\n",
    "import contextlib\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, Dataset\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOConfig, GRPOTrainer, SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c27e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import userdata\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    # Access the secret\n",
    "    # Make sure you have created a secret named 'HUGGING_FACE_HUB_TOKEN' in your Colab secrets\n",
    "    hf_token = userdata.get(\"HF_TOKEN\")\n",
    "\n",
    "    # Log in to Hugging Face\n",
    "    # The 'token' parameter is used for non-interactive login\n",
    "    login(token=hf_token)\n",
    "\n",
    "    print(\"Hugging Face login successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ada8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silence_annoying_logs():\n",
    "    \"\"\"\n",
    "    Silences INFO and WARNING messages from specific libraries.\n",
    "    This is a robust method that handles placeholder loggers.\n",
    "    \"\"\"\n",
    "    # List of logger prefixes to silence\n",
    "    loggers_to_silence = [\"vllm\", \"numba\"]  # Added numba as it can also be noisy\n",
    "\n",
    "    for name, logger in logging.root.manager.loggerDict.items():\n",
    "        # Check if the logger name starts with any of the prefixes\n",
    "        if any(name.startswith(prefix) for prefix in loggers_to_silence):\n",
    "            # IMPORTANT: Check if it's a real logger and not a placeholder\n",
    "            if isinstance(logger, logging.Logger):\n",
    "                logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Also, let's suppress the UserWarning from numba if it appears\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"numba\")\n",
    "\n",
    "\n",
    "# Call the function to apply the changes\n",
    "silence_annoying_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b2399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System Prompts and Formatting ---\n",
    "\n",
    "R1_STYLE_SYSTEM_PROMPT = \"\"\"You are a financial analyst AI and you need to analyze the sentiment in the piece of news reported in square brackets. First, think in <reasoning> tags. Then, provide your answer in <answer> tags.\"\"\"\n",
    "TASK_SPECIFIC_INSTRUCTIONS = \"\"\"The answer must be a single JSON object with two keys: `sentiment` (\"positive\", \"negative\", or \"neutral\") and `explanation` (a brief justification under 50 words).\"\"\"\n",
    "\n",
    "EXAMPLE = \"\"\"<reasoning>\n",
    "</reasoning>\n",
    "<answer>\n",
    "```json\n",
    "{\n",
    "  \"sentiment\": \"...\",\n",
    "  \"explanation\": \"...\"\n",
    "}\n",
    "</answer>\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\\n\n",
    "{reasoning}\\n\n",
    "</reasoning>\\n\n",
    "<answer>\\n\n",
    "{answer}\\n\n",
    "</answer>\\n\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    R1_STYLE_SYSTEM_PROMPT\n",
    "    + \"\\n\\n\"\n",
    "    + TASK_SPECIFIC_INSTRUCTIONS\n",
    "    + \"\\n\\n\"\n",
    "    + \"Follow this structure:\\n\"\n",
    "    + EXAMPLE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83ae213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration Class ---\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters\"\"\"\n",
    "\n",
    "    SIZE = \"3-270m\"\n",
    "    #SIZE = \"3-1b\"\n",
    "    MODEL_NAME = f\"google/gemma-{SIZE}-it\"\n",
    "    OUTPUT_MODEL = f\"gemma-{SIZE}-it-grpo-finsent\"\n",
    "\n",
    "    max_prompt_length = 1024\n",
    "    max_completion_length = 1024\n",
    "\n",
    "\n",
    "# --- Environment and Utility Functions ---\n",
    "\n",
    "\n",
    "def init():\n",
    "    \"\"\"Initialization script\"\"\"\n",
    "    os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "    # It is recommended to set the HF_TOKEN as an environment variable\n",
    "    token = os.environ.get(\"HF_TOKEN\")\n",
    "    if token:\n",
    "        login(token=token)\n",
    "    else:\n",
    "        print(\"HF_TOKEN not set. You might need to log in manually.\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def close(llm=None):\n",
    "    \"\"\"Close vllm and clean up resources\"\"\"\n",
    "    destroy_model_parallel()\n",
    "    destroy_distributed_environment()\n",
    "    if llm:\n",
    "        del llm.llm_engine.model_executor\n",
    "        del llm\n",
    "    with contextlib.suppress(AssertionError):\n",
    "        torch.distributed.destroy_process_group()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def is_bfloat16_supported():\n",
    "    \"\"\"Checks if the current device supports bfloat16.\"\"\"\n",
    "    return torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "\n",
    "def info_device():\n",
    "    \"\"\"Get device for PyTorch\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "\n",
    "# Note: The data loading functions will be moved to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb42bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fpb_questions(split=\"train\"):\n",
    "    \"\"\"Upload FinancialPhraseBank dataset\"\"\"\n",
    "    repo_id = \"lmassaron/FinancialPhraseBank\"\n",
    "    data = load_dataset(repo_id, cache_dir=\"/tmp\")[split]\n",
    "\n",
    "    data = data.map(\n",
    "        lambda x: {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"user\", \"content\": SYSTEM_PROMPT + \"\\n[\" + x[\"sentence\"]+\"]\"},\n",
    "            ],\n",
    "            \"answer\": x[\"sentiment\"],\n",
    "        }\n",
    "    )\n",
    "    data = data.remove_columns([\"sentiment\", \"sentence\", \"label\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24edcd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PROMPT ---\n",
      "You are a financial analyst AI and you need to analyze the sentiment in the piece of news reported in square brackets. First, think in <reasoning> tags. Then, provide your answer in <answer> tags.\n",
      "\n",
      "The answer must be a single JSON object with two keys: `sentiment` (\"positive\", \"negative\", or \"neutral\") and `explanation` (a brief justification under 50 words).\n",
      "\n",
      "Follow this structure:\n",
      "<reasoning>\n",
      "</reasoning>\n",
      "<answer>\n",
      "```json\n",
      "{\n",
      "  \"sentiment\": \"...\",\n",
      "  \"explanation\": \"...\"\n",
      "}\n",
      "</answer>\n",
      "[The new agreement , which expands a long-established cooperation between the companies , involves the transfer of certain engineering and documentation functions from Larox to Etteplan .]\n",
      "\n",
      "--- GROUND TRUTH ANSWER ---\n",
      "positive\n",
      "==============================\n",
      "--- PROMPT ---\n",
      "You are a financial analyst AI and you need to analyze the sentiment in the piece of news reported in square brackets. First, think in <reasoning> tags. Then, provide your answer in <answer> tags.\n",
      "\n",
      "The answer must be a single JSON object with two keys: `sentiment` (\"positive\", \"negative\", or \"neutral\") and `explanation` (a brief justification under 50 words).\n",
      "\n",
      "Follow this structure:\n",
      "<reasoning>\n",
      "</reasoning>\n",
      "<answer>\n",
      "```json\n",
      "{\n",
      "  \"sentiment\": \"...\",\n",
      "  \"explanation\": \"...\"\n",
      "}\n",
      "</answer>\n",
      "[( ADP News ) - Finnish handling systems provider Cargotec Oyj ( HEL : CGCBV ) announced on Friday it won orders worth EUR 10 million ( USD 13.2 m ) to deliver linkspans to Jordan , Morocco and Ireland .]\n",
      "\n",
      "--- GROUND TRUTH ANSWER ---\n",
      "positive\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Let's load a small sample of the GSM8K data to see its structure.\n",
    "fpb_sample = get_fpb_questions(split=\"test\").select(range(2))\n",
    "\n",
    "for response in fpb_sample:\n",
    "    print(\"--- PROMPT ---\")\n",
    "    print(response[\"prompt\"][0][\"content\"])\n",
    "    print(\"\\n--- GROUND TRUTH ANSWER ---\")\n",
    "    print(response[\"answer\"])\n",
    "    print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1a9e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_answer(text: str, start_tag=\"<answer>\", end_tag=\"</answer>\") -> dict | None:\n",
    "    \"\"\"\n",
    "    Extracts the content from the last occurrence of <answer> tags, cleans it,\n",
    "    \"\"\"\n",
    "    pattern = re.escape(start_tag) + r\"(.*?)\" + re.escape(end_tag)\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "\n",
    "def extract_last_json_from_answer(text: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Tries to extract the last JSON object using a simple greedy regex.\n",
    "    This is unreliable for nested JSON.\n",
    "    \"\"\"\n",
    "    matches = re.findall(r\"\\{[\\s\\S]*?\\}\", text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(matches[-1])\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "def extract_reasoning(\n",
    "    text: str, start_tag=\"<reasoning>\", end_tag=\"</reasoning>\"\n",
    ") -> str:\n",
    "    \"\"\"Extracts the content within the <reasoning> tags.\"\"\"\n",
    "    pattern = re.escape(start_tag) + r\"(.*?)\" + re.escape(end_tag)\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49edf8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_REASONING = 0.5\n",
    "PENALTY_REASONING = -0.5\n",
    "\n",
    "REWARD_ANSWER = 1.0\n",
    "PENALTY_ANSWER = -1.0\n",
    "\n",
    "REWARD_VALID_JSON = 1.0\n",
    "PENALTY_VALID_JSON = -1.0\n",
    "\n",
    "REWARD_REASONING_LENGTH = 1.0\n",
    "MIN_REASONING_LEN = 10\n",
    "MAX_REASONING_LEN = 50\n",
    "\n",
    "# Sentiment rewards\n",
    "REWARD_SENTIMENT_CORRECT = 3.0  # Greatly reward correct sentiment\n",
    "REWARD_SENTIMENT_NEAR = -1.5  # Punish for \"near\" sentiment\n",
    "PENALTY_SENTIMENT_WRONG = -3.0  # Harshly punish incorrect sentiment\n",
    "\n",
    "def flatten(completion):\n",
    "    if not completion:\n",
    "        return []\n",
    "    if isinstance(completion[0], list):\n",
    "        return [item for sublist in completion for item in sublist]\n",
    "    return completion\n",
    "\n",
    "\n",
    "def reasoning_reward_func(completions, **kwargs):\n",
    "    output = flatten(completions)\n",
    "    rewards = []\n",
    "    for response in output:\n",
    "        reasoning = extract_reasoning(response[\"content\"])\n",
    "        if reasoning != \"\":\n",
    "            rewards.append(REWARD_REASONING)\n",
    "        else:\n",
    "            rewards.append(PENALTY_REASONING)\n",
    "    # print(\"reasoning_reward_func\", rewards)\n",
    "    return rewards\n",
    "\n",
    "def answer_reward_func(completions, **kwargs):\n",
    "    output = flatten(completions)\n",
    "    rewards = []\n",
    "    for response in output:\n",
    "        answer = extract_answer(response[\"content\"])\n",
    "        if answer != \"\":\n",
    "            rewards.append(REWARD_ANSWER)\n",
    "        else:\n",
    "            rewards.append(PENALTY_ANSWER)\n",
    "    # print(\"answer_reward_func\", rewards)\n",
    "    return rewards\n",
    "\n",
    "def valid_json_func(completions, **kwargs):\n",
    "    output = flatten(completions)\n",
    "    rewards = []\n",
    "    for item in output:\n",
    "        parsed_answer = extract_last_json_from_answer(item[\"content\"])\n",
    "        if (\n",
    "            parsed_answer != \"\"\n",
    "            and \"sentiment\" in parsed_answer\n",
    "            and \"explanation\" in parsed_answer\n",
    "        ):\n",
    "            rewards.append(REWARD_VALID_JSON)\n",
    "        else:\n",
    "            rewards.append(PENALTY_VALID_JSON)\n",
    "    # print(\"valid_json_func\", rewards)\n",
    "    return rewards\n",
    "\n",
    "def correct_sentiment_func(completions, answer, **kwargs):\n",
    "    sentiment_scores = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "    output = flatten(completions)\n",
    "    rewards = []\n",
    "    for response, actual_sentiment in zip(output, answer):\n",
    "        #print(f\"{response} [[[{actual_sentiment}]]]\")\n",
    "        parsed_answer = extract_last_json_from_answer(response[\"content\"])\n",
    "        if (\n",
    "            parsed_answer != \"\"\n",
    "            and \"sentiment\" in parsed_answer\n",
    "        ):\n",
    "            predicted_sentiment = parsed_answer[\"sentiment\"].lower()\n",
    "            if predicted_sentiment not in sentiment_scores:\n",
    "                rewards.append(PENALTY_SENTIMENT_WRONG)\n",
    "            else:\n",
    "                pred_score = sentiment_scores[predicted_sentiment]\n",
    "                actual_score = sentiment_scores[actual_sentiment]\n",
    "                distance = abs(pred_score - actual_score)\n",
    "\n",
    "                if distance == 0:\n",
    "                    rewards.append(REWARD_SENTIMENT_CORRECT)\n",
    "                elif distance == 1:  # e.g., positive vs neutral, or negative vs neutral\n",
    "                    rewards.append(REWARD_SENTIMENT_NEAR)\n",
    "                elif distance == 2:  # positive vs negative\n",
    "                    rewards.append(PENALTY_SENTIMENT_WRONG)\n",
    "        else:\n",
    "            rewards.append(PENALTY_SENTIMENT_WRONG)\n",
    "    # print(\"correct sent:\", rewards)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c038f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward_function(model, tokenizer, test_data, num_test_samples, seed=0):\n",
    "    random.seed(seed)\n",
    "    random_indices = random.sample(range(len(test_data)), num_test_samples)\n",
    "    accuracy = []\n",
    "    scored_reward = []\n",
    "    reward_dict = {\n",
    "        \"reasoning_reward_func\": [],\n",
    "        \"answer_reward_func\": [],\n",
    "        \"valid_json_func\": [],\n",
    "        \"correct_sentiment_func\": [],\n",
    "    }\n",
    "    for i in tqdm(random_indices):\n",
    "        sample = test_data.select(range(i, i + 1))[0]\n",
    "        full_prompt = sample[\"prompt\"][0][\"content\"]\n",
    "        ground_truth_answer = sample[\"answer\"]\n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # print(\n",
    "        #    f\"\\n==================== Sample {i+1}/{len(test_data)} ====================\"\n",
    "        # )\n",
    "        # print(\n",
    "        #    f\"\\n>>> Input text:\\n{full_prompt[len(SYSTEM_PROMPT):]}...\"\n",
    "        # )\n",
    "        # print(f\"\\n>>> Ground Truth Answer:\\n{json.dumps(ground_truth_answer, indent=2)}\")\n",
    "\n",
    "        # Generate a completion from the base model\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,  # Give it enough space to generate something\n",
    "            temperature=1.0,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        # Decode only the newly generated tokens\n",
    "        generated_text = tokenizer.decode(\n",
    "            outputs[0, inputs.input_ids.shape[-1] :], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        #print(f\"\\n>>> Untrained Model's Raw Output:\\n{generated_text}\")\n",
    "        if ground_truth_answer in generated_text.lower():\n",
    "            accuracy.append(1)\n",
    "        else:\n",
    "            accuracy.append(0)\n",
    "\n",
    "        # --- 4. Calculate Reward ---\n",
    "        # The reward function expects data in a specific batch format, so we wrap our single sample\n",
    "        completions_batch = [[{\"content\": generated_text}]]\n",
    "        ground_truth_batch = [ground_truth_answer]\n",
    "        rewards = [\n",
    "            reasoning_reward_func(completions_batch),\n",
    "            answer_reward_func(completions_batch),\n",
    "            valid_json_func(completions_batch),\n",
    "            correct_sentiment_func(completions_batch, ground_truth_batch),\n",
    "        ]\n",
    "        reward_dict[\"reasoning_reward_func\"].append(rewards[0])\n",
    "        reward_dict[\"answer_reward_func\"].append(rewards[1])\n",
    "        reward_dict[\"valid_json_func\"].append(rewards[2])\n",
    "        reward_dict[\"correct_sentiment_func\"].append(rewards[3])\n",
    "\n",
    "        calculated_reward = sum([sum(reward) for reward in rewards])  # Get the reward for our single sample\n",
    "        scored_reward.append(calculated_reward)\n",
    "\n",
    "        # print(f\"\\n>>> Calculated Reward: {calculated_reward:.2f}\")\n",
    "        # print(\"========================================================\\n\")\n",
    "\n",
    "    print(f\"\\n>>> Sentiment prediction accuracy: {np.mean(accuracy):0.2f}\")\n",
    "    print(f\">>> Mean scored reward: {np.mean(scored_reward):0.3f}\")\n",
    "    for scorer, scores in reward_dict.items():\n",
    "        print(f\"\\t> {scorer} : {np.mean(scores):0.3f} ({np.mean(np.array(scores)>0):0.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "235a0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False\n",
    "\n",
    "if TEST:\n",
    "    # --- 1. Configuration and Model Loading ---\n",
    "    params = Config()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n",
    "    print(f\"Loading base model: {params.MODEL_NAME} on device: {device}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params.MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        params.MODEL_NAME, torch_dtype=dtype, device_map=device\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # --- 2. Load Dataset ---\n",
    "    print(\"Loading test dataset...\")\n",
    "    fpb_test = get_fpb_questions(split=\"test\")\n",
    "\n",
    "    # --- 3. Run test ----\n",
    "    test_reward_function(model, tokenizer, test_data=fpb_test, num_test_samples=100, seed=0)\n",
    "\n",
    "    # ---4. Clean up ----\n",
    "    try:\n",
    "        del model\n",
    "        del tokenizer\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f648071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grpo_only_pipeline(run_training=False, debug_mode=False):\n",
    "    \"\"\"\n",
    "    Runs the GRPO fine-tuning pipeline.\n",
    "    \"\"\"\n",
    "    if not run_training:\n",
    "        print(\"Training is skipped. Set run_training=True to execute.\")\n",
    "        return None, None\n",
    "\n",
    "    if debug_mode:\n",
    "        print(\"--- RUNNING IN DEBUG MODE: Training for a few steps only. ---\")\n",
    "\n",
    "    init()\n",
    "    params = Config()\n",
    "    fpb_train = get_fpb_questions(split=\"train\")\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    training_args = GRPOConfig(\n",
    "        # --- Core Training Hyperparameters ---\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        num_train_epochs=1,\n",
    "        max_steps=5 if debug_mode else -1,  # Stop after 5 steps in debug mode\n",
    "        optim=\"adamw_8bit\",\n",
    "        max_grad_norm=0.1,\n",
    "        # --- Batching & Generation Settings ---\n",
    "        per_device_train_batch_size=2,  # Smaller for debug\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_generations=4,\n",
    "        temperature=0.5,\n",
    "        max_prompt_length=params.max_prompt_length,\n",
    "        max_completion_length=params.max_completion_length,\n",
    "        # --- Performance & Precision ---\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        gradient_checkpointing=True,\n",
    "        use_vllm=USE_VLLM,\n",
    "        vllm_mode=\"colocate\",\n",
    "        vllm_gpu_memory_utilization=0.35,\n",
    "        # --- Logging & Saving ---\n",
    "        output_dir=\"grpo_only_output\",\n",
    "        report_to=\"tensorboard\",\n",
    "        logging_dir=\"logs/grpo_only_runs\",\n",
    "        logging_steps=1 if debug_mode else 100,\n",
    "        save_strategy=\"no\" if debug_mode else \"steps\",\n",
    "        save_steps=500,\n",
    "        # --- Model Initialization ---\n",
    "        model_init_kwargs={\"attn_implementation\": \"eager\"},\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params.MODEL_NAME)\n",
    "    # Ensure a padding token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    trainer = GRPOTrainer(\n",
    "        model=params.MODEL_NAME,\n",
    "        args=training_args,\n",
    "        train_dataset=fpb_train,\n",
    "        reward_funcs=[\n",
    "            reasoning_reward_func,\n",
    "            answer_reward_func,\n",
    "            valid_json_func, \n",
    "            correct_sentiment_func,\n",
    "        ],\n",
    "        peft_config=peft_config,\n",
    "        processing_class=tokenizer,  # Use processing_class instead of tokenizer\n",
    "    )\n",
    "\n",
    "    print(\"--- Starting GRPO training... ---\")\n",
    "    trainer.train()\n",
    "    print(\"--- Training finished. ---\")\n",
    "\n",
    "    print(\"\\nMerging the final model\")\n",
    "    merged_model = trainer.model.merge_and_unload()\n",
    "\n",
    "    # In debug mode, we don't save the final model\n",
    "    if not debug_mode:\n",
    "        lora_output_dir = f\"{params.OUTPUT_MODEL}_lora\"\n",
    "        trainer.model.save_pretrained(lora_output_dir)\n",
    "        print(f\"LoRA adapters saved to {lora_output_dir}\")\n",
    "        # Also save the tokenizer with the LoRA adapters for convenience\n",
    "        tokenizer.save_pretrained(lora_output_dir)\n",
    "\n",
    "        # Save model and tokenizer\n",
    "        output_path = f\"{params.OUTPUT_MODEL}_grpo\"\n",
    "        print(f\"\\nSaving the final model to {output_path}\")\n",
    "        merged_model.save_pretrained(output_path)\n",
    "        tokenizer.save_pretrained(output_path)\n",
    "\n",
    "    close()\n",
    "\n",
    "    return merged_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9979e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[W920 09:38:31.597560456 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9886214f2c814b78b9f304e183015491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 71.41it/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting GRPO training... ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='258' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [258/450 02:50 < 02:07, 1.50 it/s, Epoch 0.57/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>-0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = run_grpo_only_pipeline(run_training=True, debug_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc3be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Config()\n",
    "params.OUTPUT_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name for your repository on the Hub\n",
    "repo_name = \"lmassaron/\" + params.OUTPUT_MODEL\n",
    "\n",
    "# Push the model to the Hub\n",
    "model.push_to_hub(repo_name)\n",
    "\n",
    "# Push the tokenizer to the Hub\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Model and tokenizer pushed to {repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f05d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = True\n",
    "\n",
    "if TEST:\n",
    "    # --- 1. Configuration and Model Loading ---\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Set use_cache to True in the model's configuration\n",
    "    model.config.use_cache = True \n",
    "\n",
    "    # --- 2. Load Dataset ---\n",
    "    print(\"Loading test dataset...\")\n",
    "    fpb_test = get_fpb_questions(split=\"test\")\n",
    "\n",
    "    # --- 3. Run test ----\n",
    "    test_reward_function(\n",
    "        model, tokenizer, test_data=fpb_test, num_test_samples=100, seed=0\n",
    "    )\n",
    "\n",
    "    # ---4. Clean up ----\n",
    "    try:\n",
    "        del model\n",
    "        del tokenizer\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126b1906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
