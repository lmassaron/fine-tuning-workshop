{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmassaron/fine-tuning-workshop/blob/main/02_synthetic_data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OJvuwcN0ynrh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm synthetic-data-kit==0.0.3\n",
        "else:\n",
        "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except: get_numpy = \"numpy\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade         unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "    !uv pip install synthetic-data-kit==0.0.3\n",
        "!uv pip install transformers==4.55.4\n",
        "!uv pip install --no-deps trl==0.22.2\n",
        "!uv pip install wikipedia-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dL-6wLPoyyET"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except: get_numpy = \"numpy\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade \\\n",
        "        unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "!uv pip install transformers==4.55.4\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzDmaFKS2Mg2",
        "outputId": "cb6f8958-6d21-4980-fab9-d1e1031565f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "INFO 09-25 08:31:23 [__init__.py:244] Automatically detected platform cuda.\n",
            "ERROR 09-25 08:31:30 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import wikipediaapi\n",
        "from unsloth.dataprep import SyntheticDataKit\n",
        "import huggingface_hub\n",
        "from collections import Counter\n",
        "import itertools\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict, ClassLabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZHzKje014dJf"
      },
      "outputs": [],
      "source": [
        "DEMO = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O6EH83m_2Rz8"
      },
      "outputs": [],
      "source": [
        "# Pre-compile the regular expression pattern for better performance\n",
        "BRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n",
        "\n",
        "def remove_braces_and_content(text):\n",
        "    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n",
        "    return BRACES_PATTERN.sub('', text)\n",
        "\n",
        "def clean_string(input_string):\n",
        "    \"\"\"Clean the input string.\"\"\"\n",
        "\n",
        "    # Remove extra spaces by splitting the string by spaces and joining back together\n",
        "    cleaned_string = ' '.join(input_string.split())\n",
        "\n",
        "    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n",
        "    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n",
        "\n",
        "    # Remove all occurrences of curly braces and their content from the cleaned string\n",
        "    cleaned_string = remove_braces_and_content(cleaned_string)\n",
        "\n",
        "    # Return the cleaned string\n",
        "    return cleaned_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Nq8-Z4RBykFh"
      },
      "outputs": [],
      "source": [
        "def extract_wikipedia_pages(wiki_wiki, category_name):\n",
        "    \"\"\"Extract all references from a category on Wikipedia\"\"\"\n",
        "\n",
        "    # Get the Wikipedia page corresponding to the provided category name\n",
        "    category = wiki_wiki.page(\"Category:\" + category_name)\n",
        "\n",
        "    # Initialize an empty list to store page titles\n",
        "    pages = []\n",
        "\n",
        "    # Check if the category exists\n",
        "    if category.exists():\n",
        "        # Iterate through each article in the category and append its title to the list\n",
        "        for article in category.categorymembers.values():\n",
        "            pages.append(article.title)\n",
        "\n",
        "    # Return the list of page titles\n",
        "    return pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SLYvGfu72XUF"
      },
      "outputs": [],
      "source": [
        "def get_wikipedia_pages(categories):\n",
        "    \"\"\"Retrieve Wikipedia pages from a list of categories and extract their content\"\"\"\n",
        "\n",
        "    # Create a Wikipedia object\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('Gemma AI Assistant (gemma@example.com)', 'en')\n",
        "\n",
        "    # Initialize lists to store explored categories and Wikipedia pages\n",
        "    explored_categories = []\n",
        "    wikipedia_pages = []\n",
        "\n",
        "    # Iterate through each category\n",
        "    print(\"- Processing Wikipedia categories:\")\n",
        "    for category_name in categories:\n",
        "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
        "\n",
        "        # Get the Wikipedia page corresponding to the category\n",
        "        category = wiki_wiki.page(\"Category:\" + category_name)\n",
        "\n",
        "        # Extract Wikipedia pages from the category and extend the list\n",
        "        wikipedia_pages.extend(extract_wikipedia_pages(wiki_wiki, category_name))\n",
        "\n",
        "        # Add the explored category to the list\n",
        "        explored_categories.append(category_name)\n",
        "\n",
        "    # Extract subcategories and remove duplicate categories\n",
        "    categories_to_explore = [item.replace(\"Category:\", \"\") for item in wikipedia_pages if \"Category:\" in item]\n",
        "    wikipedia_pages = list(set([item for item in wikipedia_pages if \"Category:\" not in item]))\n",
        "\n",
        "    # Explore subcategories recursively\n",
        "    while categories_to_explore:\n",
        "        category_name = categories_to_explore.pop()\n",
        "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
        "\n",
        "        # Extract more references from the subcategory\n",
        "        more_refs = extract_wikipedia_pages(wiki_wiki, category_name)\n",
        "\n",
        "        # Iterate through the references\n",
        "        for ref in more_refs:\n",
        "            # Check if the reference is a category\n",
        "            if \"Category:\" in ref:\n",
        "                new_category = ref.replace(\"Category:\", \"\")\n",
        "                # Add the new category to the explored categories list\n",
        "                if new_category not in explored_categories:\n",
        "                    explored_categories.append(new_category)\n",
        "            else:\n",
        "                # Add the reference to the Wikipedia pages list\n",
        "                if ref not in wikipedia_pages:\n",
        "                    wikipedia_pages.append(ref)\n",
        "\n",
        "    # Initialize a list to store extracted texts\n",
        "    extracted_texts = []\n",
        "\n",
        "    # Iterate through each Wikipedia page\n",
        "    print(\"- Processing Wikipedia pages:\")\n",
        "    for page_title in tqdm(wikipedia_pages):\n",
        "        try:\n",
        "            # Make a request to the Wikipedia page\n",
        "            page = wiki_wiki.page(page_title)\n",
        "\n",
        "            # Check if the page summary does not contain certain keywords\n",
        "            if \"Biden\" not in page.summary and \"Trump\" not in page.summary:\n",
        "                # Append the page title and summary to the extracted texts list\n",
        "                if len(page.summary) > len(page.title):\n",
        "                    extracted_texts.append(page.title + \" : \" + clean_string(page.summary))\n",
        "\n",
        "                # Iterate through the sections in the page\n",
        "                for section in page.sections:\n",
        "                    # Append the page title and section text to the extracted texts list\n",
        "                    if len(section.text) > len(page.title):\n",
        "                        extracted_texts.append(page.title + \" : \" + clean_string(section.text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing page {page.title}: {e}\")\n",
        "\n",
        "    # Return the extracted texts\n",
        "    return extracted_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk-aqKqq2ehW",
        "outputId": "5e402166-f505-4841-e7e9-2bc54ec913cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Processing Wikipedia categories:\n",
            "\tExploring Sherlock_Holmes on Wikipedia\n",
            "\tExploring Writers of Sherlock Holmes pastiches on Wikipedia\n",
            "\tExploring Works based on Sherlock Holmes on Wikipedia\n",
            "\tExploring Sherlock Holmes short story collections on Wikipedia\n",
            "\tExploring Sherlock Holmes short stories on Wikipedia\n",
            "\tExploring Sherlock Holmes audio adaptations on Wikipedia\n",
            "\tExploring Sherlock Holmes scholars on Wikipedia\n",
            "\tExploring Sherlock Holmes novels on Wikipedia\n",
            "\tExploring Sherlock Holmes navigational boxes on Wikipedia\n",
            "\tExploring Sherlock Holmes lists on Wikipedia\n",
            "\tExploring Dartmoor on Wikipedia\n",
            "\tExploring Sherlock Holmes characters on Wikipedia\n",
            "\tExploring Baker Street on Wikipedia\n",
            "- Processing Wikipedia pages:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 459/459 [01:04<00:00,  7.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2042 Wikipedia pages\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "categories = [\n",
        "    \"Sherlock_Holmes\",\n",
        "    \"Arthur_Conan_Doyle\",\n",
        "    \"A_Scandal_in_Bohemia\",\n",
        "    \"The_Adventures_of_Sherlock_Holmes\",\n",
        "    \"A_Study_in_Scarlet\",\n",
        "    \"The_Sign_of_the_Four\",\n",
        "    \"The_Memoirs_of_Sherlock_Holmes\",\n",
        "    \"The_Hound_of_the_Baskervilles\",\n",
        "    \"The_Return_of_Sherlock_Holmes\",\n",
        "    \"The_Valley_of_Fear\",\n",
        "    \"His_Last_Bow\",\n",
        "    \"The_Case-Book_of_Sherlock_Holmes\",\n",
        "    \"Canon_of_Sherlock_Holmes\",\n",
        "    \"Dr._Watson\",\n",
        "    \"221B_Baker_Street\",\n",
        "    \"Mrs._Hudson\",\n",
        "    \"Professor_Moriarty\",\n",
        "    \"The_Strand_Magazine\",\n",
        "    \"Minor_Sherlock_Holmes_characters\",\n",
        "    \"Inspector_Lestrade\",\n",
        "    \"Mycroft_Holmes\",\n",
        "    \"Irene_Adler\",\n",
        "    \"Colonel_Moran\",\n",
        "    \"Baker_Street_Irregulars\",\n",
        "    \"Giant_rat_of_Sumatra\",\n",
        "    \"The_Story_of_the_Lost_Special\",\n",
        "    \"How_Watson_Learned_the_Trick\",\n",
        "    \"Diogenes_Club\",\n",
        "    \"The_Dynamics_of_an_Asteroid\",\n",
        "    \"Reichenbach_Falls\",\n",
        "    \"A_Treatise_on_the_Binomial_Theorem\",\n",
        "    \"Sherlockian_game\",\n",
        "    \"List_of_Holmesian_studies\",\n",
        "    \"The_New_Annotated_Sherlock_Holmes\",\n",
        "    \"The_Private_Life_of_Sherlock_Holmes_(book)\",\n",
        "    \"The_Great_Detective_(book)\",\n",
        "    \"Naked_Is_the_Best_Disguise\",\n",
        "    \"Sherlock_Holmes_fandom\",\n",
        "    \"Sherlockiana\",\n",
        "    \"Sherlock_Holmes_Museum\",\n",
        "    \"The_Sherlock_Holmes\",\n",
        "    \"The_Baker_Street_Irregulars\",\n",
        "    \"The_Baker_Street_Journal\",\n",
        "    \"Sidney_Paget\",\n",
        "    \"The_Strand_Magazine\",\n",
        "    \"Undershaw\",\n",
        "    \"Canon_of_Sherlock_Holmes\",\n",
        "    \"Adaptations_of_Sherlock_Holmes\",\n",
        "    \"Sherlock_Holmes_pastiches\",\n",
        "    \"Popular_culture_references_to_Sherlock_Holmes\",\n",
        "]\n",
        "\n",
        "if DEMO:\n",
        "    categories = [\"Sherlock_Holmes\"]\n",
        "\n",
        "extracted_texts = get_wikipedia_pages(categories)\n",
        "print(\"Found\", len(extracted_texts), \"Wikipedia pages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7ii2mZw26x0",
        "outputId": "a001c710-919d-4d9b-e557-e876fedcfc30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All texts have been saved successfully.\n"
          ]
        }
      ],
      "source": [
        "output_dir = 'data/output'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for k, text in enumerate(extracted_texts):\n",
        "    file_path = os.path.join(output_dir, f'sherlock_{k}.txt')\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write(text)\n",
        "    if DEMO and k > 9:\n",
        "        break\n",
        "\n",
        "print(\"All texts have been saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hZumHepuykFi"
      },
      "outputs": [],
      "source": [
        "filenames = [f\"data/output/{file}\" for file in os.listdir(\"data/output\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6b337fd9169d44b2912857836c2cb139",
            "841743119ad341f79e2e9578bcc80752",
            "ac633a1e9da541ce92be69e04ca9bdba",
            "f1a3081d521846f1a798895f7f193b42",
            "facc252c42d4464789c87f50a0aaedc0",
            "49f9b29386ba42a290e7b99c8aa698ea",
            "351500b2733e45f1ad0eb4ab4b8ed2d7",
            "ac429e7ebf474978859dc4d95168d04a",
            "f5a8b5f752d346838eb6d8e399950bcd",
            "1a287daf9a4d4e54933126e172851c87",
            "3b29065512914644967a0ac62ad5658c",
            "be19f046cdd44a54a27f6b2bc32748b7",
            "63ec8490d8b4460c82b0a8d3d64b95b0",
            "d9c06b0a14a940b98d8e8b83c90ef616",
            "f218346583ca4b029b9392cd6497c4dc",
            "cd3f928c8161486ba70c150972cd2a74",
            "ebc680f1399b4597b7a572544be496c1",
            "93fdd16b6b724929a7b884142fa80ba6",
            "19e03d3a43314c048a4264f6a5a1caee",
            "7a58fa7bdbf24d638441e2f66ff45eee",
            "d6a93b3cef3b46c0bd6b23976076aac0",
            "24fb26cfa81746779747e49490786ae6",
            "41cbc18590a24107a1e80ab31a3c58dd",
            "1d25a7b5bfb246c1adbbba8864986a44",
            "226b7ab90023484484745975d1f0e6c0",
            "49d4b4e0f4c343caaf076d76b895325e",
            "ea2ae9fb3cb04348be160c2d4f3ae770",
            "fe73eb9f33a24180957da13dcb0c555d",
            "5e6b2a6a845c4c72bd8f9e09c67224a5",
            "2013cf5b594e47309f4e077d3fe0282d",
            "7c79b1c7147d419cbb0cd986d86eda3a",
            "58e9e82b12364b7090198a5de6545f2d",
            "0f018cfa4cbd4ba48c8994144b68a14a",
            "0ce49735af464559a0d10deb1bacf5f1",
            "0b4a6dc10bcf45cf8ad81a85c123a87b",
            "d2c6dacca24441158a0a14a385b9dd43",
            "6bdeafe125c5414cb522edb15f1d568b",
            "9566a1540f5f4d83a879e6f3a280743b",
            "28bb4773c2b74cd4bf0a8b4581c4b915",
            "3cd342899fcd44c182af0993e2cba6fe",
            "dd3aad470c09495ca694b1a801ad7fa0",
            "70703b9cc48c439e97a50da745e6e42c",
            "a564493fb379496a99444025ba436e9b",
            "6f284950d4184309b5daf93d682ea36c",
            "8878843f09f54135a301d2ef67dc902a",
            "5ce5c8d383ba49cfb8c2050dbb56ad1d",
            "ba59cd7a3456433ebadaf607a3f8a076",
            "4e97067b6bee494c8ace55785d27c6df",
            "da292c9536924ac39ffac041c1d4dbad",
            "6e8755428cea46e5a1bdb50433b83967",
            "ea2ccac65e2242c79f5a822cb4f7cc07",
            "72507b08307845bca26605a6a7533c6e",
            "474c12ec5e9f47daab30327fae2bc712",
            "22c6a34905ed47468fcd0229a5213d74",
            "72097af672d14073aad5a2be8d00a7b6"
          ]
        },
        "id": "I5-KNLWuAvXy",
        "outputId": "8fc6993d-f615-4087-f050-e59ecabefdd2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b337fd9169d44b2912857836c2cb139"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be19f046cdd44a54a27f6b2bc32748b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41cbc18590a24107a1e80ab31a3c58dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ce49735af464559a0d10deb1bacf5f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8878843f09f54135a301d2ef67dc902a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 09-25 08:33:01 [vllm_utils.py:688] Unsloth: Patching vLLM v1 graph capture\n",
            "INFO 09-25 08:33:01 [vllm_utils.py:716] Unsloth: Patching vLLM v0 graph capture\n",
            "Unsloth: Using dtype = torch.float16 for vLLM.\n",
            "Unsloth: vLLM loading unsloth/Llama-3.2-3B-Instruct with actual GPU utilization = 89.39%\n",
            "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 192.\n",
            "Unsloth: vLLM's KV Cache can use up to 7.19 GB. Also swap space = 0 GB.\n",
            "vLLM STDOUT: INFO 09-25 08:33:22 [__init__.py:244] Automatically detected platform cuda.\n",
            "vLLM STDOUT: INFO 09-25 08:33:30 [api_server.py:1395] vLLM API server version 0.9.2\n",
            "vLLM STDOUT: INFO 09-25 08:33:30 [cli_args.py:325] non-default args: {'model': 'unsloth/Llama-3.2-3B-Instruct', 'dtype': 'float16', 'seed': 0, 'max_model_len': 2048, 'max_logprobs': 0, 'gpu_memory_utilization': 0.8938626454842437, 'swap_space': 0.0, 'enable_prefix_caching': True, 'max_num_batched_tokens': 2048, 'max_num_seqs': 192, 'enable_chunked_prefill': True, 'disable_log_stats': True}\n",
            "vLLM STDOUT: INFO 09-25 08:33:54 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\n",
            "vLLM STDOUT: WARNING 09-25 08:33:54 [config.py:3371] Casting torch.bfloat16 to torch.float16.\n",
            "vLLM STDOUT: INFO 09-25 08:33:54 [config.py:1472] Using max model len 2048\n",
            "vLLM STDOUT: WARNING 09-25 08:33:54 [arg_utils.py:1735] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
            "vLLM STDOUT: INFO 09-25 08:33:57 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "vLLM STDOUT: INFO 09-25 08:33:57 [api_server.py:268] Started engine process with PID 2299\n",
            "vLLM STDOUT: INFO 09-25 08:34:07 [__init__.py:244] Automatically detected platform cuda.\n",
            "vLLM STDOUT: INFO 09-25 08:34:10 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='unsloth/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":192,\"local_cache_dir\":null}, use_cached_outputs=True, \n",
            "vLLM STDOUT: INFO 09-25 08:34:12 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "vLLM STDOUT: INFO 09-25 08:34:12 [cuda.py:360] Using XFormers backend.\n",
            "vLLM STDOUT: INFO 09-25 08:34:13 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "vLLM STDOUT: INFO 09-25 08:34:13 [model_runner.py:1171] Starting to load model unsloth/Llama-3.2-3B-Instruct...\n",
            "vLLM STDOUT: INFO 09-25 08:34:14 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
            "vLLM STDOUT: INFO 09-25 08:39:36 [weight_utils.py:308] Time spent downloading weights for unsloth/Llama-3.2-3B-Instruct: 321.851414 seconds\n",
            "vLLM STDOUT: INFO 09-25 08:40:05 [default_loader.py:272] Loading weights took 24.70 seconds\n",
            "vLLM STDOUT: INFO 09-25 08:40:06 [model_runner.py:1203] Model loading took 6.0160 GiB and 351.462255 seconds\n",
            "vLLM STDOUT: INFO 09-25 08:40:08 [worker.py:294] Memory profiling takes 1.67 seconds\n",
            "vLLM STDOUT: INFO 09-25 08:40:08 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.89) = 13.18GiB\n",
            "vLLM STDOUT: INFO 09-25 08:40:08 [worker.py:294] model weights take 6.02GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 0.90GiB; the rest of the memory reserved for KV Cache is 6.22GiB.\n",
            "vLLM STDOUT: INFO 09-25 08:40:08 [executor_base.py:113] # cuda blocks: 3637, # CPU blocks: 0\n",
            "vLLM STDOUT: INFO 09-25 08:40:08 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 28.41x\n",
            "vLLM STDOUT: INFO 09-25 08:40:08 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "vLLM STDOUT: INFO 09-25 08:40:34 [model_runner.py:1671] Graph capturing finished in 26 secs, took 0.15 GiB\n",
            "vLLM STDOUT: INFO 09-25 08:40:34 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 28.28 seconds\n",
            "vLLM STDOUT: WARNING 09-25 08:40:34 [config.py:1392] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
            "vLLM STDOUT: INFO 09-25 08:40:34 [serving_chat.py:125] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [serving_completion.py:72] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [api_server.py:1457] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
            "vLLM Server Ready Detected\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:29] Available routes are:\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /docs, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /redoc, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /health, Methods: GET\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /load, Methods: GET\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /ping, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /ping, Methods: GET\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /tokenize, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /detokenize, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /v1/models, Methods: GET\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /version, Methods: GET\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /v1/chat/completions, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /v1/completions, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /v1/embeddings, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /pooling, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /classify, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /score, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /v1/score, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /v1/audio/translations, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /v1/rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /v2/rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /invocations, Methods: POST\n",
            "vLLM STDOUT: INFO 09-25 08:40:35 [launcher.py:37] Route: /metrics, Methods: GET\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39216 - \"GET /metrics HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "generator = SyntheticDataKit.from_pretrained(\n",
        "    # Choose any model from https://huggingface.co/unsloth\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = 2048, # Longer sequence lengths will be slower!\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eY4I1S4pRVl1"
      },
      "outputs": [],
      "source": [
        "generator.prepare_qa_generation(\n",
        "    output_folder = \"data\", # Output location of synthetic data\n",
        "    temperature = 0.7, # Higher temp makes more diverse datases\n",
        "    top_p = 0.95,\n",
        "    overlap = 64, # Overlap portion during chunking\n",
        "    max_generation_tokens = 512, # Can increase for longer QA pairs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqe2j9MGRWVk",
        "outputId": "85aab962-d202-4ff2-d153-4128f1c84b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:39226 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[?25l\u001b[32m VLLM server is running at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[32m⠋\u001b[0m\u001b[32m Checking VLLM server at http://localhost:8000/v1...\u001b[0m\r\u001b[2KAvailable models: \u001b[1m{\u001b[0m\u001b[32m'object'\u001b[0m: \u001b[32m'list'\u001b[0m, \u001b[32m'data'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model'\u001b[0m, \u001b[32m'created'\u001b[0m: \u001b[1;36m1758789636\u001b[0m, \n",
            "\u001b[32m'owned_by'\u001b[0m: \u001b[32m'vllm'\u001b[0m, \u001b[32m'root'\u001b[0m: \u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'parent'\u001b[0m: \u001b[3;35mNone\u001b[0m, \n",
            "\u001b[32m'max_model_len'\u001b[0m: \u001b[1;36m2048\u001b[0m, \u001b[32m'permission'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'modelperm-836372cf29c34ca2953745621762a2cd'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model_permission'\u001b[0m, \n",
            "\u001b[32m'created'\u001b[0m: \u001b[1;36m1758789636\u001b[0m, \u001b[32m'allow_create_engine'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_sampling'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_logprobs'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'allow_search_indices'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_view'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_fine_tuning'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'organization'\u001b[0m: \u001b[32m'*'\u001b[0m, \u001b[32m'group'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'is_blocking'\u001b[0m: \n",
            "\u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
            "\u001b[32m⠋\u001b[0m Checking VLLM server at http://localhost:8000/v1...\r\u001b[2K\u001b[32m⠋\u001b[0m Checking VLLM server at http://localhost:8000/v1...\n",
            "\u001b[?25h\r\u001b[1A\u001b[2K"
          ]
        }
      ],
      "source": [
        "!synthetic-data-kit system-check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk_ikUqrRv3J",
        "outputId": "da01b6f6-2af6-41c2-f431-1ef129a3b0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:39228 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39242 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/sherlock_5.txt...vLLM STDOUT: INFO 09-25 08:40:38 [chat_utils.py:444] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
            "vLLM STDOUT: INFO 09-25 08:40:38 [logger.py:43] Received request chatcmpl-34ad77e7c5d1478087d9517f0bfe1467: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSherlockiana : The Baker Street Journal, an Irregular Quarterly of Sherlockiana Sherlock Holmes and Sherlockiana Collection at the Harry Ransom Center Digital Collections The Universal Sherlock Holmes at the University of Minnesota<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:40:38 [engine.py:317] Added request chatcmpl-34ad77e7c5d1478087d9517f0bfe1467.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/sherlock_5.txt...vLLM STDOUT: INFO:     127.0.0.1:39246 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m⠏\u001b[0m Generating qa content from data/output/sherlock_5.txt...vLLM STDOUT: INFO 09-25 08:40:53 [logger.py:43] Received request chatcmpl-89662222b79d47238a648683ebf81774: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nSherlockiana : The Baker Street Journal, an Irregular Quarterly of Sherlockiana Sherlock Holmes and Sherlockiana Collection at the Harry Ransom Center Digital Collections The Universal Sherlock Holmes at the University of Minnesota<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:40:53 [engine.py:317] Added request chatcmpl-89662222b79d47238a648683ebf81774.\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/sherlock_5.txt...vLLM STDOUT: INFO:     127.0.0.1:38148 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 17 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_5_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_5_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/sherlock_5.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_5_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:35538 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:35554 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:41:10 [logger.py:43] Received request chatcmpl-a48e22cfca82447299994344ea9944ef: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe Dynamics of an Asteroid : The Dynamics of an Asteroid is a fictional book by Professor James Moriarty, the implacable foe of Sherlock Holmes. The only mention of it in Arthur Conan Doyle\\'s original Holmes stories is in The Valley of Fear (written in 1914, but set in 1888) when Holmes says of Moriarty: Is he not the celebrated author of The Dynamics of an Asteroid, a book which ascends to such rarefied heights of pure mathematics that it is said that there was no man in the scientific press capable of criticizing it? Participants in the \"Sherlockian game\", where Sherlock Holmes fans elaborate on elements within Doyle\\'s stories, have suggested other details about The Dynamics of an Asteroid.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:41:10 [engine.py:317] Added request chatcmpl-a48e22cfca82447299994344ea9944ef.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/sherlock_6.txt...vLLM STDOUT: INFO:     127.0.0.1:35564 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:41:14 [logger.py:43] Received request chatcmpl-4900699936974a44bc8c319a5bb5fa71: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nThe Dynamics of an Asteroid : The Dynamics of an Asteroid is a fictional book by Professor James Moriarty, the implacable foe of Sherlock Holmes. The only mention of it in Arthur Conan Doyle\\'s original Holmes stories is in The Valley of Fear (written in 1914, but set in 1888) when Holmes says of Moriarty: Is he not the celebrated author of The Dynamics of an Asteroid, a book which ascends to such rarefied heights of pure mathematics that it is said that there was no man in the scientific press capable of criticizing it? Participants in the \"Sherlockian game\", where Sherlock Holmes fans elaborate on elements within Doyle\\'s stories, have suggested other details about The Dynamics of an Asteroid.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:41:14 [engine.py:317] Added request chatcmpl-4900699936974a44bc8c319a5bb5fa71.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/sherlock_6.txt...vLLM STDOUT: INFO:     127.0.0.1:35580 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 16 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_6_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_6_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/sherlock_6.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_6_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:48518 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:48520 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:41:31 [logger.py:43] Received request chatcmpl-baf81e752c9b4cd3a006ea4278c97037: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAdaptations of Sherlock Holmes : The stories of Sherlock Holmes by Sir Arthur Conan Doyle have been very popular as adaptations for the stage, and later film, and still later television. The four volumes of the Universal Sherlock Holmes (1995) compiled by Ronald B. De Waal lists over 25,000 Holmes-related productions and products. They include the original writings, \"together with the translations of these tales into sixty-three languages, plus Braille and shorthand, the writings about the Writings or higher criticism, writings about Sherlockians and their societies, memorials and memorabilia, games, puzzles and quizzes, phonograph records, audio and video tapes, compact discs, laser discs, ballets, films, musicals, operettas, oratorios, plays, radio and television programs, parodies and pastiches, children\\'s books, cartoons, comics, and a multitude of other items — from advertisements to wine — that have accumulated throughout the world on the two most famous characters in literature.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:41:31 [engine.py:317] Added request chatcmpl-baf81e752c9b4cd3a006ea4278c97037.\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/sherlock_10.txt...vLLM STDOUT: INFO:     127.0.0.1:48526 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:41:35 [logger.py:43] Received request chatcmpl-debe8674e32949699ca41d2539aa56ea: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nAdaptations of Sherlock Holmes : The stories of Sherlock Holmes by Sir Arthur Conan Doyle have been very popular as adaptations for the stage, and later film, and still later television. The four volumes of the Universal Sherlock Holmes (1995) compiled by Ronald B. De Waal lists over 25,000 Holmes-related productions and products. They include the original writings, \"together with the translations of these tales into sixty-three languages, plus Braille and shorthand, the writings about the Writings or higher criticism, writings about Sherlockians and their societies, memorials and memorabilia, games, puzzles and quizzes, phonograph records, audio and video tapes, compact discs, laser discs, ballets, films, musicals, operettas, oratorios, plays, radio and television programs, parodies and pastiches, children\\'s books, cartoons, comics, and a multitude of other items — from advertisements to wine — that have accumulated throughout the world on the two most famous characters in literature.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:41:35 [engine.py:317] Added request chatcmpl-debe8674e32949699ca41d2539aa56ea.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/sherlock_10.txt...vLLM STDOUT: INFO:     127.0.0.1:48534 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 12 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_10_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_10_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/sherlock_10.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_10_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:60346 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:60350 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:41:52 [logger.py:43] Received request chatcmpl-03ddd684cb424f6da65f6acc199397b7: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe Dynamics of an Asteroid : In 1809, Carl Friedrich Gauss wrote a ground-breaking treatise on the dynamics of an asteroid (Ceres). However, Gauss\\'s method was understood immediately and is still used today. Two decades before Arthur Conan Doyle\\'s writing, the Canadian-American dynamic astronomer Simon Newcomb had published a series of books analyzing motions of planets in the Solar System. The notoriously spiteful Newcomb could have been an inspiration for Professor Moriarty. In 1887, Henri Poincaré\\'s submission to the celestial mechanics contest of King Oscar II of Sweden investigated the three-body problem, a theoretical basis of asteroid dynamics. It was also hard to criticize, as the jury (Weirstrass, Mittag-Leffler, and Hermite, all top-notch mathematicians) and the author himself missed a fatal error in the submission (later corrected). Another example of mathematics too abstruse to be criticized is the letters of Srinivasa Ramanujan, sent to several mathematicians at the University of Cambridge in 1913. Only one of these mathematicians, G. H. Hardy, even recognized their merit. Despite being experts in the branches of mathematics used, he and J. E. Littlewood added that many of them \"defeated me completely; I had never seen anything in the least like them before.\" It has taken over a century for this work to be understood; the last sub-field (and the last problem of the last sub-field) have been referred to as The Final Problem in explicit reference to the Sherlock Holmes story. Holmes only states that \"it is said\" (emphasis added) that no one in the scientific press was capable of criticizing Moriarty\\'s work; he stops short of recognizing the claim as indisputably accurate. Similarly, when it was jocularly suggested to Arthur Eddington in 1919 that he was one of only three people in the world who understood Albert Einstein\\'s theory of relativity, Eddington quipped that he could not think who the third person was.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:41:52 [engine.py:317] Added request chatcmpl-03ddd684cb424f6da65f6acc199397b7.\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/sherlock_7.txt...vLLM STDOUT: INFO:     127.0.0.1:60362 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:41:57 [logger.py:43] Received request chatcmpl-79c1893841c44ad6a530b082e3b4b6eb: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nThe Dynamics of an Asteroid : In 1809, Carl Friedrich Gauss wrote a ground-breaking treatise on the dynamics of an asteroid (Ceres). However, Gauss\\'s method was understood immediately and is still used today. Two decades before Arthur Conan Doyle\\'s writing, the Canadian-American dynamic astronomer Simon Newcomb had published a series of books analyzing motions of planets in the Solar System. The notoriously spiteful Newcomb could have been an inspiration for Professor Moriarty. In 1887, Henri Poincaré\\'s submission to the celestial mechanics contest of King Oscar II of Sweden investigated the three-body problem, a theoretical basis of asteroid dynamics. It was also hard to criticize, as the jury (Weirstrass, Mittag-Leffler, and Hermite, all top-notch mathematicians) and the author himself missed a fatal error in the submission (later corrected). Another example of mathematics too abstruse to be criticized is the letters of Srinivasa Ramanujan, sent to several mathematicians at the University of Cambridge in 1913. Only one of these mathematicians, G. H. Hardy, even recognized their merit. Despite being experts in the branches of mathematics used, he and J. E. Littlewood added that many of them \"defeated me completely; I had never seen anything in the least like them before.\" It has taken over a century for this work to be understood; the last sub-field (and the last problem of the last sub-field) have been referred to as The Final Problem in explicit reference to the Sherlock Holmes story. Holmes only states that \"it is said\" (emphasis added) that no one in the scientific press was capable of criticizing Moriarty\\'s work; he stops short of recognizing the claim as indisputably accurate. Similarly, when it was jocularly suggested to Arthur Eddington in 1919 that he was one of only three people in the world who understood Albert Einstein\\'s theory of relativity, Eddington quipped that he could not think who the third person was.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m⠴\u001b[0m Generating qa content from data/output/sherlock_7.txt...vLLM STDOUT: INFO 09-25 08:41:57 [engine.py:317] Added request chatcmpl-79c1893841c44ad6a530b082e3b4b6eb.\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/sherlock_7.txt...vLLM STDOUT: INFO:     127.0.0.1:60366 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 11 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_7_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_7_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/sherlock_7.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_7_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:45034 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:45042 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:42:14 [logger.py:43] Received request chatcmpl-f131b5d8b9404442881245bd4c2ca1dc: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSherlockiana : Non-canonical works of fiction featuring Sherlock Holmes, by creators other than Arthur Conan Doyle, have been referred to as examples of \"Sherlockiana\". Charles Spencer, former theatre critic for The Daily Telegraph, used the term to refer to the 2009–12 releases of the novel The House of Silk, the television series Sherlock, and two Sherlock Holmes films, Sherlock Holmes and its sequel Sherlock Holmes: A Game of Shadows, as representative of a \"golden age of Sherlockiana.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:42:14 [engine.py:317] Added request chatcmpl-f131b5d8b9404442881245bd4c2ca1dc.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/sherlock_1.txt...vLLM STDOUT: INFO:     127.0.0.1:45056 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:42:17 [logger.py:43] Received request chatcmpl-4ab755c2edf2441aa2322559ca44cfa6: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nSherlockiana : Non-canonical works of fiction featuring Sherlock Holmes, by creators other than Arthur Conan Doyle, have been referred to as examples of \"Sherlockiana\". Charles Spencer, former theatre critic for The Daily Telegraph, used the term to refer to the 2009–12 releases of the novel The House of Silk, the television series Sherlock, and two Sherlock Holmes films, Sherlock Holmes and its sequel Sherlock Holmes: A Game of Shadows, as representative of a \"golden age of Sherlockiana.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:42:17 [engine.py:317] Added request chatcmpl-4ab755c2edf2441aa2322559ca44cfa6.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/sherlock_1.txt...vLLM STDOUT: INFO:     127.0.0.1:45060 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 14 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_1_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_1_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/sherlock_1.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_1_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59956 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59962 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:42:35 [logger.py:43] Received request chatcmpl-51d380ccddd547a3b8ea865908108d39: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe Dynamics of an Asteroid : Doyle provided no indication of the contents of Dynamics other than its title. Speculation about its contents published by later authors includes: \"The Ultimate Crime\", short story by Isaac Asimov, in More Tales of the Black Widowers, and republished in Sherlock Holmes through Time and Space. \"The Dynamics of an Asteroid\", short story by Robert Bloch, The Baker Street Journal, 1953, and also found in The Game is Afoot. \"The Adventure of the Russian Grave\", a short story by William Barton and Michael Capobianco, collected in Sherlock Holmes in Orbit. In the novel Spider-Man: The Revenge of the Sinister Six, by Adam-Troy Castro, a veiled reference is made to Moriarty and his Dynamics. Here the work is said to still be the authority on orbital bombardment. Physicist Alejandro Jenkins in 2013 suggested chaos theory, an esoteric branch of mathematics whose association with asteroid dynamics was not appreciated by real-world mathematicians until the work of Henri Poincaré in 1890. Simon P. Norton and Alain Goriely have each suggested that the book might have been Moriarty\\'s submission to the 1887 celestial mechanics contest of King Oscar II of Sweden (which was won by Henri Poincaré.)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:42:35 [engine.py:317] Added request chatcmpl-51d380ccddd547a3b8ea865908108d39.\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/sherlock_8.txt...vLLM STDOUT: INFO:     127.0.0.1:59966 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:42:39 [logger.py:43] Received request chatcmpl-ed363d647ba448818ebb799e307cabd9: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nThe Dynamics of an Asteroid : Doyle provided no indication of the contents of Dynamics other than its title. Speculation about its contents published by later authors includes: \"The Ultimate Crime\", short story by Isaac Asimov, in More Tales of the Black Widowers, and republished in Sherlock Holmes through Time and Space. \"The Dynamics of an Asteroid\", short story by Robert Bloch, The Baker Street Journal, 1953, and also found in The Game is Afoot. \"The Adventure of the Russian Grave\", a short story by William Barton and Michael Capobianco, collected in Sherlock Holmes in Orbit. In the novel Spider-Man: The Revenge of the Sinister Six, by Adam-Troy Castro, a veiled reference is made to Moriarty and his Dynamics. Here the work is said to still be the authority on orbital bombardment. Physicist Alejandro Jenkins in 2013 suggested chaos theory, an esoteric branch of mathematics whose association with asteroid dynamics was not appreciated by real-world mathematicians until the work of Henri Poincaré in 1890. Simon P. Norton and Alain Goriely have each suggested that the book might have been Moriarty\\'s submission to the 1887 celestial mechanics contest of King Oscar II of Sweden (which was won by Henri Poincaré.)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:42:39 [engine.py:317] Added request chatcmpl-ed363d647ba448818ebb799e307cabd9.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/sherlock_8.txt...vLLM STDOUT: INFO:     127.0.0.1:39762 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 14 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_8_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_8_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/sherlock_8.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_8_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:58254 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:58268 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:42:57 [logger.py:43] Received request chatcmpl-e7411e78b971466b933fdd7ce0417d42: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSherlockiana : The interest in works about Sherlock Holmes has extended to intrigue by the United States Smithsonian Museums about the original location of 221B Baker Street. The investigation found that the supposed location of Holmes and Watson\\'s flat did not exist during the early stories such as A Study in Scarlet. However, in 1990, the Sherlock Holmes International Society opened up the Sherlock Holmes Museum at 221B Baker Street. Furthermore, statues of Holmes displays \"Sherlockian\" culture as idolizing elements of the world.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:42:57 [engine.py:317] Added request chatcmpl-e7411e78b971466b933fdd7ce0417d42.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/sherlock_4.txt...vLLM STDOUT: INFO:     127.0.0.1:58284 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:43:00 [logger.py:43] Received request chatcmpl-91f97fae7500413bb096a6111f5b1aab: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nSherlockiana : The interest in works about Sherlock Holmes has extended to intrigue by the United States Smithsonian Museums about the original location of 221B Baker Street. The investigation found that the supposed location of Holmes and Watson\\'s flat did not exist during the early stories such as A Study in Scarlet. However, in 1990, the Sherlock Holmes International Society opened up the Sherlock Holmes Museum at 221B Baker Street. Furthermore, statues of Holmes displays \"Sherlockian\" culture as idolizing elements of the world.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m⠏\u001b[0m Generating qa content from data/output/sherlock_4.txt...vLLM STDOUT: INFO 09-25 08:43:00 [engine.py:317] Added request chatcmpl-91f97fae7500413bb096a6111f5b1aab.\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/sherlock_4.txt...vLLM STDOUT: INFO:     127.0.0.1:37662 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 15 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_4_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_4_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/sherlock_4.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_4_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43516 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43530 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:43:17 [logger.py:43] Received request chatcmpl-b427f6978d774b25a3579f9ff6861715: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSherlockiana : Sherlockiana encompasses various categories of materials and content related to the fictional detective Sherlock Holmes, created by Arthur Conan Doyle. The word \"Sherlockiana\" has been used for literary studies and scholarship concerning Sherlock Holmes, Sherlock Holmes pastiches in print and other media such as films, and memorabilia associated with Sherlock Holmes. Sherlockiana may be \"anything about, inspired by, or tangentially concerning\" Sherlock Holmes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:43:17 [engine.py:317] Added request chatcmpl-b427f6978d774b25a3579f9ff6861715.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/sherlock_0.txt...vLLM STDOUT: INFO:     127.0.0.1:43546 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:43:20 [logger.py:43] Received request chatcmpl-c0ae65b80c3c446e9381f49a10a515fa: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nSherlockiana : Sherlockiana encompasses various categories of materials and content related to the fictional detective Sherlock Holmes, created by Arthur Conan Doyle. The word \"Sherlockiana\" has been used for literary studies and scholarship concerning Sherlock Holmes, Sherlock Holmes pastiches in print and other media such as films, and memorabilia associated with Sherlock Holmes. Sherlockiana may be \"anything about, inspired by, or tangentially concerning\" Sherlock Holmes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/sherlock_0.txt...vLLM STDOUT: INFO 09-25 08:43:20 [engine.py:317] Added request chatcmpl-c0ae65b80c3c446e9381f49a10a515fa.\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/sherlock_0.txt...vLLM STDOUT: INFO:     127.0.0.1:48600 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 16 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/sherlock_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:33370 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:33384 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:43:38 [logger.py:43] Received request chatcmpl-9cfd915d2e014a58aaf37e36945c2d8c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSherlockiana : The term \"Sherlockiana\" has been used to refer to objects connected to Sherlock Holmes. Collections of Sherlockiana may include audio-visual recordings, books, magazines, newspaper clippings, art, clothing, advertising, stationery, and any other items associated with Holmes. The University of Minnesota contains the world\\'s largest archive of Sherlockiana as of 2015, a large portion of which was bequeathed by American collector John Bennett Shaw upon his death in 1994.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:43:38 [engine.py:317] Added request chatcmpl-9cfd915d2e014a58aaf37e36945c2d8c.\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/sherlock_3.txt...vLLM STDOUT: INFO:     127.0.0.1:33390 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:43:41 [logger.py:43] Received request chatcmpl-236bae4e77bb47d4958d3b46f6950e97: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nSherlockiana : The term \"Sherlockiana\" has been used to refer to objects connected to Sherlock Holmes. Collections of Sherlockiana may include audio-visual recordings, books, magazines, newspaper clippings, art, clothing, advertising, stationery, and any other items associated with Holmes. The University of Minnesota contains the world\\'s largest archive of Sherlockiana as of 2015, a large portion of which was bequeathed by American collector John Bennett Shaw upon his death in 1994.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:43:41 [engine.py:317] Added request chatcmpl-236bae4e77bb47d4958d3b46f6950e97.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/sherlock_3.txt...vLLM STDOUT: INFO:     127.0.0.1:48210 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 15 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_3_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_3_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/sherlock_3.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_3_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:36334 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:36346 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:43:59 [logger.py:43] Received request chatcmpl-daf330cfc1a342a1aaf62ee9ab416311: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSherlockiana : When used to refer to literary studies, \"Sherlockiana\" includes essays and works about Sherlock Holmes such as Vincent Starrett\\'s 1933 book The Private Life of Sherlock Holmes. Some of these studies concern the Sherlockian game, a pastime of attempting to resolve anomalies and clarify implied details about Holmes and Watson. The word is used in the title of The Encyclopaedia Sherlockiana, first published in 1977 and republished as The Ultimate Sherlock Holmes Encyclopedia in 1987, a reference text containing an exhaustive list of over 3,500 people, places, and things associated with the universe of Sherlock Holmes. The quarterly journal The Baker Street Journal is subtitled An Irregular Quarterly of Sherlockiana.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:43:59 [engine.py:317] Added request chatcmpl-daf330cfc1a342a1aaf62ee9ab416311.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/sherlock_2.txt...vLLM STDOUT: INFO:     127.0.0.1:36360 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:44:02 [logger.py:43] Received request chatcmpl-2f128fc7f3364cd3904b3c2945605952: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nSherlockiana : When used to refer to literary studies, \"Sherlockiana\" includes essays and works about Sherlock Holmes such as Vincent Starrett\\'s 1933 book The Private Life of Sherlock Holmes. Some of these studies concern the Sherlockian game, a pastime of attempting to resolve anomalies and clarify implied details about Holmes and Watson. The word is used in the title of The Encyclopaedia Sherlockiana, first published in 1977 and republished as The Ultimate Sherlock Holmes Encyclopedia in 1987, a reference text containing an exhaustive list of over 3,500 people, places, and things associated with the universe of Sherlock Holmes. The quarterly journal The Baker Street Journal is subtitled An Irregular Quarterly of Sherlockiana.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:44:02 [engine.py:317] Added request chatcmpl-2f128fc7f3364cd3904b3c2945605952.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/sherlock_2.txt...vLLM STDOUT: INFO:     127.0.0.1:36366 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 16 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_2_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_2_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/sherlock_2.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_2_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54664 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54670 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:44:20 [logger.py:43] Received request chatcmpl-6b278c23a6114659b8dcf99e82cf6c28: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe Dynamics of an Asteroid : List of many references to Dynamics, as well as other works of Moriarty, Holmes, and others.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:44:20 [engine.py:317] Added request chatcmpl-6b278c23a6114659b8dcf99e82cf6c28.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/sherlock_9.txt...vLLM STDOUT: INFO:     127.0.0.1:54682 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m⠏\u001b[0m Generating qa content from data/output/sherlock_9.txt...vLLM STDOUT: INFO 09-25 08:44:25 [logger.py:43] Received request chatcmpl-493525d97ea1437784bb461e59ef695e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nThe Dynamics of an Asteroid : List of many references to Dynamics, as well as other works of Moriarty, Holmes, and others.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:44:25 [engine.py:317] Added request chatcmpl-493525d97ea1437784bb461e59ef695e.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/sherlock_9.txt...vLLM STDOUT: INFO:     127.0.0.1:54698 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 19 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_9_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_9_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/sherlock_9.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_9_qa_pairs.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Process chunks\n",
        "for filename in filenames:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create {filename} \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\"\n",
        "    time.sleep(2) # Sleep some time to leave some room for processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD-qc85VsRCT",
        "outputId": "cd8f7a82-4f10-4558-e8da-b14ed5273e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:56882 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:56898 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:44:42 [logger.py:43] Received request chatcmpl-f1895ef507f340e6b2eb87864a987a6e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What does Sherlockiana encompass?\",\\n    \"answer\": \"various categories of materials and content related to the fictional detective Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Who created the fictional detective Sherlock Holmes?\",\\n    \"answer\": \"Arthur Conan Doyle\"\\n  },\\n  {\\n    \"question\": \"What is Sherlockiana used for?\",\\n    \"answer\": \"literary studies and scholarship concerning Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Where is Sherlockiana used besides literary studies?\",\\n    \"answer\": \"Sherlock Holmes pastiches in print and other media such as films\"\\n  },\\n  {\\n    \"question\": \"What kind of memorabilia is associated with Sherlock Holmes?\",\\n    \"answer\": \"memorabilia associated with Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"What can be considered part of Sherlockiana?\",\\n    \"answer\": \"anything about, inspired by, or tangentially concerning Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"What is a Sherlock Holmes pastiche?\",\\n    \"answer\": \"not mentioned in the text\"\\n  },\\n  {\\n    \"question\": \"What is the text primarily about?\",\\n    \"answer\": \"Sherlockiana\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:44:42 [engine.py:317] Added request chatcmpl-f1895ef507f340e6b2eb87864a987a6e.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from data/generated/sherlock_0_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:56904 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:44:51 [logger.py:43] Received request chatcmpl-5540158be74a4c4c864a34b56c97c4ca: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Who wrote about Sherlock Holmes?\",\\n    \"answer\": \"Arthur Conan Doyle\"\\n  },\\n  {\\n    \"question\": \"What is the main focus of the text?\",\\n    \"answer\": \"Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"What is the definition of Sherlockiana?\",\\n    \"answer\": \"not explicitly stated in the text\"\\n  },\\n  {\\n    \"question\": \"What is the purpose of using the term \\'Sherlockiana\\'?\",\\n    \"answer\": \"to describe materials and content related to Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"What is a Sherlock Holmes pastiche?\",\\n    \"answer\": \"not mentioned in the text\"\\n  },\\n  {\\n    \"question\": \"How is Sherlockiana related to the media?\",\\n    \"answer\": \"Sherlock Holmes pastiches in print and other media such as films\"\\n  },\\n  {\\n    \"question\": \"What type of content is associated with Sherlock Holmes?\",\\n    \"answer\": \"memorabilia associated with Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"What is the range of topics covered by Sherlockiana?\",\\n    \"answer\": \"anything about, inspired by, or tangentially concerning Sherlock Holmes\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:44:51 [engine.py:317] Added request chatcmpl-5540158be74a4c4c864a34b56c97c4ca.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from data/generated/sherlock_0_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:44214 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 16 QA pairs\n",
            "\u001b[2KRetained 16 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.4\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from data/generated/sherlock_0_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_0_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:47058 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:47064 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:45:01 [logger.py:43] Received request chatcmpl-95080ab2b6a84340b5809e3a64b97944: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the term used to refer to non-canonical works of fiction featuring Sherlock Holmes?\",\\n    \"answer\": \"Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"Who used the term \\'Sherlockiana\\' to refer to non-canonical works?\",\\n    \"answer\": \"Charles Spencer\"\\n  },\\n  {\\n    \"question\": \"What works were referred to as representative of a \\'golden age of Sherlockiana\\'?\",\\n    \"answer\": \"The House of Silk, Sherlock, Sherlock Holmes and Sherlock Holmes: A Game of Shadows\"\\n  },\\n  {\\n    \"question\": \"In what years did the term \\'Sherlockiana\\' refer to the mentioned works?\",\\n    \"answer\": \"2009\\\\u201312\"\\n  },\\n  {\\n    \"question\": \"Who was the theatre critic for The Daily Telegraph?\",\\n    \"answer\": \"Charles Spencer\"\\n  },\\n  {\\n    \"question\": \"What publication was Charles Spencer a theatre critic for?\",\\n    \"answer\": \"The Daily Telegraph\"\\n  },\\n  {\\n    \"question\": \"What is the name of the novel that was part of the \\'golden age of Sherlockiana\\'?\",\\n    \"answer\": \"The House of Silk\"\\n  },\\n  {\\n    \"question\": \"What is the name of the television series that was part of the \\'golden age of Sherlockiana\\'?\",\\n    \"answer\": \"Sherlock\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:45:01 [engine.py:317] Added request chatcmpl-95080ab2b6a84340b5809e3a64b97944.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from data/generated/sherlock_1_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:47068 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from data/generated/sherlock_1_qa_pairs.json...vLLM STDOUT: INFO 09-25 08:45:11 [logger.py:43] Received request chatcmpl-32a7a3bf46fc4ca7876a4bd55fa1bbf8: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"How many Sherlock Holmes films were part of the \\'golden age of Sherlockiana\\'?\",\\n    \"answer\": \"2\"\\n  },\\n  {\\n    \"question\": \"What is the name of the first Sherlock Holmes film that was part of the \\'golden age of Sherlockiana\\'?\",\\n    \"answer\": \"Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"What is the name of the sequel to the first Sherlock Holmes film?\",\\n    \"answer\": \"Sherlock Holmes: A Game of Shadows\"\\n  },\\n  {\\n    \"question\": \"What is the term used to describe the works mentioned?\",\\n    \"answer\": \"Non-canonical works of fiction featuring Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Who referred to the works as non-canonical?\",\\n    \"answer\": \"Creators other than Arthur Conan Doyle\"\\n  },\\n  {\\n    \"question\": \"What is the term used to describe these works?\",\\n    \"answer\": \"Non-canonical\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:45:11 [engine.py:317] Added request chatcmpl-32a7a3bf46fc4ca7876a4bd55fa1bbf8.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from data/generated/sherlock_1_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:41874 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 14 QA pairs\n",
            "\u001b[2KRetained 14 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.4\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from data/generated/sherlock_1_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_1_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39810 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39826 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:45:19 [logger.py:43] Received request chatcmpl-ff04e89633c24a018a5f55ce862e50a5: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is Sherlockiana?\",\\n    \"answer\": \"Literary studies of Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Who wrote the book The Private Life of Sherlock Holmes?\",\\n    \"answer\": \"Vincent Starrett\"\\n  },\\n  {\\n    \"question\": \"When was The Encyclopaedia Sherlockiana first published?\",\\n    \"answer\": \"1977\"\\n  },\\n  {\\n    \"question\": \"What is the Baker Street Journal?\",\\n    \"answer\": \"A quarterly journal subtitled An Irregular Quarterly of Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"What is the purpose of the Sherlockian game?\",\\n    \"answer\": \"To resolve anomalies and clarify implied details about Holmes and Watson\"\\n  },\\n  {\\n    \"question\": \"How many people, places, and things are listed in The Ultimate Sherlock Holmes Encyclopedia?\",\\n    \"answer\": \"Over 3,500\"\\n  },\\n  {\\n    \"question\": \"When was The Ultimate Sherlock Holmes Encyclopedia republished?\",\\n    \"answer\": \"1987\"\\n  },\\n  {\\n    \"question\": \"What is The Baker Street Journal subtitled?\",\\n    \"answer\": \"An Irregular Quarterly of Sherlockiana\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:45:19 [engine.py:317] Added request chatcmpl-ff04e89633c24a018a5f55ce862e50a5.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from data/generated/sherlock_2_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:39830 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:45:27 [logger.py:43] Received request chatcmpl-5d3af604d1054d359143da2cb93b8fc2: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"When was The Baker Street Journal first published?\",\\n    \"answer\": \"Not specified in the text\"\\n  },\\n  {\\n    \"question\": \"What is The Baker Street Journal?\",\\n    \"answer\": \"A quarterly journal\"\\n  },\\n  {\\n    \"question\": \"What is The Encyclopaedia Sherlockiana?\",\\n    \"answer\": \"A reference text\"\\n  },\\n  {\\n    \"question\": \"When was The Encyclopaedia Sherlockiana republished?\",\\n    \"answer\": \"1987\"\\n  },\\n  {\\n    \"question\": \"Who republished The Ultimate Sherlock Holmes Encyclopedia?\",\\n    \"answer\": \"Not specified in the text\"\\n  },\\n  {\\n    \"question\": \"What is The Private Life of Sherlock Holmes?\",\\n    \"answer\": \"A book about Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"When was The Private Life of Sherlock Holmes published?\",\\n    \"answer\": \"1933\"\\n  },\\n  {\\n    \"question\": \"What is The Ultimate Sherlock Holmes Encyclopedia?\",\\n    \"answer\": \"A reference text containing an exhaustive list of over 3,500 people, places, and things\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from data/generated/sherlock_2_qa_pairs.json...vLLM STDOUT: INFO 09-25 08:45:27 [engine.py:317] Added request chatcmpl-5d3af604d1054d359143da2cb93b8fc2.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from data/generated/sherlock_2_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:39842 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 16 QA pairs\n",
            "\u001b[2KRetained 16 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.2\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from data/generated/sherlock_2_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_2_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39360 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39368 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:45:36 [logger.py:43] Received request chatcmpl-b2d5482c13b9419386fdf4e01c68d065: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is Sherlockiana?\",\\n    \"answer\": \"Objects connected to Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"What types of items can be part of a collection of Sherlockiana?\",\\n    \"answer\": \"Audio-visual recordings, books, magazines, newspaper clippings, art, clothing, advertising, stationery\"\\n  },\\n  {\\n    \"question\": \"What is the University of Minnesota\\'s collection of Sherlockiana?\",\\n    \"answer\": \"The world\\'s largest archive of Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"Who donated a significant portion of the University of Minnesota\\'s Sherlockiana collection?\",\\n    \"answer\": \"John Bennett Shaw\"\\n  },\\n  {\\n    \"question\": \"When did John Bennett Shaw pass away?\",\\n    \"answer\": \"1994\"\\n  },\\n  {\\n    \"question\": \"When was the information about the University of Minnesota\\'s Sherlockiana collection available?\",\\n    \"answer\": \"2015\"\\n  },\\n  {\\n    \"question\": \"What was bequeathed by John Bennett Shaw to the University of Minnesota?\",\\n    \"answer\": \"A large portion of his collection of Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"What is the purpose of the term \\'Sherlockiana\\'?\",\\n    \"answer\": \"To refer to objects connected to Sherlock Holmes\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:45:36 [engine.py:317] Added request chatcmpl-b2d5482c13b9419386fdf4e01c68d065.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from data/generated/sherlock_3_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:39378 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:45:45 [logger.py:43] Received request chatcmpl-838c29a92d8d4a6b9eeb0975d7b63729: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What types of media can be part of a collection of Sherlockiana?\",\\n    \"answer\": \"Audio-visual recordings, books, magazines\"\\n  },\\n  {\\n    \"question\": \"What is included in a collection of Sherlockiana, according to the text?\",\\n    \"answer\": \"Any items associated with Holmes\"\\n  },\\n  {\\n    \"question\": \"Where is the University of Minnesota\\'s collection of Sherlockiana located?\",\\n    \"answer\": \"Not specified in the text\"\\n  },\\n  {\\n    \"question\": \"What year was the University of Minnesota\\'s Sherlockiana collection evaluated?\",\\n    \"answer\": \"2015\"\\n  },\\n  {\\n    \"question\": \"What is the significance of John Bennett Shaw\\'s donation to the University of Minnesota?\",\\n    \"answer\": \"The University of Minnesota contains the world\\'s largest archive of Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"What is the date of John Bennett Shaw\\'s death?\",\\n    \"answer\": \"1994\"\\n  },\\n  {\\n    \"question\": \"Who is John Bennett Shaw?\",\\n    \"answer\": \"An American collector of Sherlockiana\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from data/generated/sherlock_3_qa_pairs.json...vLLM STDOUT: INFO 09-25 08:45:45 [engine.py:317] Added request chatcmpl-838c29a92d8d4a6b9eeb0975d7b63729.\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from data/generated/sherlock_3_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:45920 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 15 QA pairs\n",
            "\u001b[2KRetained 15 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.3\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from data/generated/sherlock_3_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_3_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59658 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59666 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:45:54 [logger.py:43] Received request chatcmpl-90f2cf0cf3c2460fb26ed2e1ff3b17d3: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What institution is intrigued by the original location of 221B Baker Street?\",\\n    \"answer\": \"The United States Smithsonian Museums\"\\n  },\\n  {\\n    \"question\": \"When did the Sherlock Holmes International Society open the Sherlock Holmes Museum at 221B Baker Street?\",\\n    \"answer\": \"1990\"\\n  },\\n  {\\n    \"question\": \"What was found about the supposed location of Holmes and Watson\\'s flat during the investigation?\",\\n    \"answer\": \"It did not exist during the early stories such as A Study in Scarlet\"\\n  },\\n  {\\n    \"question\": \"What type of culture is represented by statues of Holmes?\",\\n    \"answer\": \"Sherlockian culture\"\\n  },\\n  {\\n    \"question\": \"What elements of the world are idolized in Sherlockian culture?\",\\n    \"answer\": \"Elements of the world\"\\n  },\\n  {\\n    \"question\": \"Where is the Sherlock Holmes Museum located?\",\\n    \"answer\": \"221B Baker Street\"\\n  },\\n  {\\n    \"question\": \"Is the supposed location of Holmes and Watson\\'s flat mentioned in the early stories?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"What type of stories mention the supposed location of Holmes and Watson\\'s flat?\",\\n    \"answer\": \"The early stories such as A Study in Scarlet\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:45:54 [engine.py:317] Added request chatcmpl-90f2cf0cf3c2460fb26ed2e1ff3b17d3.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from data/generated/sherlock_4_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:59680 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:04 [logger.py:43] Received request chatcmpl-29ef8ffe6c6a4d53b8c3c093b0f829d5: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Is the Sherlock Holmes Museum mentioned as existing at 221B Baker Street?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Is the United States Smithsonian Museums mentioned as being from the US?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Is the Sherlock Holmes International Society mentioned as being international?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Is the location of Holmes and Watson\\'s flat mentioned as not existing during the early stories?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Is Sherlockian culture mentioned as being idolizing elements of the world?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Is the Sherlock Holmes Museum mentioned as being opened by the Sherlock Holmes International Society?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Is 221B Baker Street mentioned as being the location of the Sherlock Holmes Museum?\",\\n    \"answer\": \"Yes\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:46:04 [engine.py:317] Added request chatcmpl-29ef8ffe6c6a4d53b8c3c093b0f829d5.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from data/generated/sherlock_4_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:48234 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 15 QA pairs\n",
            "\u001b[2KRetained 15 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.4\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Cleaning content from data/generated/sherlock_4_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_4_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:33470 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:33482 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:12 [logger.py:43] Received request chatcmpl-cc9b68d9440048a88a40efd9a333f7c3: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the title of the Sherlockiana publication?\",\\n    \"answer\": \"The Baker Street Journal\"\\n  },\\n  {\\n    \"question\": \"How often is The Baker Street Journal published?\",\\n    \"answer\": \"Irregularly\"\\n  },\\n  {\\n    \"question\": \"Where is the Sherlock Holmes and Sherlockiana Collection housed?\",\\n    \"answer\": \"at the Harry Ransom Center\"\\n  },\\n  {\\n    \"question\": \"What digital collection is available for Sherlockiana?\",\\n    \"answer\": \"Digital Collections\"\\n  },\\n  {\\n    \"question\": \"What is the name of the digital collection of Sherlockiana?\",\\n    \"answer\": \"The Universal Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Where is The Universal Sherlock Holmes housed?\",\\n    \"answer\": \"at the University of Minnesota\"\\n  },\\n  {\\n    \"question\": \"What type of publication is The Baker Street Journal?\",\\n    \"answer\": \"Journal\"\\n  },\\n  {\\n    \"question\": \"What is the focus of The Baker Street Journal?\",\\n    \"answer\": \"Sherlockiana\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:46:12 [engine.py:317] Added request chatcmpl-cc9b68d9440048a88a40efd9a333f7c3.\n",
            "\u001b[?25lProcessing 3 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Cleaning content from data/generated/sherlock_5_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:33490 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:21 [logger.py:43] Received request chatcmpl-5d03e4f3a77e4f8cbcedf538d59f9bf2: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the frequency of publication for The Baker Street Journal?\",\\n    \"answer\": \"Quarterly\"\\n  },\\n  {\\n    \"question\": \"Where is the Sherlock Holmes and Sherlockiana Collection at the Harry Ransom Center located?\",\\n    \"answer\": \"Not specified\"\\n  },\\n  {\\n    \"question\": \"What type of collection is the Sherlock Holmes and Sherlockiana Collection at the Harry Ransom Center?\",\\n    \"answer\": \"Collection\"\\n  },\\n  {\\n    \"question\": \"Where is the Universal Sherlock Holmes at the University of Minnesota located?\",\\n    \"answer\": \"Not specified\"\\n  },\\n  {\\n    \"question\": \"What type of publication is The Universal Sherlock Holmes?\",\\n    \"answer\": \"Publication\"\\n  },\\n  {\\n    \"question\": \"What is the focus of The Universal Sherlock Holmes?\",\\n    \"answer\": \"Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"What type of collection is the Digital Collections at the Harry Ransom Center?\",\\n    \"answer\": \"Digital\"\\n  },\\n  {\\n    \"question\": \"What is the focus of the Digital Collections at the Harry Ransom Center?\",\\n    \"answer\": \"Sherlockiana\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:46:21 [engine.py:317] Added request chatcmpl-5d03e4f3a77e4f8cbcedf538d59f9bf2.\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Cleaning content from data/generated/sherlock_5_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:36632 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:30 [logger.py:43] Received request chatcmpl-eca05a1344a64bba9e1aca27c1087f9a: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What type of publication is The Baker Street Journal?\",\\n    \"answer\": \"Publication\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from data/generated/sherlock_5_qa_pairs.json...vLLM STDOUT: INFO 09-25 08:46:30 [engine.py:317] Added request chatcmpl-eca05a1344a64bba9e1aca27c1087f9a.\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from data/generated/sherlock_5_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:54396 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 17 QA pairs\n",
            "\u001b[2KRetained 17 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.2\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from data/generated/sherlock_5_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_5_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54410 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54424 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:32 [logger.py:43] Received request chatcmpl-03ab56e4400b4740922214e00a5a9c74: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Who wrote The Dynamics of an Asteroid?\",\\n    \"answer\": \"Professor James Moriarty\"\\n  },\\n  {\\n    \"question\": \"In which year was The Dynamics of an Asteroid written?\",\\n    \"answer\": \"1914\"\\n  },\\n  {\\n    \"question\": \"In which year was The Dynamics of an Asteroid set?\",\\n    \"answer\": \"1888\"\\n  },\\n  {\\n    \"question\": \"What is the subject of The Dynamics of an Asteroid?\",\\n    \"answer\": \"Pure mathematics\"\\n  },\\n  {\\n    \"question\": \"According to the scientific press, who was the only man capable of criticizing The Dynamics of an Asteroid?\",\\n    \"answer\": \"There was no man in the scientific press capable of criticizing it\"\\n  },\\n  {\\n    \"question\": \"In which Sherlock Holmes story is The Dynamics of an Asteroid mentioned?\",\\n    \"answer\": \"The Valley of Fear\"\\n  },\\n  {\\n    \"question\": \"Who is the author of the Sherlockian game where details about The Dynamics of an Asteroid are elaborated?\",\\n    \"answer\": \"Sherlock Holmes fans\"\\n  },\\n  {\\n    \"question\": \"What is The Dynamics of an Asteroid said to ascend to?\",\\n    \"answer\": \"Rarefied heights\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:46:32 [engine.py:317] Added request chatcmpl-03ab56e4400b4740922214e00a5a9c74.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from data/generated/sherlock_6_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:54432 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:41 [logger.py:43] Received request chatcmpl-8bc91f7b57aa4b2d994f41a496413fa4: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Who is the author of The Dynamics of an Asteroid according to Arthur Conan Doyle?\",\\n    \"answer\": \"Professor James Moriarty\"\\n  },\\n  {\\n    \"question\": \"What is The Dynamics of an Asteroid compared to?\",\\n    \"answer\": \"A book\"\\n  },\\n  {\\n    \"question\": \"Who is the protagonist of the Sherlockian game?\",\\n    \"answer\": \"Sherlock Holmes fans\"\\n  },\\n  {\\n    \"question\": \"What year was The Valley of Fear written?\",\\n    \"answer\": \"1914\"\\n  },\\n  {\\n    \"question\": \"Who is the author of the original Holmes stories?\",\\n    \"answer\": \"Arthur Conan Doyle\"\\n  },\\n  {\\n    \"question\": \"Who is the main character in The Valley of Fear?\",\\n    \"answer\": \"Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Who is the main character in The Dynamics of an Asteroid?\",\\n    \"answer\": \"Professor James Moriarty\"\\n  },\\n  {\\n    \"question\": \"What genre does The Dynamics of an Asteroid belong to?\",\\n    \"answer\": \"Fictional book\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:46:41 [engine.py:317] Added request chatcmpl-8bc91f7b57aa4b2d994f41a496413fa4.\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from data/generated/sherlock_6_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:49568 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 16 QA pairs\n",
            "\u001b[2KRetained 7 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 4.2\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from data/generated/sherlock_6_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_6_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:60260 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:60266 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:51 [logger.py:43] Received request chatcmpl-87bddf020fe74aa3981a53ef75333f74: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"In what year was Carl Friedrich Gauss\\'s treatise on the dynamics of an asteroid published?\",\\n    \"answer\": \"1809\"\\n  },\\n  {\\n    \"question\": \"Who published a series of books analyzing motions of planets in the Solar System two decades before Arthur Conan Doyle\\'s writing?\",\\n    \"answer\": \"Simon Newcomb\"\\n  },\\n  {\\n    \"question\": \"In what year did Henri Poincar\\\\u00e9 submit his work to the celestial mechanics contest of King Oscar II of Sweden?\",\\n    \"answer\": \"1887\"\\n  },\\n  {\\n    \"question\": \"What was the title of the submission that Henri Poincar\\\\u00e9 investigated in the celestial mechanics contest?\",\\n    \"answer\": \"The three-body problem\"\\n  },\\n  {\\n    \"question\": \"In what year did Srinivasa Ramanujan send letters to several mathematicians at the University of Cambridge?\",\\n    \"answer\": \"1913\"\\n  },\\n  {\\n    \"question\": \"Who was the only mathematician to recognize the merit of Srinivasa Ramanujan\\'s letters?\",\\n    \"answer\": \"G. H. Hardy\"\\n  },\\n  {\\n    \"question\": \"How did G. H. Hardy and J. E. Littlewood respond to Srinivasa Ramanujan\\'s letters?\",\\n    \"answer\": \"They said that many of the letters \\'defeated me completely; I had never seen anything in the least like them before.\\'\"\\n  },\\n  {\\n    \"question\": \"What was referred to as \\'The Final Problem\\' in reference to Sherlock Holmes?\",\\n    \"answer\": \"The last sub-field (and the last problem of the last sub-field) of mathematics\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:46:51 [engine.py:317] Added request chatcmpl-87bddf020fe74aa3981a53ef75333f74.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from data/generated/sherlock_7_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:60274 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:03 [logger.py:43] Received request chatcmpl-38812f1384504ec8a1b839adc82bbaff: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Who was the subject of a comment by Arthur Eddington in 1919?\",\\n    \"answer\": \"Albert Einstein\"\\n  },\\n  {\\n    \"question\": \"How did Arthur Eddington respond when it was suggested that he was one of only three people in the world who understood Albert Einstein\\'s theory of relativity?\",\\n    \"answer\": \"He quipped that he could not think who the third person was\"\\n  },\\n  {\\n    \"question\": \"Who was the author of the Sherlock Holmes story \\'The Final Problem\\'?\",\\n    \"answer\": \"Arthur Conan Doyle\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:03 [engine.py:317] Added request chatcmpl-38812f1384504ec8a1b839adc82bbaff.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from data/generated/sherlock_7_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:60668 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 11 QA pairs\n",
            "\u001b[2KRetained 11 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.5\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from data/generated/sherlock_7_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_7_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55902 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55908 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:08 [logger.py:43] Received request chatcmpl-6be935e142724fbab38c9a84d88eac6a: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Who wrote the short story \\'The Ultimate Crime\\'?\",\\n    \"answer\": \"Isaac Asimov\"\\n  },\\n  {\\n    \"question\": \"What is the title of the short story by Robert Bloch?\",\\n    \"answer\": \"The Dynamics of an Asteroid\"\\n  },\\n  {\\n    \"question\": \"In what year was Henri Poincar\\\\u00e9\\'s work on chaos theory appreciated?\",\\n    \"answer\": \"1890\"\\n  },\\n  {\\n    \"question\": \"Who won the 1887 celestial mechanics contest?\",\\n    \"answer\": \"Henri Poincar\\\\u00e9\"\\n  },\\n  {\\n    \"question\": \"Who wrote the novel \\'Spider-Man: The Revenge of the Sinister Six\\'?\",\\n    \"answer\": \"Adam-Troy Castro\"\\n  },\\n  {\\n    \"question\": \"What is the title of the collection of Sherlock Holmes short stories by William Barton and Michael Capobianco?\",\\n    \"answer\": \"Sherlock Holmes in Orbit\"\\n  },\\n  {\\n    \"question\": \"Who suggested that chaos theory might be associated with asteroid dynamics?\",\\n    \"answer\": \"Alejandro Jenkins\"\\n  },\\n  {\\n    \"question\": \"Who won the 1887 celestial mechanics contest?\",\\n    \"answer\": \"Henri Poincar\\\\u00e9\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:08 [engine.py:317] Added request chatcmpl-6be935e142724fbab38c9a84d88eac6a.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from data/generated/sherlock_8_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:55922 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:18 [logger.py:43] Received request chatcmpl-19b568c707b44605a7636657619b80d8: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Who wrote the short story \\'The Adventure of the Russian Grave\\'?\",\\n    \"answer\": \"William Barton and Michael Capobianco\"\\n  },\\n  {\\n    \"question\": \"What is the title of the Sherlock Holmes journal where Robert Bloch\\'s short story was published?\",\\n    \"answer\": \"The Baker Street Journal\"\\n  },\\n  {\\n    \"question\": \"What year did Simon P. Norton and Alain Goriely suggest that the book might have been Moriarty\\'s submission?\",\\n    \"answer\": \"Not specified\"\\n  },\\n  {\\n    \"question\": \"Who wrote the short story \\'The Ultimate Crime\\'?\",\\n    \"answer\": \"Isaac Asimov\"\\n  },\\n  {\\n    \"question\": \"What is the title of the Sherlock Holmes through Time and Space collection?\",\\n    \"answer\": \"More Tales of the Black Widowers\"\\n  },\\n  {\\n    \"question\": \"Who wrote the novel \\'The Game is Afoot\\'?\",\\n    \"answer\": \"Not specified\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:18 [engine.py:317] Added request chatcmpl-19b568c707b44605a7636657619b80d8.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Cleaning content from data/generated/sherlock_8_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:38948 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 14 QA pairs\n",
            "\u001b[2KRetained 14 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 7.7\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Cleaning content from data/generated/sherlock_8_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_8_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:38964 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:38980 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:26 [logger.py:43] Received request chatcmpl-d67cdbd02323473cb91a8662c8a54437: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the topic of the text?\",\\n    \"answer\": \"The Dynamics of an Asteroid\"\\n  },\\n  {\\n    \"question\": \"What type of references are listed in the text?\",\\n    \"answer\": \"List of many references to Dynamics\"\\n  },\\n  {\\n    \"question\": \"Who are some authors mentioned in the text?\",\\n    \"answer\": \"Moriarty, Holmes, and others\"\\n  },\\n  {\\n    \"question\": \"What is the main subject of the text?\",\\n    \"answer\": \"An Asteroid\"\\n  },\\n  {\\n    \"question\": \"What is being referenced by the title?\",\\n    \"answer\": \"The Dynamics\"\\n  },\\n  {\\n    \"question\": \"What type of work is the text referencing?\",\\n    \"answer\": \"Other works\"\\n  },\\n  {\\n    \"question\": \"Is the text a study on Dynamics?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"What is the main subject of the study?\",\\n    \"answer\": \"An Asteroid\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:26 [engine.py:317] Added request chatcmpl-d67cdbd02323473cb91a8662c8a54437.\n",
            "\u001b[?25lProcessing 3 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from data/generated/sherlock_9_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:38988 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:34 [logger.py:43] Received request chatcmpl-f5f5b59f705a47fe99b3f82cacc175e5: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Is the text a general information piece?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Who wrote the text?\",\\n    \"answer\": \"Not specified\"\\n  },\\n  {\\n    \"question\": \"Is the text academic?\",\\n    \"answer\": \"Not specified\"\\n  },\\n  {\\n    \"question\": \"What type of text is this?\",\\n    \"answer\": \"Introduction or List of References\"\\n  },\\n  {\\n    \"question\": \"What is the purpose of the text?\",\\n    \"answer\": \"To list references and provide context\"\\n  },\\n  {\\n    \"question\": \"Is the text a primary source?\",\\n    \"answer\": \"No\"\\n  },\\n  {\\n    \"question\": \"What type of information is provided in the text?\",\\n    \"answer\": \"General information about Dynamics and other works\"\\n  },\\n  {\\n    \"question\": \"Is the text a comprehensive overview?\",\\n    \"answer\": \"Yes\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:34 [engine.py:317] Added request chatcmpl-f5f5b59f705a47fe99b3f82cacc175e5.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from data/generated/sherlock_9_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:39338 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:42 [logger.py:43] Received request chatcmpl-ce00c9ab264946f4a83e05b3e85b0197: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What type of content is the text about?\",\\n    \"answer\": \"References to Dynamics and other works\"\\n  },\\n  {\\n    \"question\": \"Is the text a study on a specific topic?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"What type of information is directly referenced in the text?\",\\n    \"answer\": \"Dynamics\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:42 [engine.py:317] Added request chatcmpl-ce00c9ab264946f4a83e05b3e85b0197.\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Cleaning content from data/generated/sherlock_9_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:50328 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 19 QA pairs\n",
            "\u001b[2KRetained 17 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 7.3\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Cleaning content from data/generated/sherlock_9_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_9_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:50332 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:50346 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[?25lvLLM STDOUT: INFO 09-25 08:47:46 [logger.py:43] Received request chatcmpl-eadbd92df8fd432ebaa397f069929917: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"How many volumes of the Universal Sherlock Holmes were compiled?\",\\n    \"answer\": \"four\"\\n  },\\n  {\\n    \"question\": \"In what year were the Universal Sherlock Holmes volumes compiled?\",\\n    \"answer\": \"1995\"\\n  },\\n  {\\n    \"question\": \"How many languages have the Sherlock Holmes stories been translated into?\",\\n    \"answer\": \"sixty-three\"\\n  },\\n  {\\n    \"question\": \"What is included in the Universal Sherlock Holmes compilation?\",\\n    \"answer\": \"original writings, translations, writings about Sherlockians, memorials, games, puzzles, phonograph records, audio and video tapes, compact discs, laser discs, and more\"\\n  },\\n  {\\n    \"question\": \"How many items have accumulated worldwide on the two most famous characters in literature?\",\\n    \"answer\": \"a multitude of other items\"\\n  },\\n  {\\n    \"question\": \"What is the name of the compilation of Sherlock Holmes stories?\",\\n    \"answer\": \"Universal Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Who compiled the Universal Sherlock Holmes volumes?\",\\n    \"answer\": \"Ronald B. De Waal\"\\n  },\\n  {\\n    \"question\": \"How many Holmes-related productions and products are listed in the Universal Sherlock Holmes?\",\\n    \"answer\": \"over 25,000\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:46 [engine.py:317] Added request chatcmpl-eadbd92df8fd432ebaa397f069929917.\n",
            "Processing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Cleaning content from data/generated/sherlock_10_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:50360 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:54 [logger.py:43] Received request chatcmpl-2f275bdf68794daf9f6580a4455c5e34: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is included in the Universal Sherlock Holmes compilation, besides the original writings?\",\\n    \"answer\": \"translations, writings about Sherlockians, memorials, games, puzzles, phonograph records, audio and video tapes, compact discs, laser discs, ballets, films, musicals, operettas, oratorios, plays, radio and television programs, parodies and pastiches, children\\'s books, cartoons, comics\"\\n  },\\n  {\\n    \"question\": \"What formats of media are included in the Universal Sherlock Holmes compilation?\",\\n    \"answer\": \"phonograph records, audio and video tapes, compact discs, laser discs, ballets, films, musicals, operettas, oratorios, plays, radio and television programs\"\\n  },\\n  {\\n    \"question\": \"What languages are included in the Universal Sherlock Holmes compilation besides English?\",\\n    \"answer\": \"Braille and sixty-three other languages\"\\n  },\\n  {\\n    \"question\": \"What is the purpose of the Universal Sherlock Holmes compilation?\",\\n    \"answer\": \"to compile all adaptations of Sherlock Holmes\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:54 [engine.py:317] Added request chatcmpl-2f275bdf68794daf9f6580a4455c5e34.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Cleaning content from data/generated/sherlock_10_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:43938 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 5 QA pairs\n",
            "\u001b[2KRetained 5 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 7.6\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Cleaning content from data/generated/sherlock_10_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_10_qa_pairs_cleaned.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "QUALITY_CHECK = True\n",
        "\n",
        "if QUALITY_CHECK:\n",
        "    qa_pairs_filenames = [\n",
        "        f\"data/generated/sherlock_{i}_qa_pairs.json\"\n",
        "        for i in range(len(filenames))\n",
        "    ]\n",
        "    for filename in qa_pairs_filenames:\n",
        "        !synthetic-data-kit \\\n",
        "            -c synthetic_data_kit_config.yaml \\\n",
        "            curate --threshold 5.0 \\\n",
        "            {filename}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "P5S8O6Y6l3eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cfb0a03-dddd-444f-8c56-78dd71b7126f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/sherlock_0_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_0_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/sherlock_1_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_1_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/sherlock_2_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_2_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/sherlock_3_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_3_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/sherlock_4_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_4_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/sherlock_5_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_5_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/sherlock_6_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_6_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/sherlock_7_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_7_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/sherlock_8_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_8_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/sherlock_9_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_9_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/sherlock_10_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_10_qa_pairs_ft.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "qa_pairs_filenames = [\n",
        "    f\"data/generated/sherlock_{i}_qa_pairs.json\"\n",
        "    for i in range(len(filenames))\n",
        "]\n",
        "for filename in qa_pairs_filenames:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        save-as {filename} -f ft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MmFFvFtWmNpc"
      },
      "outputs": [],
      "source": [
        "final_filenames = os.listdir(\"data/final\")\n",
        "\n",
        "conversations = pd.concat(\n",
        "    [pd.read_json(f\"data/final/{name}\") for name in final_filenames]\n",
        ").reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7_Zt_cj2ykFi"
      },
      "outputs": [],
      "source": [
        "all_contents = list(\n",
        "    itertools.chain.from_iterable(\n",
        "        [\n",
        "            [message[\"content\"] for message in conversation]\n",
        "            for conversation in conversations[\"messages\"]\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "content_counts = Counter(all_contents)\n",
        "\n",
        "most_common_content = content_counts.most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EKRKqvrHykFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "174dad27-45c5-49e4-c125-966a1cb52fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('You are a helpful assistant.', 165), ('Yes', 12), ('Sherlockiana', 6), ('Not specified', 6), ('Arthur Conan Doyle', 4), ('Not specified in the text', 3), ('Professor James Moriarty', 3), ('What is Sherlockiana?', 2), ('1987', 2), ('Charles Spencer', 2), ('Sherlock Holmes', 2), ('Sherlock Holmes pastiches in print and other media such as films', 2), ('memorabilia associated with Sherlock Holmes', 2), ('anything about, inspired by, or tangentially concerning Sherlock Holmes', 2), ('What is a Sherlock Holmes pastiche?', 2), ('not mentioned in the text', 2), (\"Who wrote the short story 'The Ultimate Crime'?\", 2), ('Isaac Asimov', 2), ('The Dynamics of an Asteroid', 2), ('Who won the 1887 celestial mechanics contest?', 2), ('Henri Poincaré', 2), ('The Baker Street Journal', 2), ('1994', 2), ('2015', 2), ('1914', 2), ('Sherlock Holmes fans', 2), ('What type of publication is The Baker Street Journal?', 2), ('Publication', 2), ('An Asteroid', 2), ('What institution is intrigued by the original location of 221B Baker Street?', 1), ('The United States Smithsonian Museums', 1), ('When did the Sherlock Holmes International Society open the Sherlock Holmes Museum at 221B Baker Street?', 1), ('1990', 1), (\"What was found about the supposed location of Holmes and Watson's flat during the investigation?\", 1), ('It did not exist during the early stories such as A Study in Scarlet', 1), ('What type of culture is represented by statues of Holmes?', 1), ('Sherlockian culture', 1), ('What elements of the world are idolized in Sherlockian culture?', 1), ('Elements of the world', 1), ('Where is the Sherlock Holmes Museum located?', 1), ('221B Baker Street', 1), (\"Is the supposed location of Holmes and Watson's flat mentioned in the early stories?\", 1), (\"What type of stories mention the supposed location of Holmes and Watson's flat?\", 1), ('The early stories such as A Study in Scarlet', 1), ('Is the Sherlock Holmes Museum mentioned as existing at 221B Baker Street?', 1), ('Is the United States Smithsonian Museums mentioned as being from the US?', 1), ('Is the Sherlock Holmes International Society mentioned as being international?', 1), (\"Is the location of Holmes and Watson's flat mentioned as not existing during the early stories?\", 1), ('Is Sherlockian culture mentioned as being idolizing elements of the world?', 1), ('Is the Sherlock Holmes Museum mentioned as being opened by the Sherlock Holmes International Society?', 1)]\n"
          ]
        }
      ],
      "source": [
        "print(most_common_content[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eQWaPYMXykFj"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_pandas(conversations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fH_7pBeRmltK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6050290-73de-4b7f-a9f9-bf915d0509c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Hugging Face Dataset object:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['messages'],\n",
            "        num_rows: 165\n",
            "    })\n",
            "})\n",
            "\n",
            "Example from the training set:\n",
            "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What institution is intrigued by the original location of 221B Baker Street?', 'role': 'user'}, {'content': 'The United States Smithsonian Museums', 'role': 'assistant'}]}\n"
          ]
        }
      ],
      "source": [
        "final_dataset = DatasetDict({\n",
        "    'train': dataset,\n",
        "})\n",
        "\n",
        "print(\"\\nFinal Hugging Face Dataset object:\")\n",
        "print(final_dataset)\n",
        "\n",
        "# You can inspect an example\n",
        "print(\"\\nExample from the training set:\")\n",
        "print(final_dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CHQJu6Rgn-GW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a265c4-5047-4899-8e72-b5de39183162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face login successful using Google Colab secrets!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from huggingface_hub import login\n",
        "    from google.colab import userdata\n",
        "\n",
        "    # Retrieve your Hugging Face token from Colab's secrets manager\n",
        "    # The name 'HF_TOKEN' should match the name you used in the secrets tab\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "    # Check if the token was successfully retrieved\n",
        "    if hf_token:\n",
        "        # Log in to Hugging Face using the retrieved token\n",
        "        # The `add_to_git_credential=True` argument is optional and useful if you plan to push models to the Hub\n",
        "        login(token=hf_token, add_to_git_credential=True)\n",
        "        print(\"Hugging Face login successful using Google Colab secrets!\")\n",
        "    else:\n",
        "        print(\"Error: HF_TOKEN not found in Google Colab secrets or is empty.\")\n",
        "        print(\"Please ensure you have created a secret named 'HF_TOKEN' in the 'Secrets' tab (🔑) on the left sidebar.\")\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pxLjmg8vnzrz"
      },
      "outputs": [],
      "source": [
        "if not DEMO:\n",
        "\n",
        "    # Your final_dataset object from the script above is ready\n",
        "    repo_id = \"lmassaron/Sherlock_QA\"\n",
        "    print(f\"\\nUploading dataset to the Hub at {repo_id}...\")\n",
        "\n",
        "    # This command uploads the dataset. It will create the repo if it doesn't exist.\n",
        "    final_dataset.push_to_hub(repo_id)\n",
        "    print(\"Upload complete!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6b337fd9169d44b2912857836c2cb139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_841743119ad341f79e2e9578bcc80752",
              "IPY_MODEL_ac633a1e9da541ce92be69e04ca9bdba",
              "IPY_MODEL_f1a3081d521846f1a798895f7f193b42"
            ],
            "layout": "IPY_MODEL_facc252c42d4464789c87f50a0aaedc0"
          }
        },
        "841743119ad341f79e2e9578bcc80752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49f9b29386ba42a290e7b99c8aa698ea",
            "placeholder": "​",
            "style": "IPY_MODEL_351500b2733e45f1ad0eb4ab4b8ed2d7",
            "value": "config.json: 100%"
          }
        },
        "ac633a1e9da541ce92be69e04ca9bdba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac429e7ebf474978859dc4d95168d04a",
            "max": 890,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5a8b5f752d346838eb6d8e399950bcd",
            "value": 890
          }
        },
        "f1a3081d521846f1a798895f7f193b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a287daf9a4d4e54933126e172851c87",
            "placeholder": "​",
            "style": "IPY_MODEL_3b29065512914644967a0ac62ad5658c",
            "value": " 890/890 [00:00&lt;00:00, 68.0kB/s]"
          }
        },
        "facc252c42d4464789c87f50a0aaedc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49f9b29386ba42a290e7b99c8aa698ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "351500b2733e45f1ad0eb4ab4b8ed2d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac429e7ebf474978859dc4d95168d04a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5a8b5f752d346838eb6d8e399950bcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a287daf9a4d4e54933126e172851c87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b29065512914644967a0ac62ad5658c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be19f046cdd44a54a27f6b2bc32748b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63ec8490d8b4460c82b0a8d3d64b95b0",
              "IPY_MODEL_d9c06b0a14a940b98d8e8b83c90ef616",
              "IPY_MODEL_f218346583ca4b029b9392cd6497c4dc"
            ],
            "layout": "IPY_MODEL_cd3f928c8161486ba70c150972cd2a74"
          }
        },
        "63ec8490d8b4460c82b0a8d3d64b95b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebc680f1399b4597b7a572544be496c1",
            "placeholder": "​",
            "style": "IPY_MODEL_93fdd16b6b724929a7b884142fa80ba6",
            "value": "tokenizer_config.json: "
          }
        },
        "d9c06b0a14a940b98d8e8b83c90ef616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19e03d3a43314c048a4264f6a5a1caee",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a58fa7bdbf24d638441e2f66ff45eee",
            "value": 1
          }
        },
        "f218346583ca4b029b9392cd6497c4dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6a93b3cef3b46c0bd6b23976076aac0",
            "placeholder": "​",
            "style": "IPY_MODEL_24fb26cfa81746779747e49490786ae6",
            "value": " 54.7k/? [00:00&lt;00:00, 829kB/s]"
          }
        },
        "cd3f928c8161486ba70c150972cd2a74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebc680f1399b4597b7a572544be496c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93fdd16b6b724929a7b884142fa80ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19e03d3a43314c048a4264f6a5a1caee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7a58fa7bdbf24d638441e2f66ff45eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6a93b3cef3b46c0bd6b23976076aac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24fb26cfa81746779747e49490786ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41cbc18590a24107a1e80ab31a3c58dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d25a7b5bfb246c1adbbba8864986a44",
              "IPY_MODEL_226b7ab90023484484745975d1f0e6c0",
              "IPY_MODEL_49d4b4e0f4c343caaf076d76b895325e"
            ],
            "layout": "IPY_MODEL_ea2ae9fb3cb04348be160c2d4f3ae770"
          }
        },
        "1d25a7b5bfb246c1adbbba8864986a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe73eb9f33a24180957da13dcb0c555d",
            "placeholder": "​",
            "style": "IPY_MODEL_5e6b2a6a845c4c72bd8f9e09c67224a5",
            "value": "tokenizer.json: 100%"
          }
        },
        "226b7ab90023484484745975d1f0e6c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2013cf5b594e47309f4e077d3fe0282d",
            "max": 17209920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c79b1c7147d419cbb0cd986d86eda3a",
            "value": 17209920
          }
        },
        "49d4b4e0f4c343caaf076d76b895325e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58e9e82b12364b7090198a5de6545f2d",
            "placeholder": "​",
            "style": "IPY_MODEL_0f018cfa4cbd4ba48c8994144b68a14a",
            "value": " 17.2M/17.2M [00:00&lt;00:00, 24.2MB/s]"
          }
        },
        "ea2ae9fb3cb04348be160c2d4f3ae770": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe73eb9f33a24180957da13dcb0c555d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e6b2a6a845c4c72bd8f9e09c67224a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2013cf5b594e47309f4e077d3fe0282d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c79b1c7147d419cbb0cd986d86eda3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58e9e82b12364b7090198a5de6545f2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f018cfa4cbd4ba48c8994144b68a14a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ce49735af464559a0d10deb1bacf5f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b4a6dc10bcf45cf8ad81a85c123a87b",
              "IPY_MODEL_d2c6dacca24441158a0a14a385b9dd43",
              "IPY_MODEL_6bdeafe125c5414cb522edb15f1d568b"
            ],
            "layout": "IPY_MODEL_9566a1540f5f4d83a879e6f3a280743b"
          }
        },
        "0b4a6dc10bcf45cf8ad81a85c123a87b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28bb4773c2b74cd4bf0a8b4581c4b915",
            "placeholder": "​",
            "style": "IPY_MODEL_3cd342899fcd44c182af0993e2cba6fe",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "d2c6dacca24441158a0a14a385b9dd43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd3aad470c09495ca694b1a801ad7fa0",
            "max": 454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70703b9cc48c439e97a50da745e6e42c",
            "value": 454
          }
        },
        "6bdeafe125c5414cb522edb15f1d568b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a564493fb379496a99444025ba436e9b",
            "placeholder": "​",
            "style": "IPY_MODEL_6f284950d4184309b5daf93d682ea36c",
            "value": " 454/454 [00:00&lt;00:00, 12.2kB/s]"
          }
        },
        "9566a1540f5f4d83a879e6f3a280743b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28bb4773c2b74cd4bf0a8b4581c4b915": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cd342899fcd44c182af0993e2cba6fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd3aad470c09495ca694b1a801ad7fa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70703b9cc48c439e97a50da745e6e42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a564493fb379496a99444025ba436e9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f284950d4184309b5daf93d682ea36c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8878843f09f54135a301d2ef67dc902a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ce5c8d383ba49cfb8c2050dbb56ad1d",
              "IPY_MODEL_ba59cd7a3456433ebadaf607a3f8a076",
              "IPY_MODEL_4e97067b6bee494c8ace55785d27c6df"
            ],
            "layout": "IPY_MODEL_da292c9536924ac39ffac041c1d4dbad"
          }
        },
        "5ce5c8d383ba49cfb8c2050dbb56ad1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e8755428cea46e5a1bdb50433b83967",
            "placeholder": "​",
            "style": "IPY_MODEL_ea2ccac65e2242c79f5a822cb4f7cc07",
            "value": "chat_template.jinja: "
          }
        },
        "ba59cd7a3456433ebadaf607a3f8a076": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72507b08307845bca26605a6a7533c6e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_474c12ec5e9f47daab30327fae2bc712",
            "value": 1
          }
        },
        "4e97067b6bee494c8ace55785d27c6df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22c6a34905ed47468fcd0229a5213d74",
            "placeholder": "​",
            "style": "IPY_MODEL_72097af672d14073aad5a2be8d00a7b6",
            "value": " 3.83k/? [00:00&lt;00:00, 157kB/s]"
          }
        },
        "da292c9536924ac39ffac041c1d4dbad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e8755428cea46e5a1bdb50433b83967": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea2ccac65e2242c79f5a822cb4f7cc07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72507b08307845bca26605a6a7533c6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "474c12ec5e9f47daab30327fae2bc712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22c6a34905ed47468fcd0229a5213d74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72097af672d14073aad5a2be8d00a7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}