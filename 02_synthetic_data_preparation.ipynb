{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmassaron/fine-tuning-workshop/blob/main/02_synthetic_data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **`wikipedia-api`** package is a Python library that makes it easy to access and retrieve data from Wikipedia.\n",
        "\n",
        "It is a convenient **wrapper** around Wikipedia's official API. Instead of having to deal with complex web requests and raw data formats (like JSON), this package provides simple Python functions to:\n",
        "\n",
        "*   **Get a Wikipedia page:** Fetch the full text, summary, and other details of a specific article.\n",
        "*   **Search for pages:** Find articles related to a search query.\n",
        "*   **Handle multiple languages:** Easily switch between different language editions of Wikipedia (e.g., 'en' for English, 'es' for Spanish).\n",
        "*   **Manage categories and links:** List all pages in a category or all links on a page."
      ],
      "metadata": {
        "id": "_Opuf0GkMOXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **`synthetic-data-kit`** is a powerful command-line interface (CLI) tool, developed by Meta, designed to streamline and accelerate the process of creating high-quality, synthetic datasets for fine-tuning Large Language Models (LLMs).\n",
        "\n",
        "The primary problem it solves is the lack of high-quality, domain-specific data needed for customizing a general-purpose LLM for a particular task or industry (like law, medicine, or finance).\n",
        "\n",
        "The package is built around a simple and modular 4-command workflow that takes you from raw documents to a ready-to-use fine-tuning dataset.\n",
        "\n",
        "1.  **`ingest`**: This first step takes your raw data from various sources and converts it into a standardized text format. It can handle a wide range of file types, including:\n",
        "    *   PDFs (`.pdf`)\n",
        "    *   Word Documents (`.docx`)\n",
        "    *   PowerPoint Presentations (`.ppt`)\n",
        "    *   Web pages (`.html`)\n",
        "    *   YouTube video transcripts\n",
        "    *   Plain text (`.txt`)\n",
        "\n",
        "2.  **`create`**: This is the core data generation step. It takes the ingested text, intelligently splits it into manageable chunks, and then uses a powerful LLM (like Llama 3) to generate new, synthetic data based on that text. You can instruct it to create different types of datasets, such as:\n",
        "    *   **Question-Answer (QA) pairs**: Ideal for building chatbots and Q&A systems.\n",
        "    *   **Reasoning Traces / Chain-of-Thought (CoT)**: Creates examples that show the step-by-step reasoning process, which is useful for improving a model's logical abilities.\n",
        "    *   **Summaries**: Generates summaries of the text chunks.\n",
        "\n",
        "3.  **`curate`**: To ensure high quality, this command uses an LLM as a \"judge\" to review the synthetically generated examples. It filters out low-quality or irrelevant pairs, ensuring that the final dataset is clean and effective for fine-tuning.\n",
        "\n",
        "4.  **`save-as`**: The final step is to export the curated data into a format that is compatible with popular fine-tuning libraries and workflows. You can save the dataset as:\n",
        "    *   Hugging Face Datasets\n",
        "    *   JSON or JSONL files\n",
        "\n",
        "In essence, the `synthetic-data-kit` provides an end-to-end, customizable pipeline for turning your private documents or domain-specific knowledge into a structured dataset that can be used to make a powerful, general LLM an expert in your specific area of interest."
      ],
      "metadata": {
        "id": "PFmZOt4jMk-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`unsloth`** is a high-performance Python library designed to make **fine-tuning Large Language Models (LLMs) dramatically faster and more memory-efficient.**\n",
        "\n",
        "It is a powerful optimization layer that sits on top of popular libraries like Hugging Face's `transformers`, PyTorch, and PEFT (for LoRA).\n",
        "\n",
        "Unsloth achieves its incredible performance by re-implementing the most computationally intensive parts of the training process from scratch.\n",
        "\n",
        "1.  **Custom GPU Kernels:** It replaces standard PyTorch operations with its own highly optimized code written in Triton (a language for writing efficient GPU code).\n",
        "2.  **Manual Autograd Engine:** Instead of using PyTorch's general-purpose automatic differentiation (autograd), Unsloth uses a specialized, manual backpropagation engine that is tailored *specifically* for training LLMs with LoRA. This eliminates a massive amount of overhead.\n",
        "\n",
        "This results in:\n",
        "\n",
        "*   **Massive Speedup:** It can make your fine-tuning process **2-5 times faster** than a standard implementation. A training job that took 10 hours might now take only 2-4 hours.\n",
        "*   **Drastic Memory Reduction:** It reduces VRAM usage by **up to 80%**. This is its most significant advantage. It allows you to:\n",
        "    *   Fine-tune much larger models on consumer-grade GPUs (like an RTX 3090 or a free Google Colab T4).\n",
        "    *   Use a much larger batch size, which can further speed up training and improve model performance.\n",
        "*   **No Performance Loss:** These optimizations are achieved without sacrificing the final model's accuracy or performance.\n",
        "*   **Easy to Use:** It's designed as a \"drop-in\" replacement. You typically only need to change a few lines of your existing Hugging Face training script to enable it. For example, you replace `AutoModelForCausalLM` with Unsloth's `FastLanguageModel`."
      ],
      "metadata": {
        "id": "E25dWyn7M6Mf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJvuwcN0ynrh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm synthetic-data-kit==0.0.3\n",
        "else:\n",
        "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except: get_numpy = \"numpy\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade         unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "    !uv pip install synthetic-data-kit==0.0.3\n",
        "!uv pip install transformers==4.55.4\n",
        "!uv pip install --no-deps trl==0.22.2\n",
        "!uv pip install wikipedia-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL-6wLPoyyET"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except: get_numpy = \"numpy\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade \\\n",
        "        unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "!uv pip install transformers==4.55.4\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzDmaFKS2Mg2",
        "outputId": "cb6f8958-6d21-4980-fab9-d1e1031565f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "INFO 09-25 08:31:23 [__init__.py:244] Automatically detected platform cuda.\n",
            "ERROR 09-25 08:31:30 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import wikipediaapi\n",
        "from unsloth.dataprep import SyntheticDataKit\n",
        "import huggingface_hub\n",
        "from collections import Counter\n",
        "import itertools\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict, ClassLabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHzKje014dJf"
      },
      "outputs": [],
      "source": [
        "DEMO = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6EH83m_2Rz8"
      },
      "outputs": [],
      "source": [
        "# Pre-compile the regular expression pattern for better performance\n",
        "BRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n",
        "\n",
        "def remove_braces_and_content(text):\n",
        "    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n",
        "    return BRACES_PATTERN.sub('', text)\n",
        "\n",
        "def clean_string(input_string):\n",
        "    \"\"\"Clean the input string.\"\"\"\n",
        "\n",
        "    # Remove extra spaces by splitting the string by spaces and joining back together\n",
        "    cleaned_string = ' '.join(input_string.split())\n",
        "\n",
        "    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n",
        "    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n",
        "\n",
        "    # Remove all occurrences of curly braces and their content from the cleaned string\n",
        "    cleaned_string = remove_braces_and_content(cleaned_string)\n",
        "\n",
        "    # Return the cleaned string\n",
        "    return cleaned_string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " This function **finds a specific category on Wikipedia and returns a list of all the article titles that belong to it.**\n",
        "\n",
        "\n",
        "1.  **`category = wiki_wiki.page(\"Category:\" + category_name)`**: It first finds the specific \"Category\" page on Wikipedia (e.g., \"Category:Physics\").\n",
        "\n",
        "2.  **`if category.exists():`**: It checks to make sure this category page actually exists to avoid errors.\n",
        "\n",
        "3.  **`for article in category.categorymembers.values():`**: If the category exists, it loops through all the \"members\" (the articles and subcategories) listed on that page.\n",
        "\n",
        "4.  **`pages.append(article.title)`**: Inside the loop, it grabs the `title` of each article and adds it to the `pages` list.\n",
        "\n",
        "5.  **`return pages`**: Finally, it returns the complete list of collected article titles."
      ],
      "metadata": {
        "id": "VuTB3ZxXa8_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq8-Z4RBykFh"
      },
      "outputs": [],
      "source": [
        "def extract_wikipedia_pages(wiki_wiki, category_name):\n",
        "    \"\"\"Extract all references from a category on Wikipedia\"\"\"\n",
        "\n",
        "    # Get the Wikipedia page corresponding to the provided category name\n",
        "    category = wiki_wiki.page(\"Category:\" + category_name)\n",
        "\n",
        "    # Initialize an empty list to store page titles\n",
        "    pages = []\n",
        "\n",
        "    # Check if the category exists\n",
        "    if category.exists():\n",
        "        # Iterate through each article in the category and append its title to the list\n",
        "        for article in category.categorymembers.values():\n",
        "            pages.append(article.title)\n",
        "\n",
        "    # Return the list of page titles\n",
        "    return pages"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main purpose of this function is to build a large, custom dataset of text about specific topics by **recursively crawling Wikipedia categories.** It starts with a few main topics, finds all related articles and sub-topics, and then extracts and cleans the text from every unique article it discovers.\n",
        "\n",
        "It works in four main phases:\n",
        "\n",
        "### Phase 1: Initial Crawl\n",
        "It begins by looping through the initial list of `categories` you provide (e.g., \"Physics\"). For each one, it grabs the titles of all the immediate member articles and subcategories.\n",
        "\n",
        "### Phase 2: Cleanup and Preparation\n",
        "After the first pass, it separates the articles from the subcategories it found. It puts the subcategories (anything with \"Category:\" in the title) into a new \"to-do\" list called `categories_to_explore` and makes sure the main `wikipedia_pages` list only contains unique article titles.\n",
        "\n",
        "### Phase 3: Deep Dive (Recursive Crawl)\n",
        "This is the core crawling logic. It uses a `while` loop that continues as long as there are subcategories left in the `categories_to_explore` list. In each loop, it:\n",
        "1.  Pops a subcategory off the list.\n",
        "2.  Finds all *its* members.\n",
        "3.  Adds any new articles it finds to the master `wikipedia_pages` list.\n",
        "4.  Adds any new *sub-subcategories* it finds back onto the `categories_to_explore` list.\n",
        "\n",
        "This process continues until it has explored every related subcategory and has a comprehensive list of all unique article titles.\n",
        "\n",
        "### Phase 4: Text Extraction and Cleaning\n",
        "Finally, once it has the complete list of article titles, it loops through each one and:\n",
        "1.  Downloads the page content.\n",
        "2.  Performs a simple check to filter out unwanted topics (e.g., pages mentioning \"Biden\" or \"Trump\").\n",
        "3.  Extracts the text from the page's **summary** and each of its **sections**.\n",
        "4.  Adds this cleaned text to the final `extracted_texts` list, which is then returned."
      ],
      "metadata": {
        "id": "R8OvIImIbiX4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLYvGfu72XUF"
      },
      "outputs": [],
      "source": [
        "def get_wikipedia_pages(categories):\n",
        "    \"\"\"Retrieve Wikipedia pages from a list of categories and extract their content\"\"\"\n",
        "\n",
        "    # Create a Wikipedia object\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('Gemma AI Assistant (gemma@example.com)', 'en')\n",
        "\n",
        "    # Initialize lists to store explored categories and Wikipedia pages\n",
        "    explored_categories = []\n",
        "    wikipedia_pages = []\n",
        "\n",
        "    # Iterate through each category\n",
        "    print(\"- Processing Wikipedia categories:\")\n",
        "    for category_name in categories:\n",
        "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
        "\n",
        "        # Get the Wikipedia page corresponding to the category\n",
        "        category = wiki_wiki.page(\"Category:\" + category_name)\n",
        "\n",
        "        # Extract Wikipedia pages from the category and extend the list\n",
        "        wikipedia_pages.extend(extract_wikipedia_pages(wiki_wiki, category_name))\n",
        "\n",
        "        # Add the explored category to the list\n",
        "        explored_categories.append(category_name)\n",
        "\n",
        "    # Extract subcategories and remove duplicate categories\n",
        "    categories_to_explore = [item.replace(\"Category:\", \"\") for item in wikipedia_pages if \"Category:\" in item]\n",
        "    wikipedia_pages = list(set([item for item in wikipedia_pages if \"Category:\" not in item]))\n",
        "\n",
        "    # Explore subcategories recursively\n",
        "    while categories_to_explore:\n",
        "        category_name = categories_to_explore.pop()\n",
        "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
        "\n",
        "        # Extract more references from the subcategory\n",
        "        more_refs = extract_wikipedia_pages(wiki_wiki, category_name)\n",
        "\n",
        "        # Iterate through the references\n",
        "        for ref in more_refs:\n",
        "            # Check if the reference is a category\n",
        "            if \"Category:\" in ref:\n",
        "                new_category = ref.replace(\"Category:\", \"\")\n",
        "                # Add the new category to the explored categories list\n",
        "                if new_category not in explored_categories:\n",
        "                    explored_categories.append(new_category)\n",
        "            else:\n",
        "                # Add the reference to the Wikipedia pages list\n",
        "                if ref not in wikipedia_pages:\n",
        "                    wikipedia_pages.append(ref)\n",
        "\n",
        "    # Initialize a list to store extracted texts\n",
        "    extracted_texts = []\n",
        "\n",
        "    # Iterate through each Wikipedia page\n",
        "    print(\"- Processing Wikipedia pages:\")\n",
        "    for page_title in tqdm(wikipedia_pages):\n",
        "        try:\n",
        "            # Make a request to the Wikipedia page\n",
        "            page = wiki_wiki.page(page_title)\n",
        "\n",
        "            # Check if the page summary does not contain certain keywords\n",
        "            if \"Biden\" not in page.summary and \"Trump\" not in page.summary:\n",
        "                # Append the page title and summary to the extracted texts list\n",
        "                if len(page.summary) > len(page.title):\n",
        "                    extracted_texts.append(page.title + \" : \" + clean_string(page.summary))\n",
        "\n",
        "                # Iterate through the sections in the page\n",
        "                for section in page.sections:\n",
        "                    # Append the page title and section text to the extracted texts list\n",
        "                    if len(section.text) > len(page.title):\n",
        "                        extracted_texts.append(page.title + \" : \" + clean_string(section.text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing page {page.title}: {e}\")\n",
        "\n",
        "    # Return the extracted texts\n",
        "    return extracted_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk-aqKqq2ehW",
        "outputId": "5e402166-f505-4841-e7e9-2bc54ec913cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Processing Wikipedia categories:\n",
            "\tExploring Sherlock_Holmes on Wikipedia\n",
            "\tExploring Writers of Sherlock Holmes pastiches on Wikipedia\n",
            "\tExploring Works based on Sherlock Holmes on Wikipedia\n",
            "\tExploring Sherlock Holmes short story collections on Wikipedia\n",
            "\tExploring Sherlock Holmes short stories on Wikipedia\n",
            "\tExploring Sherlock Holmes audio adaptations on Wikipedia\n",
            "\tExploring Sherlock Holmes scholars on Wikipedia\n",
            "\tExploring Sherlock Holmes novels on Wikipedia\n",
            "\tExploring Sherlock Holmes navigational boxes on Wikipedia\n",
            "\tExploring Sherlock Holmes lists on Wikipedia\n",
            "\tExploring Dartmoor on Wikipedia\n",
            "\tExploring Sherlock Holmes characters on Wikipedia\n",
            "\tExploring Baker Street on Wikipedia\n",
            "- Processing Wikipedia pages:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 459/459 [01:04<00:00,  7.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2042 Wikipedia pages\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "categories = [\n",
        "    \"Sherlock_Holmes\",\n",
        "    \"Arthur_Conan_Doyle\",\n",
        "    \"A_Scandal_in_Bohemia\",\n",
        "    \"The_Adventures_of_Sherlock_Holmes\",\n",
        "    \"A_Study_in_Scarlet\",\n",
        "    \"The_Sign_of_the_Four\",\n",
        "    \"The_Memoirs_of_Sherlock_Holmes\",\n",
        "    \"The_Hound_of_the_Baskervilles\",\n",
        "    \"The_Return_of_Sherlock_Holmes\",\n",
        "    \"The_Valley_of_Fear\",\n",
        "    \"His_Last_Bow\",\n",
        "    \"The_Case-Book_of_Sherlock_Holmes\",\n",
        "    \"Canon_of_Sherlock_Holmes\",\n",
        "    \"Dr._Watson\",\n",
        "    \"221B_Baker_Street\",\n",
        "    \"Mrs._Hudson\",\n",
        "    \"Professor_Moriarty\",\n",
        "    \"The_Strand_Magazine\",\n",
        "    \"Minor_Sherlock_Holmes_characters\",\n",
        "    \"Inspector_Lestrade\",\n",
        "    \"Mycroft_Holmes\",\n",
        "    \"Irene_Adler\",\n",
        "    \"Colonel_Moran\",\n",
        "    \"Baker_Street_Irregulars\",\n",
        "    \"Giant_rat_of_Sumatra\",\n",
        "    \"The_Story_of_the_Lost_Special\",\n",
        "    \"How_Watson_Learned_the_Trick\",\n",
        "    \"Diogenes_Club\",\n",
        "    \"The_Dynamics_of_an_Asteroid\",\n",
        "    \"Reichenbach_Falls\",\n",
        "    \"A_Treatise_on_the_Binomial_Theorem\",\n",
        "    \"Sherlockian_game\",\n",
        "    \"List_of_Holmesian_studies\",\n",
        "    \"The_New_Annotated_Sherlock_Holmes\",\n",
        "    \"The_Private_Life_of_Sherlock_Holmes_(book)\",\n",
        "    \"The_Great_Detective_(book)\",\n",
        "    \"Naked_Is_the_Best_Disguise\",\n",
        "    \"Sherlock_Holmes_fandom\",\n",
        "    \"Sherlockiana\",\n",
        "    \"Sherlock_Holmes_Museum\",\n",
        "    \"The_Sherlock_Holmes\",\n",
        "    \"The_Baker_Street_Irregulars\",\n",
        "    \"The_Baker_Street_Journal\",\n",
        "    \"Sidney_Paget\",\n",
        "    \"The_Strand_Magazine\",\n",
        "    \"Undershaw\",\n",
        "    \"Canon_of_Sherlock_Holmes\",\n",
        "    \"Adaptations_of_Sherlock_Holmes\",\n",
        "    \"Sherlock_Holmes_pastiches\",\n",
        "    \"Popular_culture_references_to_Sherlock_Holmes\",\n",
        "]\n",
        "\n",
        "if DEMO:\n",
        "    categories = [\"Sherlock_Holmes\"]\n",
        "\n",
        "extracted_texts = get_wikipedia_pages(categories)\n",
        "print(\"Found\", len(extracted_texts), \"Wikipedia pages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7ii2mZw26x0",
        "outputId": "a001c710-919d-4d9b-e557-e876fedcfc30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All texts have been saved successfully.\n"
          ]
        }
      ],
      "source": [
        "output_dir = 'data/output'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for k, text in enumerate(extracted_texts):\n",
        "    file_path = os.path.join(output_dir, f'sherlock_{k}.txt')\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write(text)\n",
        "    if DEMO and k > 9:\n",
        "        break\n",
        "\n",
        "print(\"All texts have been saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZumHepuykFi"
      },
      "outputs": [],
      "source": [
        "filenames = [f\"data/output/{file}\" for file in os.listdir(\"data/output\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell initializes the `SyntheticDataKit` tool:\n",
        "\n",
        "1.  **Loads the \"engine\":** It downloads and sets up the language model that will act as the \"engine\" for creating the data. It's using **`\"unsloth/Llama-3.2-3B-Instruct\"`**, which is a version of Meta's powerful Llama 3.2 model that has been heavily optimized by **Unsloth** for maximum speed and memory efficiency.\n",
        "\n",
        "2.  **Sets a Limit:** It configures the `max_seq_length` to **2048 tokens**. This tells the generator the maximum length of a text chunk it should process at one time, which is a key setting for balancing performance and the quality of the generated data."
      ],
      "metadata": {
        "id": "EIVm3ya9b6ik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5-KNLWuAvXy"
      },
      "outputs": [],
      "source": [
        "generator = SyntheticDataKit.from_pretrained(\n",
        "    # Choose any model from https://huggingface.co/unsloth\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = 2048, # Longer sequence lengths will be slower!\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up for **Question-Answer (QA) pair** generation with the following rules:\n",
        "\n",
        "*   **`output_folder = \"data\"`**: Specifies that the final dataset should be saved in a folder named `\"data\"`.\n",
        "*   **`temperature = 0.7` & `top_p = 0.95`**: These settings control the creativity and randomness of the LLM. A higher temperature allows the model to generate a more diverse and varied set of questions and answers, making the final dataset richer.\n",
        "*   **`overlap = 64`**: When the tool chops up your long source documents into smaller pieces, this ensures that each piece overlaps with the previous one by 64 tokens. This helps maintain context and prevents ideas from being cut off at the edges.\n",
        "*   **`max_generation_tokens = 512`**: This sets the maximum length for each generated question and answer pair, preventing them from becoming too long."
      ],
      "metadata": {
        "id": "8IAxG-KDcQeB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY4I1S4pRVl1"
      },
      "outputs": [],
      "source": [
        "generator.prepare_qa_generation(\n",
        "    output_folder = \"data\", # Output location of synthetic data\n",
        "    temperature = 0.7, # Higher temp makes more diverse datases\n",
        "    top_p = 0.95,\n",
        "    overlap = 64, # Overlap portion during chunking\n",
        "    max_generation_tokens = 512, # Can increase for longer QA pairs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell is just a confirmation step to ensure that the necessary components are active before you start the main data generation process.:\n",
        "\n",
        "1.  **`VLLM server is running`**: This tells you that **vLLM**, the high-speed engine used by the `synthetic-data-kit` to run the LLM, has started successfully.\n",
        "2.  **`Available models: ... 'unsloth/Llama-3.2-3B-Instruct'`**: This is the crucial part. It confirms that the vLLM server has successfully loaded the correct model (`unsloth/Llama-3.2-3B-Instruct`) and it's ready to be used for data generation.\n"
      ],
      "metadata": {
        "id": "TZ5IMcAYcmDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**vLLM** is a high-performance Python library designed to make **running and serving Large Language Models (LLMs) for inference incredibly fast and efficient.**\n",
        "\n",
        "Think of it as a specialized, high-speed engine that replaces the standard inference methods in libraries like Hugging Face's `transformers`.\n",
        "\n",
        "Standard LLM inference is often inefficient, especially when handling many users or requests at once. A huge amount of GPU memory is wasted managing a dynamic memory block called the **KV Cache**, which stores the context of the conversation. This leads to low throughput (fewer requests served per second) and higher costs.\n",
        "\n",
        "vLLM's key innovation is an algorithm called **PagedAttention**. Inspired by how operating systems use virtual memory and paging to manage computer memory, PagedAttention does the same for the GPU's KV Cache:\n",
        "\n",
        "*   It breaks the large, clunky KV Cache into a collection of small, fixed-size \"blocks.\"\n",
        "*   These blocks can be stored anywhere in the GPU's memory, eliminating wasted space and fragmentation.\n",
        "*   This allows vLLM to pack many more user requests onto a single GPU and manage them with extreme efficiency, much like a well-organized file system.\n",
        "\n",
        "In summary, **vLLM is a specialized engine for LLM *serving***. While a library like Unsloth makes *training* models faster, vLLM makes *using* them in a production environment faster and cheaper."
      ],
      "metadata": {
        "id": "l-JqSPEDcr5M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqe2j9MGRWVk",
        "outputId": "85aab962-d202-4ff2-d153-4128f1c84b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:39226 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[?25l\u001b[32m VLLM server is running at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[32m‚†ã\u001b[0m\u001b[32m Checking VLLM server at http://localhost:8000/v1...\u001b[0m\r\u001b[2KAvailable models: \u001b[1m{\u001b[0m\u001b[32m'object'\u001b[0m: \u001b[32m'list'\u001b[0m, \u001b[32m'data'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model'\u001b[0m, \u001b[32m'created'\u001b[0m: \u001b[1;36m1758789636\u001b[0m, \n",
            "\u001b[32m'owned_by'\u001b[0m: \u001b[32m'vllm'\u001b[0m, \u001b[32m'root'\u001b[0m: \u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'parent'\u001b[0m: \u001b[3;35mNone\u001b[0m, \n",
            "\u001b[32m'max_model_len'\u001b[0m: \u001b[1;36m2048\u001b[0m, \u001b[32m'permission'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'modelperm-836372cf29c34ca2953745621762a2cd'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model_permission'\u001b[0m, \n",
            "\u001b[32m'created'\u001b[0m: \u001b[1;36m1758789636\u001b[0m, \u001b[32m'allow_create_engine'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_sampling'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_logprobs'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'allow_search_indices'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_view'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_fine_tuning'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'organization'\u001b[0m: \u001b[32m'*'\u001b[0m, \u001b[32m'group'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'is_blocking'\u001b[0m: \n",
            "\u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
            "\u001b[32m‚†ã\u001b[0m Checking VLLM server at http://localhost:8000/v1...\r\u001b[2K\u001b[32m‚†ã\u001b[0m Checking VLLM server at http://localhost:8000/v1...\n",
            "\u001b[?25h\r\u001b[1A\u001b[2K"
          ]
        }
      ],
      "source": [
        "!synthetic-data-kit system-check"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell is the **main data generation step**. It iterates through your source text files and uses the `synthetic-data-kit` to create the question-answer dataset.\n",
        "\n",
        "**`--num-pairs 25 --type \"qa\"`**: These flags are the specific instructions. They tell the model to generate exactly **25 unique Question-Answer pairs** from the text in the current file.\n",
        "\n",
        "**`time.sleep(2)`**: After generating the data for one file, the code pauses for 2 seconds. This is a small safety measure to give the system a moment to finish processing and avoid any potential issues before starting the next file."
      ],
      "metadata": {
        "id": "26y28G6ceFUr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk_ikUqrRv3J",
        "outputId": "da01b6f6-2af6-41c2-f431-1ef129a3b0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:39228 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39242 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[2K\u001b[32m‚†π\u001b[0m Generating qa content from data/output/sherlock_5.txt...vLLM STDOUT: INFO 09-25 08:40:38 [chat_utils.py:444] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
            "vLLM STDOUT: INFO 09-25 08:40:38 [logger.py:43] Received request chatcmpl-34ad77e7c5d1478087d9517f0bfe1467: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSherlockiana : The Baker Street Journal, an Irregular Quarterly of Sherlockiana Sherlock Holmes and Sherlockiana Collection at the Harry Ransom Center Digital Collections The Universal Sherlock Holmes at the University of Minnesota<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:40:38 [engine.py:317] Added request chatcmpl-34ad77e7c5d1478087d9517f0bfe1467.\n",
            "\u001b[2K\u001b[32m‚†á\u001b[0m Generating qa content from data/output/sherlock_5.txt...vLLM STDOUT: INFO:     127.0.0.1:39246 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m‚†è\u001b[0m Generating qa content from data/output/sherlock_5.txt...vLLM STDOUT: INFO 09-25 08:40:53 [logger.py:43] Received request chatcmpl-89662222b79d47238a648683ebf81774: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nSherlockiana : The Baker Street Journal, an Irregular Quarterly of Sherlockiana Sherlock Holmes and Sherlockiana Collection at the Harry Ransom Center Digital Collections The Universal Sherlock Holmes at the University of Minnesota<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:40:53 [engine.py:317] Added request chatcmpl-89662222b79d47238a648683ebf81774.\n",
            "\u001b[2K\u001b[32m‚†ô\u001b[0m Generating qa content from data/output/sherlock_5.txt...vLLM STDOUT: INFO:     127.0.0.1:38148 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 17 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_5_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_5_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†∏\u001b[0m Generating qa content from data/output/sherlock_5.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_5_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:35538 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:35554 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:41:10 [logger.py:43] Received request chatcmpl-a48e22cfca82447299994344ea9944ef: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe Dynamics of an Asteroid : The Dynamics of an Asteroid is a fictional book by Professor James Moriarty, the implacable foe of Sherlock Holmes. The only mention of it in Arthur Conan Doyle\\'s original Holmes stories is in The Valley of Fear (written in 1914, but set in 1888) when Holmes says of Moriarty: Is he not the celebrated author of The Dynamics of an Asteroid, a book which ascends to such rarefied heights of pure mathematics that it is said that there was no man in the scientific press capable of criticizing it? Participants in the \"Sherlockian game\", where Sherlock Holmes fans elaborate on elements within Doyle\\'s stories, have suggested other details about The Dynamics of an Asteroid.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:41:10 [engine.py:317] Added request chatcmpl-a48e22cfca82447299994344ea9944ef.\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Generating qa content from data/output/sherlock_6.txt...vLLM STDOUT: INFO:     127.0.0.1:35564 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:41:14 [logger.py:43] Received request chatcmpl-4900699936974a44bc8c319a5bb5fa71: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nThe Dynamics of an Asteroid : The Dynamics of an Asteroid is a fictional book by Professor James Moriarty, the implacable foe of Sherlock Holmes. The only mention of it in Arthur Conan Doyle\\'s original Holmes stories is in The Valley of Fear (written in 1914, but set in 1888) when Holmes says of Moriarty: Is he not the celebrated author of The Dynamics of an Asteroid, a book which ascends to such rarefied heights of pure mathematics that it is said that there was no man in the scientific press capable of criticizing it? Participants in the \"Sherlockian game\", where Sherlock Holmes fans elaborate on elements within Doyle\\'s stories, have suggested other details about The Dynamics of an Asteroid.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:41:14 [engine.py:317] Added request chatcmpl-4900699936974a44bc8c319a5bb5fa71.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m‚†á\u001b[0m Generating qa content from data/output/sherlock_6.txt...vLLM STDOUT: INFO:     127.0.0.1:35580 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 16 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_6_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_6_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†ô\u001b[0m Generating qa content from data/output/sherlock_6.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_6_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:48518 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:48520 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:41:31 [logger.py:43] Received request chatcmpl-baf81e752c9b4cd3a006ea4278c97037: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAdaptations of Sherlock Holmes : The stories of Sherlock Holmes by Sir Arthur Conan Doyle have been very popular as adaptations for the stage, and later film, and still later television. The four volumes of the Universal Sherlock Holmes (1995) compiled by Ronald B. De Waal lists over 25,000 Holmes-related productions and products. They include the original writings, \"together with the translations of these tales into sixty-three languages, plus Braille and shorthand, the writings about the Writings or higher criticism, writings about Sherlockians and their societies, memorials and memorabilia, games, puzzles and quizzes, phonograph records, audio and video tapes, compact discs, laser discs, ballets, films, musicals, operettas, oratorios, plays, radio and television programs, parodies and pastiches, children\\'s books, cartoons, comics, and a multitude of other items ‚Äî from advertisements to wine ‚Äî that have accumulated throughout the world on the two most famous characters in literature.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:41:31 [engine.py:317] Added request chatcmpl-baf81e752c9b4cd3a006ea4278c97037.\n",
            "\u001b[2K\u001b[32m‚†ô\u001b[0m Generating qa content from data/output/sherlock_10.txt...vLLM STDOUT: INFO:     127.0.0.1:48526 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:41:35 [logger.py:43] Received request chatcmpl-debe8674e32949699ca41d2539aa56ea: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nAdaptations of Sherlock Holmes : The stories of Sherlock Holmes by Sir Arthur Conan Doyle have been very popular as adaptations for the stage, and later film, and still later television. The four volumes of the Universal Sherlock Holmes (1995) compiled by Ronald B. De Waal lists over 25,000 Holmes-related productions and products. They include the original writings, \"together with the translations of these tales into sixty-three languages, plus Braille and shorthand, the writings about the Writings or higher criticism, writings about Sherlockians and their societies, memorials and memorabilia, games, puzzles and quizzes, phonograph records, audio and video tapes, compact discs, laser discs, ballets, films, musicals, operettas, oratorios, plays, radio and television programs, parodies and pastiches, children\\'s books, cartoons, comics, and a multitude of other items ‚Äî from advertisements to wine ‚Äî that have accumulated throughout the world on the two most famous characters in literature.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:41:35 [engine.py:317] Added request chatcmpl-debe8674e32949699ca41d2539aa56ea.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m‚†¶\u001b[0m Generating qa content from data/output/sherlock_10.txt...vLLM STDOUT: INFO:     127.0.0.1:48534 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 12 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_10_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_10_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†è\u001b[0m Generating qa content from data/output/sherlock_10.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_10_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:60346 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:60350 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:41:52 [logger.py:43] Received request chatcmpl-03ddd684cb424f6da65f6acc199397b7: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe Dynamics of an Asteroid : In 1809, Carl Friedrich Gauss wrote a ground-breaking treatise on the dynamics of an asteroid (Ceres). However, Gauss\\'s method was understood immediately and is still used today. Two decades before Arthur Conan Doyle\\'s writing, the Canadian-American dynamic astronomer Simon Newcomb had published a series of books analyzing motions of planets in the Solar System. The notoriously spiteful Newcomb could have been an inspiration for Professor Moriarty. In 1887, Henri Poincar√©\\'s submission to the celestial mechanics contest of King Oscar II of Sweden investigated the three-body problem, a theoretical basis of asteroid dynamics. It was also hard to criticize, as the jury (Weirstrass, Mittag-Leffler, and Hermite, all top-notch mathematicians) and the author himself missed a fatal error in the submission (later corrected). Another example of mathematics too abstruse to be criticized is the letters of Srinivasa Ramanujan, sent to several mathematicians at the University of Cambridge in 1913. Only one of these mathematicians, G. H. Hardy, even recognized their merit. Despite being experts in the branches of mathematics used, he and J. E. Littlewood added that many of them \"defeated me completely; I had never seen anything in the least like them before.\" It has taken over a century for this work to be understood; the last sub-field (and the last problem of the last sub-field) have been referred to as The Final Problem in explicit reference to the Sherlock Holmes story. Holmes only states that \"it is said\" (emphasis added) that no one in the scientific press was capable of criticizing Moriarty\\'s work; he stops short of recognizing the claim as indisputably accurate. Similarly, when it was jocularly suggested to Arthur Eddington in 1919 that he was one of only three people in the world who understood Albert Einstein\\'s theory of relativity, Eddington quipped that he could not think who the third person was.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:41:52 [engine.py:317] Added request chatcmpl-03ddd684cb424f6da65f6acc199397b7.\n",
            "\u001b[2K\u001b[32m‚†º\u001b[0m Generating qa content from data/output/sherlock_7.txt...vLLM STDOUT: INFO:     127.0.0.1:60362 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:41:57 [logger.py:43] Received request chatcmpl-79c1893841c44ad6a530b082e3b4b6eb: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nThe Dynamics of an Asteroid : In 1809, Carl Friedrich Gauss wrote a ground-breaking treatise on the dynamics of an asteroid (Ceres). However, Gauss\\'s method was understood immediately and is still used today. Two decades before Arthur Conan Doyle\\'s writing, the Canadian-American dynamic astronomer Simon Newcomb had published a series of books analyzing motions of planets in the Solar System. The notoriously spiteful Newcomb could have been an inspiration for Professor Moriarty. In 1887, Henri Poincar√©\\'s submission to the celestial mechanics contest of King Oscar II of Sweden investigated the three-body problem, a theoretical basis of asteroid dynamics. It was also hard to criticize, as the jury (Weirstrass, Mittag-Leffler, and Hermite, all top-notch mathematicians) and the author himself missed a fatal error in the submission (later corrected). Another example of mathematics too abstruse to be criticized is the letters of Srinivasa Ramanujan, sent to several mathematicians at the University of Cambridge in 1913. Only one of these mathematicians, G. H. Hardy, even recognized their merit. Despite being experts in the branches of mathematics used, he and J. E. Littlewood added that many of them \"defeated me completely; I had never seen anything in the least like them before.\" It has taken over a century for this work to be understood; the last sub-field (and the last problem of the last sub-field) have been referred to as The Final Problem in explicit reference to the Sherlock Holmes story. Holmes only states that \"it is said\" (emphasis added) that no one in the scientific press was capable of criticizing Moriarty\\'s work; he stops short of recognizing the claim as indisputably accurate. Similarly, when it was jocularly suggested to Arthur Eddington in 1919 that he was one of only three people in the world who understood Albert Einstein\\'s theory of relativity, Eddington quipped that he could not think who the third person was.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m‚†¥\u001b[0m Generating qa content from data/output/sherlock_7.txt...vLLM STDOUT: INFO 09-25 08:41:57 [engine.py:317] Added request chatcmpl-79c1893841c44ad6a530b082e3b4b6eb.\n",
            "\u001b[2K\u001b[32m‚†ô\u001b[0m Generating qa content from data/output/sherlock_7.txt...vLLM STDOUT: INFO:     127.0.0.1:60366 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 11 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_7_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_7_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†º\u001b[0m Generating qa content from data/output/sherlock_7.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_7_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:45034 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:45042 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:42:14 [logger.py:43] Received request chatcmpl-f131b5d8b9404442881245bd4c2ca1dc: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSherlockiana : Non-canonical works of fiction featuring Sherlock Holmes, by creators other than Arthur Conan Doyle, have been referred to as examples of \"Sherlockiana\". Charles Spencer, former theatre critic for The Daily Telegraph, used the term to refer to the 2009‚Äì12 releases of the novel The House of Silk, the television series Sherlock, and two Sherlock Holmes films, Sherlock Holmes and its sequel Sherlock Holmes: A Game of Shadows, as representative of a \"golden age of Sherlockiana.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:42:14 [engine.py:317] Added request chatcmpl-f131b5d8b9404442881245bd4c2ca1dc.\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Generating qa content from data/output/sherlock_1.txt...vLLM STDOUT: INFO:     127.0.0.1:45056 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:42:17 [logger.py:43] Received request chatcmpl-4ab755c2edf2441aa2322559ca44cfa6: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nSherlockiana : Non-canonical works of fiction featuring Sherlock Holmes, by creators other than Arthur Conan Doyle, have been referred to as examples of \"Sherlockiana\". Charles Spencer, former theatre critic for The Daily Telegraph, used the term to refer to the 2009‚Äì12 releases of the novel The House of Silk, the television series Sherlock, and two Sherlock Holmes films, Sherlock Holmes and its sequel Sherlock Holmes: A Game of Shadows, as representative of a \"golden age of Sherlockiana.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:42:17 [engine.py:317] Added request chatcmpl-4ab755c2edf2441aa2322559ca44cfa6.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m‚†ã\u001b[0m Generating qa content from data/output/sherlock_1.txt...vLLM STDOUT: INFO:     127.0.0.1:45060 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 14 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_1_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_1_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†∏\u001b[0m Generating qa content from data/output/sherlock_1.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_1_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59956 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59962 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:42:35 [logger.py:43] Received request chatcmpl-51d380ccddd547a3b8ea865908108d39: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe Dynamics of an Asteroid : Doyle provided no indication of the contents of Dynamics other than its title. Speculation about its contents published by later authors includes: \"The Ultimate Crime\", short story by Isaac Asimov, in More Tales of the Black Widowers, and republished in Sherlock Holmes through Time and Space. \"The Dynamics of an Asteroid\", short story by Robert Bloch, The Baker Street Journal, 1953, and also found in The Game is Afoot. \"The Adventure of the Russian Grave\", a short story by William Barton and Michael Capobianco, collected in Sherlock Holmes in Orbit. In the novel Spider-Man: The Revenge of the Sinister Six, by Adam-Troy Castro, a veiled reference is made to Moriarty and his Dynamics. Here the work is said to still be the authority on orbital bombardment. Physicist Alejandro Jenkins in 2013 suggested chaos theory, an esoteric branch of mathematics whose association with asteroid dynamics was not appreciated by real-world mathematicians until the work of Henri Poincar√© in 1890. Simon P. Norton and Alain Goriely have each suggested that the book might have been Moriarty\\'s submission to the 1887 celestial mechanics contest of King Oscar II of Sweden (which was won by Henri Poincar√©.)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:42:35 [engine.py:317] Added request chatcmpl-51d380ccddd547a3b8ea865908108d39.\n",
            "\u001b[2K\u001b[32m‚†º\u001b[0m Generating qa content from data/output/sherlock_8.txt...vLLM STDOUT: INFO:     127.0.0.1:59966 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:42:39 [logger.py:43] Received request chatcmpl-ed363d647ba448818ebb799e307cabd9: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nThe Dynamics of an Asteroid : Doyle provided no indication of the contents of Dynamics other than its title. Speculation about its contents published by later authors includes: \"The Ultimate Crime\", short story by Isaac Asimov, in More Tales of the Black Widowers, and republished in Sherlock Holmes through Time and Space. \"The Dynamics of an Asteroid\", short story by Robert Bloch, The Baker Street Journal, 1953, and also found in The Game is Afoot. \"The Adventure of the Russian Grave\", a short story by William Barton and Michael Capobianco, collected in Sherlock Holmes in Orbit. In the novel Spider-Man: The Revenge of the Sinister Six, by Adam-Troy Castro, a veiled reference is made to Moriarty and his Dynamics. Here the work is said to still be the authority on orbital bombardment. Physicist Alejandro Jenkins in 2013 suggested chaos theory, an esoteric branch of mathematics whose association with asteroid dynamics was not appreciated by real-world mathematicians until the work of Henri Poincar√© in 1890. Simon P. Norton and Alain Goriely have each suggested that the book might have been Moriarty\\'s submission to the 1887 celestial mechanics contest of King Oscar II of Sweden (which was won by Henri Poincar√©.)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:42:39 [engine.py:317] Added request chatcmpl-ed363d647ba448818ebb799e307cabd9.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m‚†ã\u001b[0m Generating qa content from data/output/sherlock_8.txt...vLLM STDOUT: INFO:     127.0.0.1:39762 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 14 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_8_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_8_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†∏\u001b[0m Generating qa content from data/output/sherlock_8.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_8_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:58254 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:58268 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:42:57 [logger.py:43] Received request chatcmpl-e7411e78b971466b933fdd7ce0417d42: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSherlockiana : The interest in works about Sherlock Holmes has extended to intrigue by the United States Smithsonian Museums about the original location of 221B Baker Street. The investigation found that the supposed location of Holmes and Watson\\'s flat did not exist during the early stories such as A Study in Scarlet. However, in 1990, the Sherlock Holmes International Society opened up the Sherlock Holmes Museum at 221B Baker Street. Furthermore, statues of Holmes displays \"Sherlockian\" culture as idolizing elements of the world.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:42:57 [engine.py:317] Added request chatcmpl-e7411e78b971466b933fdd7ce0417d42.\n",
            "\u001b[2K\u001b[32m‚†á\u001b[0m Generating qa content from data/output/sherlock_4.txt...vLLM STDOUT: INFO:     127.0.0.1:58284 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:43:00 [logger.py:43] Received request chatcmpl-91f97fae7500413bb096a6111f5b1aab: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nSherlockiana : The interest in works about Sherlock Holmes has extended to intrigue by the United States Smithsonian Museums about the original location of 221B Baker Street. The investigation found that the supposed location of Holmes and Watson\\'s flat did not exist during the early stories such as A Study in Scarlet. However, in 1990, the Sherlock Holmes International Society opened up the Sherlock Holmes Museum at 221B Baker Street. Furthermore, statues of Holmes displays \"Sherlockian\" culture as idolizing elements of the world.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m‚†è\u001b[0m Generating qa content from data/output/sherlock_4.txt...vLLM STDOUT: INFO 09-25 08:43:00 [engine.py:317] Added request chatcmpl-91f97fae7500413bb096a6111f5b1aab.\n",
            "\u001b[2K\u001b[32m‚†º\u001b[0m Generating qa content from data/output/sherlock_4.txt...vLLM STDOUT: INFO:     127.0.0.1:37662 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 15 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_4_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_4_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†ß\u001b[0m Generating qa content from data/output/sherlock_4.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_4_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43516 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43530 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:43:17 [logger.py:43] Received request chatcmpl-b427f6978d774b25a3579f9ff6861715: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSherlockiana : Sherlockiana encompasses various categories of materials and content related to the fictional detective Sherlock Holmes, created by Arthur Conan Doyle. The word \"Sherlockiana\" has been used for literary studies and scholarship concerning Sherlock Holmes, Sherlock Holmes pastiches in print and other media such as films, and memorabilia associated with Sherlock Holmes. Sherlockiana may be \"anything about, inspired by, or tangentially concerning\" Sherlock Holmes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:43:17 [engine.py:317] Added request chatcmpl-b427f6978d774b25a3579f9ff6861715.\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Generating qa content from data/output/sherlock_0.txt...vLLM STDOUT: INFO:     127.0.0.1:43546 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:43:20 [logger.py:43] Received request chatcmpl-c0ae65b80c3c446e9381f49a10a515fa: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nSherlockiana : Sherlockiana encompasses various categories of materials and content related to the fictional detective Sherlock Holmes, created by Arthur Conan Doyle. The word \"Sherlockiana\" has been used for literary studies and scholarship concerning Sherlock Holmes, Sherlock Holmes pastiches in print and other media such as films, and memorabilia associated with Sherlock Holmes. Sherlockiana may be \"anything about, inspired by, or tangentially concerning\" Sherlock Holmes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m‚†ß\u001b[0m Generating qa content from data/output/sherlock_0.txt...vLLM STDOUT: INFO 09-25 08:43:20 [engine.py:317] Added request chatcmpl-c0ae65b80c3c446e9381f49a10a515fa.\n",
            "\u001b[2K\u001b[32m‚†∏\u001b[0m Generating qa content from data/output/sherlock_0.txt...vLLM STDOUT: INFO:     127.0.0.1:48600 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 16 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Generating qa content from data/output/sherlock_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:33370 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:33384 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:43:38 [logger.py:43] Received request chatcmpl-9cfd915d2e014a58aaf37e36945c2d8c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSherlockiana : The term \"Sherlockiana\" has been used to refer to objects connected to Sherlock Holmes. Collections of Sherlockiana may include audio-visual recordings, books, magazines, newspaper clippings, art, clothing, advertising, stationery, and any other items associated with Holmes. The University of Minnesota contains the world\\'s largest archive of Sherlockiana as of 2015, a large portion of which was bequeathed by American collector John Bennett Shaw upon his death in 1994.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:43:38 [engine.py:317] Added request chatcmpl-9cfd915d2e014a58aaf37e36945c2d8c.\n",
            "\u001b[2K\u001b[32m‚†ô\u001b[0m Generating qa content from data/output/sherlock_3.txt...vLLM STDOUT: INFO:     127.0.0.1:33390 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:43:41 [logger.py:43] Received request chatcmpl-236bae4e77bb47d4958d3b46f6950e97: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nSherlockiana : The term \"Sherlockiana\" has been used to refer to objects connected to Sherlock Holmes. Collections of Sherlockiana may include audio-visual recordings, books, magazines, newspaper clippings, art, clothing, advertising, stationery, and any other items associated with Holmes. The University of Minnesota contains the world\\'s largest archive of Sherlockiana as of 2015, a large portion of which was bequeathed by American collector John Bennett Shaw upon his death in 1994.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:43:41 [engine.py:317] Added request chatcmpl-236bae4e77bb47d4958d3b46f6950e97.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m‚†ß\u001b[0m Generating qa content from data/output/sherlock_3.txt...vLLM STDOUT: INFO:     127.0.0.1:48210 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 15 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_3_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_3_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†ã\u001b[0m Generating qa content from data/output/sherlock_3.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_3_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:36334 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:36346 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:43:59 [logger.py:43] Received request chatcmpl-daf330cfc1a342a1aaf62ee9ab416311: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSherlockiana : When used to refer to literary studies, \"Sherlockiana\" includes essays and works about Sherlock Holmes such as Vincent Starrett\\'s 1933 book The Private Life of Sherlock Holmes. Some of these studies concern the Sherlockian game, a pastime of attempting to resolve anomalies and clarify implied details about Holmes and Watson. The word is used in the title of The Encyclopaedia Sherlockiana, first published in 1977 and republished as The Ultimate Sherlock Holmes Encyclopedia in 1987, a reference text containing an exhaustive list of over 3,500 people, places, and things associated with the universe of Sherlock Holmes. The quarterly journal The Baker Street Journal is subtitled An Irregular Quarterly of Sherlockiana.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:43:59 [engine.py:317] Added request chatcmpl-daf330cfc1a342a1aaf62ee9ab416311.\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Generating qa content from data/output/sherlock_2.txt...vLLM STDOUT: INFO:     127.0.0.1:36360 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:44:02 [logger.py:43] Received request chatcmpl-2f128fc7f3364cd3904b3c2945605952: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nSherlockiana : When used to refer to literary studies, \"Sherlockiana\" includes essays and works about Sherlock Holmes such as Vincent Starrett\\'s 1933 book The Private Life of Sherlock Holmes. Some of these studies concern the Sherlockian game, a pastime of attempting to resolve anomalies and clarify implied details about Holmes and Watson. The word is used in the title of The Encyclopaedia Sherlockiana, first published in 1977 and republished as The Ultimate Sherlock Holmes Encyclopedia in 1987, a reference text containing an exhaustive list of over 3,500 people, places, and things associated with the universe of Sherlock Holmes. The quarterly journal The Baker Street Journal is subtitled An Irregular Quarterly of Sherlockiana.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:44:02 [engine.py:317] Added request chatcmpl-2f128fc7f3364cd3904b3c2945605952.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m‚†π\u001b[0m Generating qa content from data/output/sherlock_2.txt...vLLM STDOUT: INFO:     127.0.0.1:36366 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 16 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_2_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_2_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Generating qa content from data/output/sherlock_2.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_2_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54664 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54670 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:44:20 [logger.py:43] Received request chatcmpl-6b278c23a6114659b8dcf99e82cf6c28: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe Dynamics of an Asteroid : List of many references to Dynamics, as well as other works of Moriarty, Holmes, and others.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:44:20 [engine.py:317] Added request chatcmpl-6b278c23a6114659b8dcf99e82cf6c28.\n",
            "\u001b[2K\u001b[32m‚†á\u001b[0m Generating qa content from data/output/sherlock_9.txt...vLLM STDOUT: INFO:     127.0.0.1:54682 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m‚†è\u001b[0m Generating qa content from data/output/sherlock_9.txt...vLLM STDOUT: INFO 09-25 08:44:25 [logger.py:43] Received request chatcmpl-493525d97ea1437784bb461e59ef695e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nThe Dynamics of an Asteroid : List of many references to Dynamics, as well as other works of Moriarty, Holmes, and others.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:44:25 [engine.py:317] Added request chatcmpl-493525d97ea1437784bb461e59ef695e.\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Generating qa content from data/output/sherlock_9.txt...vLLM STDOUT: INFO:     127.0.0.1:54698 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 19 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/sherlock_9_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/sherlock_9_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†á\u001b[0m Generating qa content from data/output/sherlock_9.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/sherlock_9_qa_pairs.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Process chunks\n",
        "for filename in filenames:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create {filename} \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\"\n",
        "    time.sleep(2) # Sleep some time to leave some room for processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell performs an automated **quality check** on the synthetic data you just created. Its purpose is to **filter out and remove any low-quality or irrelevant question-answer pairs**, ensuring the final dataset is clean and effective for fine-tuning.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1.  **The Loop:** The code loops through each of the `_qa_pairs.json` files that were generated in the previous step.\n",
        "\n",
        "2.  **The `curate` Command:** For each file, it runs the `synthetic-data-kit curate` command. This command uses a powerful language model (acting as a \"judge\") to read each question-answer pair and assign it a quality score.\n",
        "\n",
        "3.  **The Threshold:** The `--threshold 5.0` setting is the crucial instruction: it tells the tool to **discard any QA pair with a quality score below 5.0**."
      ],
      "metadata": {
        "id": "_6t6pJcmfzh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD-qc85VsRCT",
        "outputId": "cd8f7a82-4f10-4558-e8da-b14ed5273e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:56882 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:56898 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:44:42 [logger.py:43] Received request chatcmpl-f1895ef507f340e6b2eb87864a987a6e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What does Sherlockiana encompass?\",\\n    \"answer\": \"various categories of materials and content related to the fictional detective Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Who created the fictional detective Sherlock Holmes?\",\\n    \"answer\": \"Arthur Conan Doyle\"\\n  },\\n  {\\n    \"question\": \"What is Sherlockiana used for?\",\\n    \"answer\": \"literary studies and scholarship concerning Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Where is Sherlockiana used besides literary studies?\",\\n    \"answer\": \"Sherlock Holmes pastiches in print and other media such as films\"\\n  },\\n  {\\n    \"question\": \"What kind of memorabilia is associated with Sherlock Holmes?\",\\n    \"answer\": \"memorabilia associated with Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"What can be considered part of Sherlockiana?\",\\n    \"answer\": \"anything about, inspired by, or tangentially concerning Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"What is a Sherlock Holmes pastiche?\",\\n    \"answer\": \"not mentioned in the text\"\\n  },\\n  {\\n    \"question\": \"What is the text primarily about?\",\\n    \"answer\": \"Sherlockiana\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:44:42 [engine.py:317] Added request chatcmpl-f1895ef507f340e6b2eb87864a987a6e.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m‚†º\u001b[0m Cleaning content from data/generated/sherlock_0_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:56904 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:44:51 [logger.py:43] Received request chatcmpl-5540158be74a4c4c864a34b56c97c4ca: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Who wrote about Sherlock Holmes?\",\\n    \"answer\": \"Arthur Conan Doyle\"\\n  },\\n  {\\n    \"question\": \"What is the main focus of the text?\",\\n    \"answer\": \"Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"What is the definition of Sherlockiana?\",\\n    \"answer\": \"not explicitly stated in the text\"\\n  },\\n  {\\n    \"question\": \"What is the purpose of using the term \\'Sherlockiana\\'?\",\\n    \"answer\": \"to describe materials and content related to Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"What is a Sherlock Holmes pastiche?\",\\n    \"answer\": \"not mentioned in the text\"\\n  },\\n  {\\n    \"question\": \"How is Sherlockiana related to the media?\",\\n    \"answer\": \"Sherlock Holmes pastiches in print and other media such as films\"\\n  },\\n  {\\n    \"question\": \"What type of content is associated with Sherlock Holmes?\",\\n    \"answer\": \"memorabilia associated with Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"What is the range of topics covered by Sherlockiana?\",\\n    \"answer\": \"anything about, inspired by, or tangentially concerning Sherlock Holmes\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:44:51 [engine.py:317] Added request chatcmpl-5540158be74a4c4c864a34b56c97c4ca.\n",
            "\u001b[2K\u001b[32m‚†ã\u001b[0m Cleaning content from data/generated/sherlock_0_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:44214 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 16 QA pairs\n",
            "\u001b[2KRetained 16 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.4\n",
            "\u001b[2K\u001b[32m‚†π\u001b[0m Cleaning content from data/generated/sherlock_0_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_0_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:47058 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:47064 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:45:01 [logger.py:43] Received request chatcmpl-95080ab2b6a84340b5809e3a64b97944: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the term used to refer to non-canonical works of fiction featuring Sherlock Holmes?\",\\n    \"answer\": \"Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"Who used the term \\'Sherlockiana\\' to refer to non-canonical works?\",\\n    \"answer\": \"Charles Spencer\"\\n  },\\n  {\\n    \"question\": \"What works were referred to as representative of a \\'golden age of Sherlockiana\\'?\",\\n    \"answer\": \"The House of Silk, Sherlock, Sherlock Holmes and Sherlock Holmes: A Game of Shadows\"\\n  },\\n  {\\n    \"question\": \"In what years did the term \\'Sherlockiana\\' refer to the mentioned works?\",\\n    \"answer\": \"2009\\\\u201312\"\\n  },\\n  {\\n    \"question\": \"Who was the theatre critic for The Daily Telegraph?\",\\n    \"answer\": \"Charles Spencer\"\\n  },\\n  {\\n    \"question\": \"What publication was Charles Spencer a theatre critic for?\",\\n    \"answer\": \"The Daily Telegraph\"\\n  },\\n  {\\n    \"question\": \"What is the name of the novel that was part of the \\'golden age of Sherlockiana\\'?\",\\n    \"answer\": \"The House of Silk\"\\n  },\\n  {\\n    \"question\": \"What is the name of the television series that was part of the \\'golden age of Sherlockiana\\'?\",\\n    \"answer\": \"Sherlock\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:45:01 [engine.py:317] Added request chatcmpl-95080ab2b6a84340b5809e3a64b97944.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m‚†ß\u001b[0m Cleaning content from data/generated/sherlock_1_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:47068 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2K\u001b[32m‚†á\u001b[0m Cleaning content from data/generated/sherlock_1_qa_pairs.json...vLLM STDOUT: INFO 09-25 08:45:11 [logger.py:43] Received request chatcmpl-32a7a3bf46fc4ca7876a4bd55fa1bbf8: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"How many Sherlock Holmes films were part of the \\'golden age of Sherlockiana\\'?\",\\n    \"answer\": \"2\"\\n  },\\n  {\\n    \"question\": \"What is the name of the first Sherlock Holmes film that was part of the \\'golden age of Sherlockiana\\'?\",\\n    \"answer\": \"Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"What is the name of the sequel to the first Sherlock Holmes film?\",\\n    \"answer\": \"Sherlock Holmes: A Game of Shadows\"\\n  },\\n  {\\n    \"question\": \"What is the term used to describe the works mentioned?\",\\n    \"answer\": \"Non-canonical works of fiction featuring Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Who referred to the works as non-canonical?\",\\n    \"answer\": \"Creators other than Arthur Conan Doyle\"\\n  },\\n  {\\n    \"question\": \"What is the term used to describe these works?\",\\n    \"answer\": \"Non-canonical\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:45:11 [engine.py:317] Added request chatcmpl-32a7a3bf46fc4ca7876a4bd55fa1bbf8.\n",
            "\u001b[2K\u001b[32m‚†ã\u001b[0m Cleaning content from data/generated/sherlock_1_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:41874 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 14 QA pairs\n",
            "\u001b[2KRetained 14 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.4\n",
            "\u001b[2K\u001b[32m‚†π\u001b[0m Cleaning content from data/generated/sherlock_1_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_1_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39810 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39826 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:45:19 [logger.py:43] Received request chatcmpl-ff04e89633c24a018a5f55ce862e50a5: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is Sherlockiana?\",\\n    \"answer\": \"Literary studies of Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Who wrote the book The Private Life of Sherlock Holmes?\",\\n    \"answer\": \"Vincent Starrett\"\\n  },\\n  {\\n    \"question\": \"When was The Encyclopaedia Sherlockiana first published?\",\\n    \"answer\": \"1977\"\\n  },\\n  {\\n    \"question\": \"What is the Baker Street Journal?\",\\n    \"answer\": \"A quarterly journal subtitled An Irregular Quarterly of Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"What is the purpose of the Sherlockian game?\",\\n    \"answer\": \"To resolve anomalies and clarify implied details about Holmes and Watson\"\\n  },\\n  {\\n    \"question\": \"How many people, places, and things are listed in The Ultimate Sherlock Holmes Encyclopedia?\",\\n    \"answer\": \"Over 3,500\"\\n  },\\n  {\\n    \"question\": \"When was The Ultimate Sherlock Holmes Encyclopedia republished?\",\\n    \"answer\": \"1987\"\\n  },\\n  {\\n    \"question\": \"What is The Baker Street Journal subtitled?\",\\n    \"answer\": \"An Irregular Quarterly of Sherlockiana\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:45:19 [engine.py:317] Added request chatcmpl-ff04e89633c24a018a5f55ce862e50a5.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m‚†º\u001b[0m Cleaning content from data/generated/sherlock_2_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:39830 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:45:27 [logger.py:43] Received request chatcmpl-5d3af604d1054d359143da2cb93b8fc2: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"When was The Baker Street Journal first published?\",\\n    \"answer\": \"Not specified in the text\"\\n  },\\n  {\\n    \"question\": \"What is The Baker Street Journal?\",\\n    \"answer\": \"A quarterly journal\"\\n  },\\n  {\\n    \"question\": \"What is The Encyclopaedia Sherlockiana?\",\\n    \"answer\": \"A reference text\"\\n  },\\n  {\\n    \"question\": \"When was The Encyclopaedia Sherlockiana republished?\",\\n    \"answer\": \"1987\"\\n  },\\n  {\\n    \"question\": \"Who republished The Ultimate Sherlock Holmes Encyclopedia?\",\\n    \"answer\": \"Not specified in the text\"\\n  },\\n  {\\n    \"question\": \"What is The Private Life of Sherlock Holmes?\",\\n    \"answer\": \"A book about Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"When was The Private Life of Sherlock Holmes published?\",\\n    \"answer\": \"1933\"\\n  },\\n  {\\n    \"question\": \"What is The Ultimate Sherlock Holmes Encyclopedia?\",\\n    \"answer\": \"A reference text containing an exhaustive list of over 3,500 people, places, and things\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Cleaning content from data/generated/sherlock_2_qa_pairs.json...vLLM STDOUT: INFO 09-25 08:45:27 [engine.py:317] Added request chatcmpl-5d3af604d1054d359143da2cb93b8fc2.\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Cleaning content from data/generated/sherlock_2_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:39842 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 16 QA pairs\n",
            "\u001b[2KRetained 16 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.2\n",
            "\u001b[2K\u001b[32m‚†ß\u001b[0m Cleaning content from data/generated/sherlock_2_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_2_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39360 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39368 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:45:36 [logger.py:43] Received request chatcmpl-b2d5482c13b9419386fdf4e01c68d065: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is Sherlockiana?\",\\n    \"answer\": \"Objects connected to Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"What types of items can be part of a collection of Sherlockiana?\",\\n    \"answer\": \"Audio-visual recordings, books, magazines, newspaper clippings, art, clothing, advertising, stationery\"\\n  },\\n  {\\n    \"question\": \"What is the University of Minnesota\\'s collection of Sherlockiana?\",\\n    \"answer\": \"The world\\'s largest archive of Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"Who donated a significant portion of the University of Minnesota\\'s Sherlockiana collection?\",\\n    \"answer\": \"John Bennett Shaw\"\\n  },\\n  {\\n    \"question\": \"When did John Bennett Shaw pass away?\",\\n    \"answer\": \"1994\"\\n  },\\n  {\\n    \"question\": \"When was the information about the University of Minnesota\\'s Sherlockiana collection available?\",\\n    \"answer\": \"2015\"\\n  },\\n  {\\n    \"question\": \"What was bequeathed by John Bennett Shaw to the University of Minnesota?\",\\n    \"answer\": \"A large portion of his collection of Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"What is the purpose of the term \\'Sherlockiana\\'?\",\\n    \"answer\": \"To refer to objects connected to Sherlock Holmes\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:45:36 [engine.py:317] Added request chatcmpl-b2d5482c13b9419386fdf4e01c68d065.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Cleaning content from data/generated/sherlock_3_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:39378 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:45:45 [logger.py:43] Received request chatcmpl-838c29a92d8d4a6b9eeb0975d7b63729: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What types of media can be part of a collection of Sherlockiana?\",\\n    \"answer\": \"Audio-visual recordings, books, magazines\"\\n  },\\n  {\\n    \"question\": \"What is included in a collection of Sherlockiana, according to the text?\",\\n    \"answer\": \"Any items associated with Holmes\"\\n  },\\n  {\\n    \"question\": \"Where is the University of Minnesota\\'s collection of Sherlockiana located?\",\\n    \"answer\": \"Not specified in the text\"\\n  },\\n  {\\n    \"question\": \"What year was the University of Minnesota\\'s Sherlockiana collection evaluated?\",\\n    \"answer\": \"2015\"\\n  },\\n  {\\n    \"question\": \"What is the significance of John Bennett Shaw\\'s donation to the University of Minnesota?\",\\n    \"answer\": \"The University of Minnesota contains the world\\'s largest archive of Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"What is the date of John Bennett Shaw\\'s death?\",\\n    \"answer\": \"1994\"\\n  },\\n  {\\n    \"question\": \"Who is John Bennett Shaw?\",\\n    \"answer\": \"An American collector of Sherlockiana\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2K\u001b[32m‚†¶\u001b[0m Cleaning content from data/generated/sherlock_3_qa_pairs.json...vLLM STDOUT: INFO 09-25 08:45:45 [engine.py:317] Added request chatcmpl-838c29a92d8d4a6b9eeb0975d7b63729.\n",
            "\u001b[2K\u001b[32m‚†π\u001b[0m Cleaning content from data/generated/sherlock_3_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:45920 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 15 QA pairs\n",
            "\u001b[2KRetained 15 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.3\n",
            "\u001b[2K\u001b[32m‚†º\u001b[0m Cleaning content from data/generated/sherlock_3_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_3_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59658 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59666 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:45:54 [logger.py:43] Received request chatcmpl-90f2cf0cf3c2460fb26ed2e1ff3b17d3: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What institution is intrigued by the original location of 221B Baker Street?\",\\n    \"answer\": \"The United States Smithsonian Museums\"\\n  },\\n  {\\n    \"question\": \"When did the Sherlock Holmes International Society open the Sherlock Holmes Museum at 221B Baker Street?\",\\n    \"answer\": \"1990\"\\n  },\\n  {\\n    \"question\": \"What was found about the supposed location of Holmes and Watson\\'s flat during the investigation?\",\\n    \"answer\": \"It did not exist during the early stories such as A Study in Scarlet\"\\n  },\\n  {\\n    \"question\": \"What type of culture is represented by statues of Holmes?\",\\n    \"answer\": \"Sherlockian culture\"\\n  },\\n  {\\n    \"question\": \"What elements of the world are idolized in Sherlockian culture?\",\\n    \"answer\": \"Elements of the world\"\\n  },\\n  {\\n    \"question\": \"Where is the Sherlock Holmes Museum located?\",\\n    \"answer\": \"221B Baker Street\"\\n  },\\n  {\\n    \"question\": \"Is the supposed location of Holmes and Watson\\'s flat mentioned in the early stories?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"What type of stories mention the supposed location of Holmes and Watson\\'s flat?\",\\n    \"answer\": \"The early stories such as A Study in Scarlet\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:45:54 [engine.py:317] Added request chatcmpl-90f2cf0cf3c2460fb26ed2e1ff3b17d3.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m‚†º\u001b[0m Cleaning content from data/generated/sherlock_4_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:59680 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:04 [logger.py:43] Received request chatcmpl-29ef8ffe6c6a4d53b8c3c093b0f829d5: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Is the Sherlock Holmes Museum mentioned as existing at 221B Baker Street?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Is the United States Smithsonian Museums mentioned as being from the US?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Is the Sherlock Holmes International Society mentioned as being international?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Is the location of Holmes and Watson\\'s flat mentioned as not existing during the early stories?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Is Sherlockian culture mentioned as being idolizing elements of the world?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Is the Sherlock Holmes Museum mentioned as being opened by the Sherlock Holmes International Society?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Is 221B Baker Street mentioned as being the location of the Sherlock Holmes Museum?\",\\n    \"answer\": \"Yes\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:46:04 [engine.py:317] Added request chatcmpl-29ef8ffe6c6a4d53b8c3c093b0f829d5.\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Cleaning content from data/generated/sherlock_4_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:48234 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 15 QA pairs\n",
            "\u001b[2KRetained 15 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.4\n",
            "\u001b[2K\u001b[32m‚†¶\u001b[0m Cleaning content from data/generated/sherlock_4_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_4_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:33470 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:33482 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:12 [logger.py:43] Received request chatcmpl-cc9b68d9440048a88a40efd9a333f7c3: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the title of the Sherlockiana publication?\",\\n    \"answer\": \"The Baker Street Journal\"\\n  },\\n  {\\n    \"question\": \"How often is The Baker Street Journal published?\",\\n    \"answer\": \"Irregularly\"\\n  },\\n  {\\n    \"question\": \"Where is the Sherlock Holmes and Sherlockiana Collection housed?\",\\n    \"answer\": \"at the Harry Ransom Center\"\\n  },\\n  {\\n    \"question\": \"What digital collection is available for Sherlockiana?\",\\n    \"answer\": \"Digital Collections\"\\n  },\\n  {\\n    \"question\": \"What is the name of the digital collection of Sherlockiana?\",\\n    \"answer\": \"The Universal Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Where is The Universal Sherlock Holmes housed?\",\\n    \"answer\": \"at the University of Minnesota\"\\n  },\\n  {\\n    \"question\": \"What type of publication is The Baker Street Journal?\",\\n    \"answer\": \"Journal\"\\n  },\\n  {\\n    \"question\": \"What is the focus of The Baker Street Journal?\",\\n    \"answer\": \"Sherlockiana\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:46:12 [engine.py:317] Added request chatcmpl-cc9b68d9440048a88a40efd9a333f7c3.\n",
            "\u001b[?25lProcessing 3 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Cleaning content from data/generated/sherlock_5_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:33490 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:21 [logger.py:43] Received request chatcmpl-5d03e4f3a77e4f8cbcedf538d59f9bf2: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the frequency of publication for The Baker Street Journal?\",\\n    \"answer\": \"Quarterly\"\\n  },\\n  {\\n    \"question\": \"Where is the Sherlock Holmes and Sherlockiana Collection at the Harry Ransom Center located?\",\\n    \"answer\": \"Not specified\"\\n  },\\n  {\\n    \"question\": \"What type of collection is the Sherlock Holmes and Sherlockiana Collection at the Harry Ransom Center?\",\\n    \"answer\": \"Collection\"\\n  },\\n  {\\n    \"question\": \"Where is the Universal Sherlock Holmes at the University of Minnesota located?\",\\n    \"answer\": \"Not specified\"\\n  },\\n  {\\n    \"question\": \"What type of publication is The Universal Sherlock Holmes?\",\\n    \"answer\": \"Publication\"\\n  },\\n  {\\n    \"question\": \"What is the focus of The Universal Sherlock Holmes?\",\\n    \"answer\": \"Sherlockiana\"\\n  },\\n  {\\n    \"question\": \"What type of collection is the Digital Collections at the Harry Ransom Center?\",\\n    \"answer\": \"Digital\"\\n  },\\n  {\\n    \"question\": \"What is the focus of the Digital Collections at the Harry Ransom Center?\",\\n    \"answer\": \"Sherlockiana\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:46:21 [engine.py:317] Added request chatcmpl-5d03e4f3a77e4f8cbcedf538d59f9bf2.\n",
            "\u001b[2K\u001b[32m‚†ß\u001b[0m Cleaning content from data/generated/sherlock_5_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:36632 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:30 [logger.py:43] Received request chatcmpl-eca05a1344a64bba9e1aca27c1087f9a: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What type of publication is The Baker Street Journal?\",\\n    \"answer\": \"Publication\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2K\u001b[32m‚†á\u001b[0m Cleaning content from data/generated/sherlock_5_qa_pairs.json...vLLM STDOUT: INFO 09-25 08:46:30 [engine.py:317] Added request chatcmpl-eca05a1344a64bba9e1aca27c1087f9a.\n",
            "\u001b[2K\u001b[32m‚†ô\u001b[0m Cleaning content from data/generated/sherlock_5_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:54396 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 17 QA pairs\n",
            "\u001b[2KRetained 17 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.2\n",
            "\u001b[2K\u001b[32m‚†∏\u001b[0m Cleaning content from data/generated/sherlock_5_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_5_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54410 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54424 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:32 [logger.py:43] Received request chatcmpl-03ab56e4400b4740922214e00a5a9c74: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Who wrote The Dynamics of an Asteroid?\",\\n    \"answer\": \"Professor James Moriarty\"\\n  },\\n  {\\n    \"question\": \"In which year was The Dynamics of an Asteroid written?\",\\n    \"answer\": \"1914\"\\n  },\\n  {\\n    \"question\": \"In which year was The Dynamics of an Asteroid set?\",\\n    \"answer\": \"1888\"\\n  },\\n  {\\n    \"question\": \"What is the subject of The Dynamics of an Asteroid?\",\\n    \"answer\": \"Pure mathematics\"\\n  },\\n  {\\n    \"question\": \"According to the scientific press, who was the only man capable of criticizing The Dynamics of an Asteroid?\",\\n    \"answer\": \"There was no man in the scientific press capable of criticizing it\"\\n  },\\n  {\\n    \"question\": \"In which Sherlock Holmes story is The Dynamics of an Asteroid mentioned?\",\\n    \"answer\": \"The Valley of Fear\"\\n  },\\n  {\\n    \"question\": \"Who is the author of the Sherlockian game where details about The Dynamics of an Asteroid are elaborated?\",\\n    \"answer\": \"Sherlock Holmes fans\"\\n  },\\n  {\\n    \"question\": \"What is The Dynamics of an Asteroid said to ascend to?\",\\n    \"answer\": \"Rarefied heights\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:46:32 [engine.py:317] Added request chatcmpl-03ab56e4400b4740922214e00a5a9c74.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m‚†ô\u001b[0m Cleaning content from data/generated/sherlock_6_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:54432 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:41 [logger.py:43] Received request chatcmpl-8bc91f7b57aa4b2d994f41a496413fa4: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Who is the author of The Dynamics of an Asteroid according to Arthur Conan Doyle?\",\\n    \"answer\": \"Professor James Moriarty\"\\n  },\\n  {\\n    \"question\": \"What is The Dynamics of an Asteroid compared to?\",\\n    \"answer\": \"A book\"\\n  },\\n  {\\n    \"question\": \"Who is the protagonist of the Sherlockian game?\",\\n    \"answer\": \"Sherlock Holmes fans\"\\n  },\\n  {\\n    \"question\": \"What year was The Valley of Fear written?\",\\n    \"answer\": \"1914\"\\n  },\\n  {\\n    \"question\": \"Who is the author of the original Holmes stories?\",\\n    \"answer\": \"Arthur Conan Doyle\"\\n  },\\n  {\\n    \"question\": \"Who is the main character in The Valley of Fear?\",\\n    \"answer\": \"Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Who is the main character in The Dynamics of an Asteroid?\",\\n    \"answer\": \"Professor James Moriarty\"\\n  },\\n  {\\n    \"question\": \"What genre does The Dynamics of an Asteroid belong to?\",\\n    \"answer\": \"Fictional book\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:46:41 [engine.py:317] Added request chatcmpl-8bc91f7b57aa4b2d994f41a496413fa4.\n",
            "\u001b[2K\u001b[32m‚†ô\u001b[0m Cleaning content from data/generated/sherlock_6_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:49568 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 16 QA pairs\n",
            "\u001b[2KRetained 7 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 4.2\n",
            "\u001b[2K\u001b[32m‚†∏\u001b[0m Cleaning content from data/generated/sherlock_6_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_6_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:60260 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:60266 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:46:51 [logger.py:43] Received request chatcmpl-87bddf020fe74aa3981a53ef75333f74: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"In what year was Carl Friedrich Gauss\\'s treatise on the dynamics of an asteroid published?\",\\n    \"answer\": \"1809\"\\n  },\\n  {\\n    \"question\": \"Who published a series of books analyzing motions of planets in the Solar System two decades before Arthur Conan Doyle\\'s writing?\",\\n    \"answer\": \"Simon Newcomb\"\\n  },\\n  {\\n    \"question\": \"In what year did Henri Poincar\\\\u00e9 submit his work to the celestial mechanics contest of King Oscar II of Sweden?\",\\n    \"answer\": \"1887\"\\n  },\\n  {\\n    \"question\": \"What was the title of the submission that Henri Poincar\\\\u00e9 investigated in the celestial mechanics contest?\",\\n    \"answer\": \"The three-body problem\"\\n  },\\n  {\\n    \"question\": \"In what year did Srinivasa Ramanujan send letters to several mathematicians at the University of Cambridge?\",\\n    \"answer\": \"1913\"\\n  },\\n  {\\n    \"question\": \"Who was the only mathematician to recognize the merit of Srinivasa Ramanujan\\'s letters?\",\\n    \"answer\": \"G. H. Hardy\"\\n  },\\n  {\\n    \"question\": \"How did G. H. Hardy and J. E. Littlewood respond to Srinivasa Ramanujan\\'s letters?\",\\n    \"answer\": \"They said that many of the letters \\'defeated me completely; I had never seen anything in the least like them before.\\'\"\\n  },\\n  {\\n    \"question\": \"What was referred to as \\'The Final Problem\\' in reference to Sherlock Holmes?\",\\n    \"answer\": \"The last sub-field (and the last problem of the last sub-field) of mathematics\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:46:51 [engine.py:317] Added request chatcmpl-87bddf020fe74aa3981a53ef75333f74.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m‚†∏\u001b[0m Cleaning content from data/generated/sherlock_7_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:60274 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:03 [logger.py:43] Received request chatcmpl-38812f1384504ec8a1b839adc82bbaff: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Who was the subject of a comment by Arthur Eddington in 1919?\",\\n    \"answer\": \"Albert Einstein\"\\n  },\\n  {\\n    \"question\": \"How did Arthur Eddington respond when it was suggested that he was one of only three people in the world who understood Albert Einstein\\'s theory of relativity?\",\\n    \"answer\": \"He quipped that he could not think who the third person was\"\\n  },\\n  {\\n    \"question\": \"Who was the author of the Sherlock Holmes story \\'The Final Problem\\'?\",\\n    \"answer\": \"Arthur Conan Doyle\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:03 [engine.py:317] Added request chatcmpl-38812f1384504ec8a1b839adc82bbaff.\n",
            "\u001b[2K\u001b[32m‚†á\u001b[0m Cleaning content from data/generated/sherlock_7_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:60668 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 11 QA pairs\n",
            "\u001b[2KRetained 11 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 8.5\n",
            "\u001b[2K\u001b[32m‚†ô\u001b[0m Cleaning content from data/generated/sherlock_7_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_7_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55902 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55908 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:08 [logger.py:43] Received request chatcmpl-6be935e142724fbab38c9a84d88eac6a: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Who wrote the short story \\'The Ultimate Crime\\'?\",\\n    \"answer\": \"Isaac Asimov\"\\n  },\\n  {\\n    \"question\": \"What is the title of the short story by Robert Bloch?\",\\n    \"answer\": \"The Dynamics of an Asteroid\"\\n  },\\n  {\\n    \"question\": \"In what year was Henri Poincar\\\\u00e9\\'s work on chaos theory appreciated?\",\\n    \"answer\": \"1890\"\\n  },\\n  {\\n    \"question\": \"Who won the 1887 celestial mechanics contest?\",\\n    \"answer\": \"Henri Poincar\\\\u00e9\"\\n  },\\n  {\\n    \"question\": \"Who wrote the novel \\'Spider-Man: The Revenge of the Sinister Six\\'?\",\\n    \"answer\": \"Adam-Troy Castro\"\\n  },\\n  {\\n    \"question\": \"What is the title of the collection of Sherlock Holmes short stories by William Barton and Michael Capobianco?\",\\n    \"answer\": \"Sherlock Holmes in Orbit\"\\n  },\\n  {\\n    \"question\": \"Who suggested that chaos theory might be associated with asteroid dynamics?\",\\n    \"answer\": \"Alejandro Jenkins\"\\n  },\\n  {\\n    \"question\": \"Who won the 1887 celestial mechanics contest?\",\\n    \"answer\": \"Henri Poincar\\\\u00e9\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:08 [engine.py:317] Added request chatcmpl-6be935e142724fbab38c9a84d88eac6a.\n",
            "\u001b[?25lProcessing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m‚†∏\u001b[0m Cleaning content from data/generated/sherlock_8_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:55922 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:18 [logger.py:43] Received request chatcmpl-19b568c707b44605a7636657619b80d8: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Who wrote the short story \\'The Adventure of the Russian Grave\\'?\",\\n    \"answer\": \"William Barton and Michael Capobianco\"\\n  },\\n  {\\n    \"question\": \"What is the title of the Sherlock Holmes journal where Robert Bloch\\'s short story was published?\",\\n    \"answer\": \"The Baker Street Journal\"\\n  },\\n  {\\n    \"question\": \"What year did Simon P. Norton and Alain Goriely suggest that the book might have been Moriarty\\'s submission?\",\\n    \"answer\": \"Not specified\"\\n  },\\n  {\\n    \"question\": \"Who wrote the short story \\'The Ultimate Crime\\'?\",\\n    \"answer\": \"Isaac Asimov\"\\n  },\\n  {\\n    \"question\": \"What is the title of the Sherlock Holmes through Time and Space collection?\",\\n    \"answer\": \"More Tales of the Black Widowers\"\\n  },\\n  {\\n    \"question\": \"Who wrote the novel \\'The Game is Afoot\\'?\",\\n    \"answer\": \"Not specified\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:18 [engine.py:317] Added request chatcmpl-19b568c707b44605a7636657619b80d8.\n",
            "\u001b[2K\u001b[32m‚†á\u001b[0m Cleaning content from data/generated/sherlock_8_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:38948 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 14 QA pairs\n",
            "\u001b[2KRetained 14 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 7.7\n",
            "\u001b[2K\u001b[32m‚†ô\u001b[0m Cleaning content from data/generated/sherlock_8_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_8_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:38964 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:38980 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:26 [logger.py:43] Received request chatcmpl-d67cdbd02323473cb91a8662c8a54437: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is the topic of the text?\",\\n    \"answer\": \"The Dynamics of an Asteroid\"\\n  },\\n  {\\n    \"question\": \"What type of references are listed in the text?\",\\n    \"answer\": \"List of many references to Dynamics\"\\n  },\\n  {\\n    \"question\": \"Who are some authors mentioned in the text?\",\\n    \"answer\": \"Moriarty, Holmes, and others\"\\n  },\\n  {\\n    \"question\": \"What is the main subject of the text?\",\\n    \"answer\": \"An Asteroid\"\\n  },\\n  {\\n    \"question\": \"What is being referenced by the title?\",\\n    \"answer\": \"The Dynamics\"\\n  },\\n  {\\n    \"question\": \"What type of work is the text referencing?\",\\n    \"answer\": \"Other works\"\\n  },\\n  {\\n    \"question\": \"Is the text a study on Dynamics?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"What is the main subject of the study?\",\\n    \"answer\": \"An Asteroid\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:26 [engine.py:317] Added request chatcmpl-d67cdbd02323473cb91a8662c8a54437.\n",
            "\u001b[?25lProcessing 3 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m‚†∏\u001b[0m Cleaning content from data/generated/sherlock_9_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:38988 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:34 [logger.py:43] Received request chatcmpl-f5f5b59f705a47fe99b3f82cacc175e5: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"Is the text a general information piece?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"Who wrote the text?\",\\n    \"answer\": \"Not specified\"\\n  },\\n  {\\n    \"question\": \"Is the text academic?\",\\n    \"answer\": \"Not specified\"\\n  },\\n  {\\n    \"question\": \"What type of text is this?\",\\n    \"answer\": \"Introduction or List of References\"\\n  },\\n  {\\n    \"question\": \"What is the purpose of the text?\",\\n    \"answer\": \"To list references and provide context\"\\n  },\\n  {\\n    \"question\": \"Is the text a primary source?\",\\n    \"answer\": \"No\"\\n  },\\n  {\\n    \"question\": \"What type of information is provided in the text?\",\\n    \"answer\": \"General information about Dynamics and other works\"\\n  },\\n  {\\n    \"question\": \"Is the text a comprehensive overview?\",\\n    \"answer\": \"Yes\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:34 [engine.py:317] Added request chatcmpl-f5f5b59f705a47fe99b3f82cacc175e5.\n",
            "\u001b[2K\u001b[32m‚†ã\u001b[0m Cleaning content from data/generated/sherlock_9_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:39338 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:42 [logger.py:43] Received request chatcmpl-ce00c9ab264946f4a83e05b3e85b0197: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What type of content is the text about?\",\\n    \"answer\": \"References to Dynamics and other works\"\\n  },\\n  {\\n    \"question\": \"Is the text a study on a specific topic?\",\\n    \"answer\": \"Yes\"\\n  },\\n  {\\n    \"question\": \"What type of information is directly referenced in the text?\",\\n    \"answer\": \"Dynamics\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:42 [engine.py:317] Added request chatcmpl-ce00c9ab264946f4a83e05b3e85b0197.\n",
            "\u001b[2K\u001b[32m‚†è\u001b[0m Cleaning content from data/generated/sherlock_9_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:50328 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 19 QA pairs\n",
            "\u001b[2KRetained 17 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 7.3\n",
            "\u001b[2K\u001b[32m‚†π\u001b[0m Cleaning content from data/generated/sherlock_9_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_9_qa_pairs_cleaned.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:50332 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:50346 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[?25lvLLM STDOUT: INFO 09-25 08:47:46 [logger.py:43] Received request chatcmpl-eadbd92df8fd432ebaa397f069929917: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"How many volumes of the Universal Sherlock Holmes were compiled?\",\\n    \"answer\": \"four\"\\n  },\\n  {\\n    \"question\": \"In what year were the Universal Sherlock Holmes volumes compiled?\",\\n    \"answer\": \"1995\"\\n  },\\n  {\\n    \"question\": \"How many languages have the Sherlock Holmes stories been translated into?\",\\n    \"answer\": \"sixty-three\"\\n  },\\n  {\\n    \"question\": \"What is included in the Universal Sherlock Holmes compilation?\",\\n    \"answer\": \"original writings, translations, writings about Sherlockians, memorials, games, puzzles, phonograph records, audio and video tapes, compact discs, laser discs, and more\"\\n  },\\n  {\\n    \"question\": \"How many items have accumulated worldwide on the two most famous characters in literature?\",\\n    \"answer\": \"a multitude of other items\"\\n  },\\n  {\\n    \"question\": \"What is the name of the compilation of Sherlock Holmes stories?\",\\n    \"answer\": \"Universal Sherlock Holmes\"\\n  },\\n  {\\n    \"question\": \"Who compiled the Universal Sherlock Holmes volumes?\",\\n    \"answer\": \"Ronald B. De Waal\"\\n  },\\n  {\\n    \"question\": \"How many Holmes-related productions and products are listed in the Universal Sherlock Holmes?\",\\n    \"answer\": \"over 25,000\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:46 [engine.py:317] Added request chatcmpl-eadbd92df8fd432ebaa397f069929917.\n",
            "Processing 2 batches of QA pairs...\n",
            "\u001b[2K\u001b[32m‚†º\u001b[0m Cleaning content from data/generated/sherlock_10_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:50360 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 09-25 08:47:54 [logger.py:43] Received request chatcmpl-2f275bdf68794daf9f6580a4455c5e34: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 25 Sep 2025\\n\\nRate each of these question-answer pairs for quality and return exactly this JSON format:\\n\\n[\\n  {\"question\": \"same question text\", \"answer\": \"same answer text\", \"rating\": n}\\n]\\n\\nWhere n is a number from 1-10.\\n\\nDO NOT include any text outside of the JSON array, just return valid JSON:\\n\\n[\\n  {\\n    \"question\": \"What is included in the Universal Sherlock Holmes compilation, besides the original writings?\",\\n    \"answer\": \"translations, writings about Sherlockians, memorials, games, puzzles, phonograph records, audio and video tapes, compact discs, laser discs, ballets, films, musicals, operettas, oratorios, plays, radio and television programs, parodies and pastiches, children\\'s books, cartoons, comics\"\\n  },\\n  {\\n    \"question\": \"What formats of media are included in the Universal Sherlock Holmes compilation?\",\\n    \"answer\": \"phonograph records, audio and video tapes, compact discs, laser discs, ballets, films, musicals, operettas, oratorios, plays, radio and television programs\"\\n  },\\n  {\\n    \"question\": \"What languages are included in the Universal Sherlock Holmes compilation besides English?\",\\n    \"answer\": \"Braille and sixty-three other languages\"\\n  },\\n  {\\n    \"question\": \"What is the purpose of the Universal Sherlock Holmes compilation?\",\\n    \"answer\": \"to compile all adaptations of Sherlock Holmes\"\\n  }\\n]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 09-25 08:47:54 [engine.py:317] Added request chatcmpl-2f275bdf68794daf9f6580a4455c5e34.\n",
            "\u001b[2K\u001b[32m‚†ã\u001b[0m Cleaning content from data/generated/sherlock_10_qa_pairs.json...vLLM STDOUT: INFO:     127.0.0.1:43938 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KRated 5 QA pairs\n",
            "\u001b[2KRetained 5 pairs (threshold: 5.0)\n",
            "\u001b[2KAverage score: 7.6\n",
            "\u001b[2K\u001b[32m‚†∏\u001b[0m Cleaning content from data/generated/sherlock_10_qa_pairs.json...\n",
            "\u001b[1A\u001b[2K\u001b[32m Cleaned content saved to \u001b[0m\u001b[1;32mdata/cleaned/sherlock_10_qa_pairs_cleaned.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "QUALITY_CHECK = True\n",
        "\n",
        "if QUALITY_CHECK:\n",
        "    qa_pairs_filenames = [\n",
        "        f\"data/generated/sherlock_{i}_qa_pairs.json\"\n",
        "        for i in range(len(filenames))\n",
        "    ]\n",
        "    for filename in qa_pairs_filenames:\n",
        "        !synthetic-data-kit \\\n",
        "            -c synthetic_data_kit_config.yaml \\\n",
        "            curate --threshold 5.0 \\\n",
        "            {filename}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell performs the **final step of the data preparation pipeline**: it **formats the synthetic dataset for fine-tuning.**\n",
        "\n",
        "1.  **Gather Files:** First, it creates a list of all the JSON files containing the question-answer pairs that you generated and curated in the previous steps.\n",
        "\n",
        "2.  **Run the `save-as` Command:** It then loops through each of these files and runs the `synthetic-data-kit save-as` command.\n",
        "\n",
        "3.  **Format for Fine-Tuning (`-f ft`):** This is the key part. The `-f ft` flag is a specific instruction that tells the tool to convert the simple QA pairs into the structured format that fine-tuning libraries (like Hugging Face's TRL) expect. This usually means organizing each entry into a conversational format with distinct \"user\" (the question) and \"assistant\" (the answer) roles."
      ],
      "metadata": {
        "id": "Qnfzab-xgLX1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5S8O6Y6l3eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cfb0a03-dddd-444f-8c56-78dd71b7126f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/sherlock_0_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_0_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/sherlock_1_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_1_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/sherlock_2_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_2_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/sherlock_3_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_3_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/sherlock_4_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_4_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/sherlock_5_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_5_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/sherlock_6_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_6_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/sherlock_7_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_7_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/sherlock_8_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_8_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/sherlock_9_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_9_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/sherlock_10_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/sherlock_10_qa_pairs_ft.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "qa_pairs_filenames = [\n",
        "    f\"data/generated/sherlock_{i}_qa_pairs.json\"\n",
        "    for i in range(len(filenames))\n",
        "]\n",
        "for filename in qa_pairs_filenames:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        save-as {filename} -f ft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmFFvFtWmNpc"
      },
      "outputs": [],
      "source": [
        "final_filenames = os.listdir(\"data/final\")\n",
        "\n",
        "conversations = pd.concat(\n",
        "    [pd.read_json(f\"data/final/{name}\") for name in final_filenames]\n",
        ").reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_Zt_cj2ykFi"
      },
      "outputs": [],
      "source": [
        "all_contents = list(\n",
        "    itertools.chain.from_iterable(\n",
        "        [\n",
        "            [message[\"content\"] for message in conversation]\n",
        "            for conversation in conversations[\"messages\"]\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "content_counts = Counter(all_contents)\n",
        "\n",
        "most_common_content = content_counts.most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKRKqvrHykFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "174dad27-45c5-49e4-c125-966a1cb52fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('You are a helpful assistant.', 165), ('Yes', 12), ('Sherlockiana', 6), ('Not specified', 6), ('Arthur Conan Doyle', 4), ('Not specified in the text', 3), ('Professor James Moriarty', 3), ('What is Sherlockiana?', 2), ('1987', 2), ('Charles Spencer', 2), ('Sherlock Holmes', 2), ('Sherlock Holmes pastiches in print and other media such as films', 2), ('memorabilia associated with Sherlock Holmes', 2), ('anything about, inspired by, or tangentially concerning Sherlock Holmes', 2), ('What is a Sherlock Holmes pastiche?', 2), ('not mentioned in the text', 2), (\"Who wrote the short story 'The Ultimate Crime'?\", 2), ('Isaac Asimov', 2), ('The Dynamics of an Asteroid', 2), ('Who won the 1887 celestial mechanics contest?', 2), ('Henri Poincar√©', 2), ('The Baker Street Journal', 2), ('1994', 2), ('2015', 2), ('1914', 2), ('Sherlock Holmes fans', 2), ('What type of publication is The Baker Street Journal?', 2), ('Publication', 2), ('An Asteroid', 2), ('What institution is intrigued by the original location of 221B Baker Street?', 1), ('The United States Smithsonian Museums', 1), ('When did the Sherlock Holmes International Society open the Sherlock Holmes Museum at 221B Baker Street?', 1), ('1990', 1), (\"What was found about the supposed location of Holmes and Watson's flat during the investigation?\", 1), ('It did not exist during the early stories such as A Study in Scarlet', 1), ('What type of culture is represented by statues of Holmes?', 1), ('Sherlockian culture', 1), ('What elements of the world are idolized in Sherlockian culture?', 1), ('Elements of the world', 1), ('Where is the Sherlock Holmes Museum located?', 1), ('221B Baker Street', 1), (\"Is the supposed location of Holmes and Watson's flat mentioned in the early stories?\", 1), (\"What type of stories mention the supposed location of Holmes and Watson's flat?\", 1), ('The early stories such as A Study in Scarlet', 1), ('Is the Sherlock Holmes Museum mentioned as existing at 221B Baker Street?', 1), ('Is the United States Smithsonian Museums mentioned as being from the US?', 1), ('Is the Sherlock Holmes International Society mentioned as being international?', 1), (\"Is the location of Holmes and Watson's flat mentioned as not existing during the early stories?\", 1), ('Is Sherlockian culture mentioned as being idolizing elements of the world?', 1), ('Is the Sherlock Holmes Museum mentioned as being opened by the Sherlock Holmes International Society?', 1)]\n"
          ]
        }
      ],
      "source": [
        "print(most_common_content[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQWaPYMXykFj"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_pandas(conversations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH_7pBeRmltK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6050290-73de-4b7f-a9f9-bf915d0509c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Hugging Face Dataset object:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['messages'],\n",
            "        num_rows: 165\n",
            "    })\n",
            "})\n",
            "\n",
            "Example from the training set:\n",
            "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What institution is intrigued by the original location of 221B Baker Street?', 'role': 'user'}, {'content': 'The United States Smithsonian Museums', 'role': 'assistant'}]}\n"
          ]
        }
      ],
      "source": [
        "final_dataset = DatasetDict({\n",
        "    'train': dataset,\n",
        "})\n",
        "\n",
        "print(\"\\nFinal Hugging Face Dataset object:\")\n",
        "print(final_dataset)\n",
        "\n",
        "# You can inspect an example\n",
        "print(\"\\nExample from the training set:\")\n",
        "print(final_dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHQJu6Rgn-GW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a265c4-5047-4899-8e72-b5de39183162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face login successful using Google Colab secrets!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from huggingface_hub import login\n",
        "    from google.colab import userdata\n",
        "\n",
        "    # Retrieve your Hugging Face token from Colab's secrets manager\n",
        "    # The name 'HF_TOKEN' should match the name you used in the secrets tab\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "    # Check if the token was successfully retrieved\n",
        "    if hf_token:\n",
        "        # Log in to Hugging Face using the retrieved token\n",
        "        # The `add_to_git_credential=True` argument is optional and useful if you plan to push models to the Hub\n",
        "        login(token=hf_token, add_to_git_credential=True)\n",
        "        print(\"Hugging Face login successful using Google Colab secrets!\")\n",
        "    else:\n",
        "        print(\"Error: HF_TOKEN not found in Google Colab secrets or is empty.\")\n",
        "        print(\"Please ensure you have created a secret named 'HF_TOKEN' in the 'Secrets' tab (üîë) on the left sidebar.\")\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxLjmg8vnzrz"
      },
      "outputs": [],
      "source": [
        "if not DEMO:\n",
        "\n",
        "    # Your final_dataset object from the script above is ready\n",
        "    repo_id = \"lmassaron/Sherlock_QA\"\n",
        "    print(f\"\\nUploading dataset to the Hub at {repo_id}...\")\n",
        "\n",
        "    # This command uploads the dataset. It will create the repo if it doesn't exist.\n",
        "    final_dataset.push_to_hub(repo_id)\n",
        "    print(\"Upload complete!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}