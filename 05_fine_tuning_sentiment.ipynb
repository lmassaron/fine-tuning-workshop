{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1192499,
          "sourceType": "datasetVersion",
          "datasetId": 622510
        },
        {
          "sourceId": 282742,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": false,
          "modelInstanceId": 239467,
          "modelId": 222398
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4190.684325,
      "end_time": "2023-12-17T22:21:21.061832",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-12-17T21:11:30.377507",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmassaron/fine-tuning-workshop/blob/main/05_fine_tuning_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune Gemma 3 1B-IT for Sentiment Analysis\n",
        "\n",
        "This tutorial covers the **fine-tuning process** of the recently launched **Gemma 3 1B** model for **sentiment analysis** on financial and economic information. Sentiment analysis in this domain is crucial for businesses for several reasons, including:\n",
        "\n",
        "- **Market Insights**: Gaining valuable insights into market trends, investor confidence, and consumer behavior.  \n",
        "- **Risk Management**: Identifying potential reputational risks.  \n",
        "- **Investment Decisions**: Assessing the sentiment of stakeholders, investors, and the general public to evaluate investment opportunities.  \n",
        "\n",
        "Before diving into the technical aspects of fine-tuning a large language model like **Gemma**, we must first select an appropriate **dataset** to showcase its capabilities."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008332,
          "end_time": "2023-12-17T21:11:33.795658",
          "exception": false,
          "start_time": "2023-12-17T21:11:33.787326",
          "status": "completed"
        },
        "tags": [],
        "id": "up4ljlLH7xtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing the Gemma 3 1B-IT\n",
        "\n",
        "**Gemma 3** is Google's latest addition to its family of lightweight, state-of-the-art open AI models, designed to deliver high performance while being resource-efficient. The **1B Instruct** version of **Gemma 3** is tailored for **instruction-based tasks**, offering developers an accessible and powerful tool for creating intelligent applications.  \n",
        "\n",
        "Announcement: [Gemma 3 Blog Post](https://blog.google/technology/developers/gemma-3/)\n",
        "\n",
        "Gemma 3 features a **transformer architecture** optimized with advanced techniques, enabling sophisticated reasoning and text generation capabilities.\n",
        "\n",
        "Key Features:\n",
        "- **128K-token context window**: Allows processing and understanding of vast amounts of information.  \n",
        "- **Multilingual support**: Over **140 languages**, ideal for global applications.  \n",
        "- **Multimodal capabilities**: Supports text, images, and videos, enabling interactive AI solutions.  \n",
        "- **Edge device optimization**: Efficiently runs on consumer hardware with a single GPU, making it accessible for developers with limited resources.\n",
        "\n",
        "Resources:\n",
        "- [Gemma 3 Model Overview](https://ai.google.dev/gemma/docs/core)  \n",
        "- [Gemma 3 Technical Report](https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf)  \n",
        "- [Gemma 3 Model Card](https://ai.google.dev/gemma/docs/core/model_card_3)"
      ],
      "metadata": {
        "id": "ik8yQLHb7xts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Selection\n",
        "\n",
        "Annotated datasets for finance and economic texts are relatively rare, with many being proprietary. To address this challenge, researchers from the **Aalto University School of Business** introduced the **FinancialPhraseBank Dataset** in 2014, which contains approximately **5,000 sentences**.  \n",
        "\n",
        "This dataset provides **human-annotated benchmarks**, allowing for consistent evaluation of different modeling techniques. The annotations were performed by **16 individuals** with a background in **financial markets**, who categorized the sentences as having a:\n",
        "\n",
        "- **Positive** impact on stock prices  \n",
        "- **Negative** impact on stock prices  \n",
        "- **Neutral** impact on stock prices  \n",
        "\n",
        "The impact was assessed from an **investor's perspective**."
      ],
      "metadata": {
        "id": "qHEdiapg7xts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More on the FinancialPhraseBank Dataset\n",
        "\n",
        "The **FinancialPhraseBank** dataset is a comprehensive collection of **financial news headlines** analyzed from the viewpoint of **retail investors**. It includes two key columns:\n",
        "\n",
        "- **Sentiment**: Classified as **negative**, **neutral**, or **positive**.  \n",
        "- **News Headline**: The actual **financial news snippet**.\n",
        "\n",
        "This dataset has been widely used in research, including the study by **Malo, P.**, **Sinha, A.**, **Korhonen, P.**, **Wallenius, J.**, and **Takala, P.**, titled \"*Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts*\" (published in the **Journal of the Association for Information Science and Technology**, 2014)."
      ],
      "metadata": {
        "id": "Lt9JLOwy7xtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Libraries\n",
        "\n",
        "To implement this tutorial, we need to install several essential libraries:"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.007824,
          "end_time": "2023-12-17T21:11:33.811681",
          "exception": false,
          "start_time": "2023-12-17T21:11:33.803857",
          "status": "completed"
        },
        "tags": [],
        "id": "CdfXow7D7xtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the GPU information\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0aZgzAH777P",
        "outputId": "855d761e-ddf6-43f2-a5cc-65b403e6b96c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep 25 16:55:15 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries for model training and evaluation\n",
        "%%capture\n",
        "!pip install -U transformers trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "papermill": {
          "duration": 25.835818,
          "end_time": "2023-12-17T21:12:12.500193",
          "exception": false,
          "start_time": "2023-12-17T21:11:46.664375",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:01.064015Z",
          "iopub.execute_input": "2025-09-25T12:40:01.064253Z",
          "iopub.status.idle": "2025-09-25T12:40:27.983904Z",
          "shell.execute_reply.started": "2025-09-25T12:40:01.064233Z",
          "shell.execute_reply": "2025-09-25T12:40:27.982545Z"
        },
        "id": "t2Vm8Bcj7xtt"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of Key Libraries  \n",
        "\n",
        "- **`transformers`**: Provides a framework to handle **pre-trained NLP models** for tasks like **text classification** and **question answering**.  \n",
        "\n",
        "- **`accelerate`**: A distributed training library by Hugging Face designed for **parallelizing training** across multiple **GPUs or CPUs**.  \n",
        "\n",
        "- **`peft`**: A library for **parameter-efficient fine-tuning (PEFT)** of pre-trained language models, including support for **LoRA (Low-Rank Adaptation)**.  \n",
        "\n",
        "- **`trl`**: A Hugging Face library for training **transformer models** with **supervised fine-tuning** or **reinforcement learning techniques**.  \n",
        "\n",
        "-   **`bitsandbytes`**: A lightweight library that provides custom CUDA functions for **8-bit and 4-bit quantization** of PyTorch models. This significantly reduces the memory footprint of large language models, making it possible to run them on less powerful hardware. `bitsandbytes` offers features like 8-bit optimizers and 4-bit quantization with QLoRA, which enables training large models with reduced memory requirements without compromising performance."
      ],
      "metadata": {
        "id": "y5T6GgjH7xtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Environment Variables\n",
        "\n",
        "The following code sets environment variables to configure the GPU usage and suppress unnecessary warnings:"
      ],
      "metadata": {
        "id": "eVMcLBbj7xtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Use the first GPU\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Disable tokenization parallelism"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.015679,
          "end_time": "2023-12-17T21:12:12.539544",
          "exception": false,
          "start_time": "2023-12-17T21:12:12.523865",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:27.984978Z",
          "iopub.execute_input": "2025-09-25T12:40:27.985289Z",
          "iopub.status.idle": "2025-09-25T12:40:27.990116Z",
          "shell.execute_reply.started": "2025-09-25T12:40:27.985260Z",
          "shell.execute_reply": "2025-09-25T12:40:27.989125Z"
        },
        "id": "y6Pvcl_I7xtu"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suppressing Warnings\n",
        "\n",
        "During training, several warnings may appear that do not impact the fine-tuning process but can be distracting. To suppress them:"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.007538,
          "end_time": "2023-12-17T21:12:12.555014",
          "exception": false,
          "start_time": "2023-12-17T21:12:12.547476",
          "status": "completed"
        },
        "tags": [],
        "id": "OkJegMXl7xtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.01423,
          "end_time": "2023-12-17T21:12:12.576944",
          "exception": false,
          "start_time": "2023-12-17T21:12:12.562714",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:27.992421Z",
          "iopub.execute_input": "2025-09-25T12:40:27.992610Z",
          "iopub.status.idle": "2025-09-25T12:40:28.006884Z",
          "shell.execute_reply.started": "2025-09-25T12:40:27.992593Z",
          "shell.execute_reply": "2025-09-25T12:40:28.006038Z"
        },
        "id": "gDoJklpy7xtu"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "Global variable for demonstrations"
      ],
      "metadata": {
        "id": "bs8_k4wAcCGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEMO = False"
      ],
      "metadata": {
        "id": "mDN1Vn_7FYis"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Libraries\n",
        "\n",
        "The following Python libraries are required for running the fine-tuning process:"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.007919,
          "end_time": "2023-12-17T21:12:12.592609",
          "exception": false,
          "start_time": "2023-12-17T21:12:12.58469",
          "status": "completed"
        },
        "tags": [],
        "id": "vEdSSvWf7xtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import transformers\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          TrainingArguments,\n",
        "                          pipeline,\n",
        "                          logging)\n",
        "\n",
        "from transformers.models.gemma3 import Gemma3ForCausalLM\n",
        "\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftConfig, PeftModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                             classification_report,\n",
        "                             confusion_matrix)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "papermill": {
          "duration": 19.450408,
          "end_time": "2023-12-17T21:12:32.05101",
          "exception": false,
          "start_time": "2023-12-17T21:12:12.600602",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:28.008455Z",
          "iopub.execute_input": "2025-09-25T12:40:28.008752Z",
          "iopub.status.idle": "2025-09-25T12:40:52.531199Z",
          "shell.execute_reply.started": "2025-09-25T12:40:28.008725Z",
          "shell.execute_reply": "2025-09-25T12:40:52.530146Z"
        },
        "id": "nY_9TIpE7xtu"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the installed version of the transformers library:"
      ],
      "metadata": {
        "id": "EBb6ZQsm7xtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"transformers=={transformers.__version__}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.532161Z",
          "iopub.execute_input": "2025-09-25T12:40:52.532493Z",
          "iopub.status.idle": "2025-09-25T12:40:52.536763Z",
          "shell.execute_reply.started": "2025-09-25T12:40:52.532461Z",
          "shell.execute_reply": "2025-09-25T12:40:52.535849Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arQ8C6-o7xtv",
        "outputId": "2799af02-191e-4499-b226-7adb78e05331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers==4.56.2\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function determines the best computing device for running the tutorial:"
      ],
      "metadata": {
        "id": "heh5X6p-7xtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_device():\n",
        "    \"\"\"Determines and returns the optimal PyTorch device based on availability.\"\"\"\n",
        "\n",
        "    print(f\"PyTorch version: {torch.__version__}\", end=\" -- \")\n",
        "\n",
        "    # Check if MPS (Metal Performance Shaders) is available for macOS\n",
        "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        print(\"using MPS device on macOS\")\n",
        "        return torch.device(\"mps\")\n",
        "\n",
        "    # Check for CUDA availability\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"using {device}\")\n",
        "    return device"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.538110Z",
          "iopub.execute_input": "2025-09-25T12:40:52.538445Z",
          "iopub.status.idle": "2025-09-25T12:40:52.793893Z",
          "shell.execute_reply.started": "2025-09-25T12:40:52.538408Z",
          "shell.execute_reply": "2025-09-25T12:40:52.792901Z"
        },
        "id": "QE_tMGrF7xtv"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code initializes the Gemma 3 1B model for causal language modeling, ensuring optimal settings based on the available hardware.  \n",
        "\n",
        "* If the GPU supports **bfloat16** (available on GPUs with Compute Capability **8.0+**), it is used for computations.  \n",
        "  Otherwise, **float16** is used as the default.  \n",
        "\n",
        "* **Device Selection:**  \n",
        "  * The function `define_device()` selects the best available device (**CPU, CUDA, or MPS**).  \n",
        "\n",
        "* **Model Initialization:**  \n",
        "  * The model is loaded with memory-efficient configurations, including `low_cpu_mem_usage=True`, and assigned to the selected device.  \n",
        "\n",
        "* **Tokenizer Setup:**  \n",
        "  * A **tokenizer** is initialized with a **maximum sequence length of 1024**.  \n",
        "  * The **end-of-sequence (EOS) token** is stored for later use.  "
      ],
      "metadata": {
        "id": "vUGE-F5b7xtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine optimal computation dtype based on GPU capability\n",
        "compute_dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
        "print(f\"Using compute dtype {compute_dtype}\")\n",
        "\n",
        "# Select the best available device (CPU, CUDA, or MPS)\n",
        "device = define_device()\n",
        "print(f\"Operating on {device}\")\n",
        "\n",
        "# Path to the pre-trained model\n",
        "GEMMA_PATH = \"google/gemma-3-1b-it\"\n",
        "\n",
        "# Load the model with optimized settings\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = Gemma3ForCausalLM.from_pretrained(\n",
        "    GEMMA_PATH,\n",
        "    torch_dtype=compute_dtype,\n",
        "    attn_implementation=\"eager\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Define maximum sequence length for the tokenizer\n",
        "max_seq_length = 1024\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    GEMMA_PATH,\n",
        "    max_seq_length=max_seq_length,\n",
        "    device_map=device\n",
        ")\n",
        "\n",
        "# Store the EOS token for later use\n",
        "EOS_TOKEN = tokenizer.eos_token"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.794810Z",
          "iopub.execute_input": "2025-09-25T12:40:52.795155Z",
          "iopub.status.idle": "2025-09-25T12:40:52.840141Z",
          "shell.execute_reply.started": "2025-09-25T12:40:52.795121Z",
          "shell.execute_reply": "2025-09-25T12:40:52.838105Z"
        },
        "id": "8EJCX5Ne7xtv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model: {model.name_or_path}\")\n",
        "print(f\"Device: {model.device}\")\n",
        "print(f\"DType: {model.dtype}\")\n",
        "\n",
        "if hasattr(model, \"is_quantized\") and model.is_quantized:\n",
        "    # The quantization_config attribute holds the BitsAndBytesConfig object\n",
        "    print(\"Quantization: Enabled\")\n",
        "    print(f\"  - 4bit: {model.is_loaded_in_4bit}\")\n",
        "    print(f\"  - Quant Type: {model.hf_quantizer.quantization_config.bnb_4bit_quant_type}\")\n",
        "    print(f\"  - Compute DType: {model.hf_quantizer.quantization_config.bnb_4bit_compute_dtype}\")\n",
        "else:\n",
        "    print(\"Quantization: Disabled\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTEsVs0-G7vF",
        "outputId": "3b85128f-0f5e-4ceb-e116-33c882cd5585"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: google/gemma-3-1b-it\n",
            "Device: cuda:0\n",
            "DType: torch.float16\n",
            "Quantization: Enabled\n",
            "  - 4bit: True\n",
            "  - Quant Type: nf4\n",
            "  - Compute DType: torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before proceeding, let's ensures that the entire model is correctly moved to the GPU.\n"
      ],
      "metadata": {
        "id": "OA3sZ8GJ7xtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "is_on_gpu = all(param.device.type == 'cuda' for param in model.parameters())\n",
        "print(\"Model is on GPU:\", is_on_gpu)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.840793Z",
          "iopub.status.idle": "2025-09-25T12:40:52.841225Z",
          "shell.execute_reply": "2025-09-25T12:40:52.841056Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqY8uaD57xtv",
        "outputId": "a9f48aa6-cf39-4c64-eafe-c69a127173e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is on GPU: True\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code prepares the dataset for fine-tuning a sentiment analysis model using Gemma. It follows these steps:  \n",
        "\n",
        "1. **Load Dataset**  \n",
        "   * Reads the dataset from `all-data.csv`, which contains two columns:  \n",
        "     - **sentiment**: The sentiment label (positive, neutral, negative).  \n",
        "     - **text**: The financial news headlines.  \n",
        "\n",
        "2. **Stratified Train-Test Split**  \n",
        "   * The dataset is split into **training** and **test** sets, each containing **300 samples per sentiment class**.  \n",
        "   * **Stratification** ensures that each set has an equal distribution of positive, neutral, and negative examples.  \n",
        "\n",
        "3. **Shuffle Training Data**  \n",
        "   * The training data is shuffled using `random_state=10` to ensure **replicability**.  \n",
        "\n",
        "4. **Prepare Evaluation Data**  \n",
        "   * The remaining (unselected) data is assigned to an **evaluation set (`X_eval`)**.  \n",
        "   * To ensure **balanced evaluation**, each sentiment class is resampled to have **50 instances** (negative samples are duplicated if needed).  \n",
        "\n",
        "5. **Convert Text into Prompts**  \n",
        "   * The **training** and **evaluation** data are transformed into **prompts** that instruct the model to classify sentiment.  \n",
        "   * **Training prompts** include sentiment labels (used for fine-tuning).  \n",
        "   * **Test prompts** omit sentiment labels (used for inference).  \n",
        "\n",
        "6. **Wrap Data Using Hugging Face's Dataset Class**  \n",
        "   * Converts `train_data`, `eval_data`, and `test_data` into **Hugging Face Dataset objects** for compatibility with the training pipeline."
      ],
      "metadata": {
        "id": "B71dgb3V7xtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "filename = \"https://github.com/lmassaron/Gemma-3-1B-financial-sentiment-analysis/raw/refs/heads/main/all-data.csv\"\n",
        "df = pd.read_csv(filename,\n",
        "                 names=[\"sentiment\", \"text\"],\n",
        "                 encoding=\"utf-8\", encoding_errors=\"replace\")\n",
        "\n",
        "# Initialize lists for train and test sets\n",
        "X_train, X_test = [], []\n",
        "\n",
        "# Stratified train-test split (300 per sentiment)\n",
        "for sentiment in [\"positive\", \"neutral\", \"negative\"]:\n",
        "    train, test = train_test_split(df[df.sentiment == sentiment],\n",
        "                                   train_size=300,\n",
        "                                   test_size=300,\n",
        "                                   random_state=42,\n",
        "                                   stratify=df[df.sentiment == sentiment][\"sentiment\"])\n",
        "    X_train.append(train)\n",
        "    X_test.append(test)\n",
        "\n",
        "# Combine and shuffle training data\n",
        "X_train = pd.concat(X_train).sample(frac=1, random_state=10).reset_index(drop=True)\n",
        "X_test = pd.concat(X_test).reset_index(drop=True)\n",
        "\n",
        "# Identify indices not included in train or test sets\n",
        "selected_indices = set(X_train.index) | set(X_test.index)\n",
        "X_eval = df.loc[~df.index.isin(selected_indices)].copy()\n",
        "\n",
        "# Resample evaluation data to maintain class balance (50 per class, with replacement for minority class)\n",
        "X_eval = X_eval.groupby('sentiment', group_keys=False).apply(\n",
        "    lambda x: x.sample(n=50, random_state=10, replace=True)\n",
        ").reset_index(drop=True)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.526476,
          "end_time": "2023-12-17T21:12:32.601566",
          "exception": false,
          "start_time": "2023-12-17T21:12:32.07509",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.842199Z",
          "iopub.status.idle": "2025-09-25T12:40:52.842571Z",
          "shell.execute_reply": "2025-09-25T12:40:52.842403Z"
        },
        "id": "D4PiM5qy7xtw"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate training and evaluation prompts\n",
        "def generate_train_prompt(data_point):\n",
        "    \"\"\"Generates a training prompt for sentiment analysis.\"\"\"\n",
        "    return f\"\"\"\n",
        "    Analyze the sentiment of the news headline enclosed in square brackets.\n",
        "    Determine if it is positive, neutral, or negative, and return the corresponding sentiment label:\n",
        "    \"positive\", \"neutral\", or \"negative\".\n",
        "\n",
        "    [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n",
        "    \"\"\".strip() + EOS_TOKEN\n",
        "\n",
        "# Function to generate test prompts (without expected answer)\n",
        "def generate_test_prompt(data_point):\n",
        "    \"\"\"Generates a test prompt for sentiment analysis.\"\"\"\n",
        "    return f\"\"\"\n",
        "    Analyze the sentiment of the news headline enclosed in square brackets.\n",
        "    Determine if it is positive, neutral, or negative, and return the corresponding sentiment label:\n",
        "    \"positive\", \"neutral\", or \"negative\".\n",
        "\n",
        "    [{data_point[\"text\"]}] =\n",
        "    \"\"\".strip()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.843134Z",
          "iopub.status.idle": "2025-09-25T12:40:52.843388Z",
          "shell.execute_reply": "2025-09-25T12:40:52.843287Z"
        },
        "id": "jsiQkwNI7xtw"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply prompt generation to datasets\n",
        "X_train = pd.DataFrame(X_train.apply(generate_train_prompt, axis=1), columns=[\"text\"])\n",
        "X_eval = pd.DataFrame(X_eval.apply(generate_train_prompt, axis=1), columns=[\"text\"])\n",
        "\n",
        "# Store ground truth labels for test data\n",
        "y_true = X_test[\"sentiment\"]\n",
        "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_data = Dataset.from_pandas(X_train)\n",
        "eval_data = Dataset.from_pandas(X_eval)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.844448Z",
          "iopub.status.idle": "2025-09-25T12:40:52.844843Z",
          "shell.execute_reply": "2025-09-25T12:40:52.844651Z"
        },
        "id": "U_2CO2l57xtw"
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function evaluates the performance of our **fine-tuned sentiment analysis model** by performing the following tasks:\n",
        "\n",
        "**1. Map Sentiment Labels to Numeric Values**\n",
        "- **Positive** → `2`\n",
        "- **Neutral** → `1`\n",
        "- **Negative** → `0`\n",
        "- Additionally, handles cases where the label is `'none'` by mapping it to **`1 (neutral)`**.\n",
        "\n",
        "**2. Calculate Overall Accuracy**\n",
        "- Computes the accuracy of the model predictions (`y_pred`) compared to the actual sentiment labels (`y_true`).\n",
        "\n",
        "**3. Compute Accuracy for Each Sentiment Label**\n",
        "- Extracts **accuracy scores** separately for:\n",
        "  - **Positive**\n",
        "  - **Neutral**\n",
        "  - **Negative**\n",
        "\n",
        "**4. Generate a Classification Report**\n",
        "- Prints **precision, recall, and F1-score** for each sentiment category.\n",
        "\n",
        "**5. Compute and Display the Confusion Matrix**\n",
        "- Displays a **confusion matrix** to show how often the model misclassifies sentiments (e.g., predicting **neutral** instead of **positive**).\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.010178,
          "end_time": "2023-12-17T21:12:32.639198",
          "exception": false,
          "start_time": "2023-12-17T21:12:32.62902",
          "status": "completed"
        },
        "tags": [],
        "id": "_3ROzT_c7xtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_true, y_pred):\n",
        "    \"\"\"Evaluates the fine-tuned sentiment model's performance.\"\"\"\n",
        "\n",
        "    # Define sentiment label mapping\n",
        "    label_mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
        "\n",
        "    # Convert labels to numeric values\n",
        "    y_true = np.array([label_mapping.get(label, 1) for label in y_true])\n",
        "    y_pred = np.array([label_mapping.get(label, 1) for label in y_pred])\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Overall Accuracy: {accuracy:.3f}')\n",
        "\n",
        "    # Compute accuracy for each sentiment label\n",
        "    unique_labels = np.unique(y_true)  # Get unique labels in y_true\n",
        "\n",
        "    for label in unique_labels:\n",
        "        label_mask = y_true == label  # Mask to filter specific class\n",
        "        label_accuracy = accuracy_score(y_true[label_mask], y_pred[label_mask])\n",
        "        print(f'Accuracy for label {label} ({list(label_mapping.keys())[list(label_mapping.values()).index(label)]}): {label_accuracy:.3f}')\n",
        "\n",
        "    # Generate classification report\n",
        "    class_report = classification_report(y_true, y_pred, target_names=label_mapping.keys())\n",
        "    print('\\nClassification Report:\\n', class_report)\n",
        "\n",
        "    # Compute and display confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])\n",
        "    print('\\nConfusion Matrix:\\n', conf_matrix)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.021281,
          "end_time": "2023-12-17T21:12:32.668587",
          "exception": false,
          "start_time": "2023-12-17T21:12:32.647306",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.846245Z",
          "iopub.status.idle": "2025-09-25T12:40:52.846619Z",
          "shell.execute_reply": "2025-09-25T12:40:52.846460Z"
        },
        "id": "8kZchGe87xtw"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function predicts the sentiment of news headlines. It takes three arguments:\n",
        "\n",
        "- **X_test**: A Pandas DataFrame containing the news headlines to be analyzed.\n",
        "- **model**: The pre-trained **Gemma-3 4B** language model.\n",
        "- **tokenizer**: The corresponding tokenizer for the **Gemma-3 4B** model.\n",
        "\n",
        "### **Function Workflow:**\n",
        "1. **Iterate through each news headline** in `X_test`:\n",
        "   - Construct a prompt asking the model to analyze the sentiment.\n",
        "   - Tokenize the input and move it to the appropriate device (GPU/CPU).\n",
        "   - Generate text using the model and extract the predicted sentiment label.\n",
        "   - Append the sentiment label to `y_pred`.\n",
        "\n",
        "2. **Use the `generate()` function** from the Hugging Face Transformers library:\n",
        "   - `max_new_tokens=5`: Limits the number of generated tokens.\n",
        "   - `temperature=0.0`: Ensures deterministic output.\n",
        "\n",
        "3. **Extract the sentiment label** from the generated text:\n",
        "   - If the text contains \"positive\", assign the label **positive**.\n",
        "   - If the text contains \"negative\", assign the label **negative**.\n",
        "   - If the text contains \"neutral\", assign the label **neutral**.\n",
        "   - If none of these are found, assign **none** as a fallback."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.009804,
          "end_time": "2023-12-17T21:14:01.291046",
          "exception": false,
          "start_time": "2023-12-17T21:14:01.281242",
          "status": "completed"
        },
        "tags": [],
        "id": "gptV2Aap7xtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X_test, model, tokenizer, device=device, max_new_tokens=5, temperature=0.0):\n",
        "    \"\"\"Predicts the sentiment of news headlines.\"\"\"\n",
        "\n",
        "    y_pred = []  # List to store predicted sentiment labels\n",
        "\n",
        "    # Iterate through each headline in X_test\n",
        "    for i in tqdm(range(len(X_test)), desc=\"Predicting Sentiments\"):\n",
        "        prompt = X_test.iloc[i][\"text\"]  # Extract headline text\n",
        "\n",
        "        # Tokenize and move input to the appropriate device\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate output from the model\n",
        "        if temperature > 0:\n",
        "          params = {\"do_sample\": True, \"temperature\": temperature}\n",
        "        else:\n",
        "          params = {\"do_sample\": False}\n",
        "\n",
        "        outputs = model.generate(**input_ids,\n",
        "                                 max_new_tokens=max_new_tokens,\n",
        "                                 **params)\n",
        "\n",
        "        # Decode the generated output into text\n",
        "        result = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
        "        result = result.split(\"=\")[-1]\n",
        "\n",
        "        # Extract sentiment from the generated text\n",
        "        if \"positive\" in result:\n",
        "            y_pred.append(\"positive\")\n",
        "        elif \"negative\" in result:\n",
        "            y_pred.append(\"negative\")\n",
        "        elif \"neutral\" in result:\n",
        "            y_pred.append(\"neutral\")\n",
        "        else:\n",
        "            y_pred.append(\"none\")  # Fallback to error if no clear sentiment is detected\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.020352,
          "end_time": "2023-12-17T21:14:01.321014",
          "exception": false,
          "start_time": "2023-12-17T21:14:01.300662",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.847727Z",
          "iopub.status.idle": "2025-09-25T12:40:52.848105Z",
          "shell.execute_reply": "2025-09-25T12:40:52.847936Z"
        },
        "id": "NFsayg5k7xtx"
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, we are ready to test the **Gemma-3 1B** model on our dataset **without any fine-tuning**. This initial evaluation provides insights into the model's **inherent performance** and helps establish a **baseline** for comparison with future fine-tuned models.\n",
        "\n",
        "We use the `predict` function to generate sentiment predictions for the test set:"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.009582,
          "end_time": "2023-12-17T21:14:01.340377",
          "exception": false,
          "start_time": "2023-12-17T21:14:01.330795",
          "status": "completed"
        },
        "tags": [],
        "id": "6aaKgSPc7xtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = predict(X_test, model, tokenizer)"
      ],
      "metadata": {
        "papermill": {
          "duration": 366.910332,
          "end_time": "2023-12-17T21:20:08.260576",
          "exception": false,
          "start_time": "2023-12-17T21:14:01.350244",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.849138Z",
          "iopub.status.idle": "2025-09-25T12:40:52.849524Z",
          "shell.execute_reply": "2025-09-25T12:40:52.849351Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ieiCIh7xtx",
        "outputId": "ae20e600-0176-43c7-a02c-56c2a6db38c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting Sentiments:   0%|          | 0/900 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Predicting Sentiments: 100%|██████████| 900/900 [08:39<00:00,  1.73it/s]\n"
          ]
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, we evaluate the model's predictions against the true sentiment labels:"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.078787,
          "end_time": "2023-12-17T21:20:08.418105",
          "exception": false,
          "start_time": "2023-12-17T21:20:08.339318",
          "status": "completed"
        },
        "tags": [],
        "id": "chlBGht_7xtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(y_true, y_pred)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.115727,
          "end_time": "2023-12-17T21:20:08.612213",
          "exception": false,
          "start_time": "2023-12-17T21:20:08.496486",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.850084Z",
          "iopub.status.idle": "2025-09-25T12:40:52.850447Z",
          "shell.execute_reply": "2025-09-25T12:40:52.850287Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dma3mFec7xtx",
        "outputId": "39609fb9-f816-4b54-bf4c-d1a735d04261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 0.444\n",
            "Accuracy for label 0 (negative): 0.223\n",
            "Accuracy for label 1 (neutral): 0.200\n",
            "Accuracy for label 2 (positive): 0.910\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.85      0.22      0.35       300\n",
            "     neutral       0.45      0.20      0.28       300\n",
            "    negative       0.40      0.91      0.55       300\n",
            "\n",
            "    accuracy                           0.44       900\n",
            "   macro avg       0.56      0.44      0.39       900\n",
            "weighted avg       0.56      0.44      0.39       900\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 67  50 183]\n",
            " [  9  60 231]\n",
            " [  3  24 273]]\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "source": [
        "We employ the `SFTTrainer`, a specialized trainer from the TRL library, to conduct supervised fine-tuning. This trainer is optimized for fine-tuning pre-trained models, especially when working with smaller datasets, and it simplifies the overall workflow.\n",
        "\n",
        "The core of our efficiency comes from the **Parameter-Efficient Fine-Tuning (PEFT)** method. Instead of retraining all the parameters of the LLM, PEFT techniques like LoRA focus on a small subset of parameters. This approach significantly reduces computational and storage costs while mitigating the risk of \"catastrophic forgetting,\" where a model loses its original capabilities after fine-tuning.\n",
        "\n",
        "### `LoraConfig`\n",
        "\n",
        "The `peft_config` object, an instance of `LoraConfig`, specifies the parameters for the LoRA fine-tuning process.\n",
        "\n",
        "-   **`lora_alpha`**: This acts as a scaling factor for the LoRA updates. It's a hyperparameter that can be tuned, and a common practice is to set it to twice the value of `r`. In this configuration, it is set to `32`.\n",
        "-   **`lora_dropout`**: This specifies the dropout probability for the LoRA layers, which helps in preventing overfitting. Here, it is set to `0.05`.\n",
        "-   **`r`**: This parameter, also known as the rank, determines the dimension of the low-rank matrices used in LoRA. A higher rank results in more trainable parameters. In this case, the rank is `64`.\n",
        "-   **`bias`**: This defines which biases to train. Setting it to \"`none`\" means no biases will be trained.\n",
        "-   **`task_type`**: This specifies the type of task the model is being fine-tuned for. For language generation models, this is set to \"`CAUSAL_LM`\" (Causal Language Modeling).\n",
        "-   **`target_modules`**: This indicates which modules within the model's architecture to apply LoRA to. Here, \"`all-linear`\" applies LoRA to all linear layers.\n",
        "\n",
        "### `SFTConfig`\n",
        "\n",
        "The `training_arguments` object, an instance of `SFTConfig`, holds all the arguments for the training process itself. `SFTConfig` is a subclass of the `TrainingArguments` class from the Transformers library, with additional parameters specific to supervised fine-tuning.\n",
        "\n",
        "-   **`output_dir`**: The directory where training logs and checkpoints are saved.\n",
        "-   **`num_train_epochs`**: The total number of training epochs to perform.\n",
        "-   **`gradient_checkpointing`**: A technique to reduce GPU memory usage at the cost of a slight slowdown in training.\n",
        "-   **`per_device_train_batch_size`**: The batch size for training on each GPU.\n",
        "-   **`gradient_accumulation_steps`**: The number of batches to process before performing a backward/update pass. This effectively increases the batch size.\n",
        "-   **`optim`**: The optimizer to use for training. \"`adamw_torch_fused`\" is a memory-efficient version of the AdamW optimizer.\n",
        "-   **`save_steps`**: A checkpoint is saved every `50` steps.\n",
        "-   **`load_best_model_at_end`**: If set to `True`, the best model checkpoint will be loaded at the end of training.\n",
        "-   **`logging_steps`**: Training metrics are logged every `25` steps.\n",
        "-   **`learning_rate`**: The initial learning rate for the optimizer.\n",
        "-   **`weight_decay`**: The amount of weight decay to apply for regularization.\n",
        "-   **`fp16` / `bf16`**: These booleans enable 16-bit precision training (either float16 or bfloat16) to speed up training and reduce memory usage, depending on the available hardware.\n",
        "-   **`max_grad_norm`**: The maximum value for gradient clipping, used to prevent exploding gradients.\n",
        "-   **`max_steps`**: The total number of training steps. If set to `-1`, it is determined by the number of epochs.\n",
        "-   **`warmup_ratio`**: The proportion of training steps used for a learning rate warm-up.\n",
        "-   **`group_by_length`**: If true, sequences of similar lengths are grouped together in batches to minimize padding and increase training efficiency.\n",
        "-   **`eval_strategy`**: Specifies the evaluation strategy. \"`steps`\" means evaluation is performed at regular step intervals.\n",
        "-   **`eval_steps`**: Evaluation is performed every `50` training steps.\n",
        "-   **`eval_accumulation_steps`**: The number of prediction steps to accumulate before moving the results to the CPU.\n",
        "-   **`lr_scheduler_type`**: The type of learning rate scheduler.\n",
        "-   **`report_to`**: The integrations to report results to, in this case, \"`tensorboard`\".\n",
        "-   **`max_length`**: The maximum sequence length for the model.\n",
        "-   **`packing`**: If `True`, multiple short examples are packed into a single input sequence to improve training efficiency.\n",
        "-   **`dataset_kwargs`**: A dictionary of keyword arguments to pass to the dataset processing function.\n",
        "\n",
        "### `SFTTrainer`\n",
        "\n",
        "The `SFTTrainer` is initialized with the model, datasets, configurations, and tokenizer.\n",
        "\n",
        "-   **`model`**: The instantiated model that will be trained.\n",
        "-   **`train_dataset`**: The preprocessed dataset for training.\n",
        "-   **`eval_dataset`**: The preprocessed dataset for evaluation.\n",
        "-   **`peft_config`**: The `LoraConfig` object for PEFT.\n",
        "-   **`processing_class`**: This should likely be `tokenizer` instead of `processing_class`. The `tokenizer` argument is used to tokenize the dataset.\n",
        "-   **`args`**: The `SFTConfig` object containing the training arguments.\n",
        "\n",
        "Once the `SFTTrainer` is initialized, the training process is started by calling the `trainer.train()` method."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.078192,
          "end_time": "2023-12-17T21:20:08.781177",
          "exception": false,
          "start_time": "2023-12-17T21:20:08.702985",
          "status": "completed"
        },
        "tags": [],
        "id": "H_mIbU9Z7xtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=\"all-linear\",\n",
        ")\n",
        "\n",
        "training_arguments = SFTConfig(\n",
        "    output_dir=\"logs\",\n",
        "    num_train_epochs=2,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Use reentrant checkpointing\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    optim=\"adamw_torch_fused\",  # Use fused AdamW optimizer\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=True if compute_dtype == torch.float16 else False,  # Use float16 precision\n",
        "    bf16=True if compute_dtype == torch.bfloat16 else False,  # Use bfloat16 precision\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=False,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    eval_accumulation_steps=1,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"tensorboard\",\n",
        "    max_length=max_seq_length,\n",
        "    packing=False,\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,  # Template with special tokens\n",
        "        \"append_concat_token\": True,  # Add EOS token as separator token\n",
        "    }\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=tokenizer,\n",
        "    args=training_arguments,\n",
        ")"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.907911,
          "end_time": "2023-12-17T21:20:09.767766",
          "exception": false,
          "start_time": "2023-12-17T21:20:08.859855",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.851270Z",
          "iopub.status.idle": "2025-09-25T12:40:52.851632Z",
          "shell.execute_reply": "2025-09-25T12:40:52.851474Z"
        },
        "id": "Uty1QsPg7xtx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `trainer.train()` function is called to start the training process. This triggers the fine-tuning of the model based on the specified training arguments, datasets, and PEFT configuration. During this process, the model will iteratively adjust its parameters, leveraging the training data to improve performance on the sentiment analysis task. The training will proceed according to the parameters set in the `training_arguments`, such as the number of epochs, batch size, and evaluation steps.\n",
        "\n",
        "This method will also handle the evaluation of the model at specified intervals, providing insights into the model's performance as it trains."
      ],
      "metadata": {
        "id": "XDooI-917xty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "papermill": {
          "duration": 3279.391068,
          "end_time": "2023-12-17T22:14:49.397236",
          "exception": false,
          "start_time": "2023-12-17T21:20:10.006168",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.852404Z",
          "iopub.status.idle": "2025-09-25T12:40:52.852794Z",
          "shell.execute_reply": "2025-09-25T12:40:52.852619Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "fi2ZnSYP7xty",
        "outputId": "67896a8c-1b25-49e8-f2c3-d78e44eb708c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='226' max='226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [226/226 22:14, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Entropy</th>\n",
              "      <th>Num Tokens</th>\n",
              "      <th>Mean Token Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.188600</td>\n",
              "      <td>1.215040</td>\n",
              "      <td>1.225811</td>\n",
              "      <td>33546.000000</td>\n",
              "      <td>0.771244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.169300</td>\n",
              "      <td>1.130326</td>\n",
              "      <td>1.195149</td>\n",
              "      <td>67868.000000</td>\n",
              "      <td>0.780688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.889600</td>\n",
              "      <td>1.116214</td>\n",
              "      <td>1.053856</td>\n",
              "      <td>101319.000000</td>\n",
              "      <td>0.783574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.905400</td>\n",
              "      <td>1.064929</td>\n",
              "      <td>1.048344</td>\n",
              "      <td>135328.000000</td>\n",
              "      <td>0.792854</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=226, training_loss=1.1058962160507135, metrics={'train_runtime': 1340.0347, 'train_samples_per_second': 1.343, 'train_steps_per_second': 0.169, 'total_flos': 687060457998336.0, 'train_loss': 1.1058962160507135, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "source": [
        "the code in the next cell provides a visual representation of how the model's performance on both the training and validation datasets has evolved over the course of the training epochs. This is crucial for diagnosing issues like overfitting (where training loss decreases but validation loss increases) or underfitting (where both losses remain high)."
      ],
      "metadata": {
        "id": "caLoGbyReRdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the log history\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Extract training / validation loss\n",
        "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
        "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
        "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
        "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
        "\n",
        "# Plot the training loss\n",
        "plt.plot(epoch_train, train_losses, label=\"Training Loss\")\n",
        "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss per Epoch\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "uX1eKWr1DrC5",
        "outputId": "320fd4aa-8f75-4466-e6f3-2188f6c0bada"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, here is an updated version of the markdown that is more streamlined and directly incorporates the code.\n",
        "\n",
        "### Saving the Fine-Tuned Model and Tokenizer\n",
        "\n",
        "The following code saves the fine-tuned LoRA adapter and the tokenizer to a directory. This is a crucial step to ensure that the trained components can be easily reloaded for inference or further training later.\n",
        "\n",
        "1.  **Saving the LoRA Adapter**:\n",
        "    The command `trainer.model.save_pretrained(output_dir)` saves only the trained LoRA adapter weights, not the full model. This is one of the key advantages of PEFT/LoRA, as the resulting adapter is very small (typically just a few megabytes). This file can be loaded on top of the original base model to restore its fine-tuned behavior.\n",
        "\n",
        "2.  **Saving the Tokenizer**:\n",
        "    The command `tokenizer.save_pretrained(output_dir)` saves the tokenizer's configuration and vocabulary files. It is essential to save the tokenizer that was used during training to ensure that text is processed identically during inference. This prevents potential issues caused by tokenization mismatches."
      ],
      "metadata": {
        "id": "CR701C8R7xtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained LoRA adapter and the tokenizer\n",
        "trainer.model.save_pretrained(\"qlora-Gemma3-model\")\n",
        "tokenizer.save_pretrained(\"qlora-Gemma3-model\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.853613Z",
          "iopub.status.idle": "2025-09-25T12:40:52.854000Z",
          "shell.execute_reply": "2025-09-25T12:40:52.853842Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_gtl0307xtz",
        "outputId": "2d068eb4-32f6-4b93-fa59-913086c35bf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('qlora-Gemma3-model/tokenizer_config.json',\n",
              " 'qlora-Gemma3-model/special_tokens_map.json',\n",
              " 'qlora-Gemma3-model/chat_template.jinja',\n",
              " 'qlora-Gemma3-model/tokenizer.model',\n",
              " 'qlora-Gemma3-model/added_tokens.json',\n",
              " 'qlora-Gemma3-model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell uploads your fine-tuned model components to the Hugging Face Hub, making them publicly accessible and easy for you or others to reuse later."
      ],
      "metadata": {
        "id": "TOHFOd40e-08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not DEMO:\n",
        "  repo_name = f\"lmassaron/qlora4b-Gemma3-1B-finsent\"\n",
        "\n",
        "  # Push the model to the Hub\n",
        "  trainer.model.push_to_hub(repo_name)\n",
        "\n",
        "  # Push the tokenizer to the Hub\n",
        "  tokenizer.push_to_hub(repo_name)\n",
        "\n",
        "  print(f\"Model and tokenizer pushed to {repo_name}\")"
      ],
      "metadata": {
        "id": "Q81kmKmqFRqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code performs sentiment label prediction on the test set and evaluates the model's performance:\n",
        "\n",
        "1. **Predict Sentiment Labels**:  \n",
        "   The `predict(X_test, model, tokenizer)` function is called to predict the sentiment labels for the test dataset (`X_test`). This function generates predictions based on the fine-tuned model.\n",
        "\n",
        "2. **Evaluate Model Performance**:  \n",
        "   The `evaluate(y_true, y_pred)` function is used to assess the model's performance by comparing the true sentiment labels (`y_true`) with the predicted labels (`y_pred`). The evaluation will compute metrics like accuracy, precision, recall, and F1-score, which provide insights into how well the model is performing for each sentiment class.\n",
        "\n",
        "With a well-fine-tuned model, we expect to achieve an overall accuracy of over 0.8, and the performance for individual sentiment labels (positive, negative, and neutral) should be high, especially for positive and negative classes. While there might still be room for improvement in predicting neutral sentiment, the results should be impressive given the relatively small dataset and the use of fine-tuning."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.079228,
          "end_time": "2023-12-17T22:14:57.418749",
          "exception": false,
          "start_time": "2023-12-17T22:14:57.339521",
          "status": "completed"
        },
        "tags": [],
        "id": "Nv4p5Toq7xtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_pred = predict(X_test, model, tokenizer)\n",
        "evaluate(y_true, y_pred)"
      ],
      "metadata": {
        "papermill": {
          "duration": 379.935342,
          "end_time": "2023-12-17T22:21:17.436577",
          "exception": false,
          "start_time": "2023-12-17T22:14:57.501235",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.856423Z",
          "iopub.status.idle": "2025-09-25T12:40:52.856839Z",
          "shell.execute_reply": "2025-09-25T12:40:52.856650Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYSn2mYp7xtz",
        "outputId": "cf93740a-19fc-4a46-b8c2-9bc240f454f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting Sentiments: 100%|██████████| 900/900 [05:00<00:00,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 0.858\n",
            "Accuracy for label 0 (negative): 0.953\n",
            "Accuracy for label 1 (neutral): 0.777\n",
            "Accuracy for label 2 (positive): 0.843\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.93      0.95      0.94       300\n",
            "     neutral       0.83      0.78      0.80       300\n",
            "    negative       0.82      0.84      0.83       300\n",
            "\n",
            "    accuracy                           0.86       900\n",
            "   macro avg       0.86      0.86      0.86       900\n",
            "weighted avg       0.86      0.86      0.86       900\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[286   8   6]\n",
            " [ 16 233  51]\n",
            " [  6  41 253]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs particularly well with the **negative class** (high recall), showing strong performance in identifying negative sentiment. The **positive class** also shows good results, with a high recall and F1-score, but the **neutral class** still lags behind, with more modest recall and F1-score.\n",
        "\n",
        "The **confusion matrix** indicates that the model frequently predicts positive and negative sentiment correctly, with only a few neutral instances misclassified as positive or negative.\n",
        "\n",
        "Compared to the results previously obtained by fine-tuning **Gemma 7B-IT** (see: [Fine-tune Gemma 7B-IT for sentiment analysis](https://www.kaggle.com/code/lucamassaron/fine-tune-gemma-7b-it-for-sentiment-analysis)), we have to admit that, despite being a smaller model, the **Gemma 3 1B-IT** shows impressive potential for sentiment analysis. While the **Gemma 7B-IT quantized 4-bit** model certainly excels in overall accuracy and demonstrates better performance for the **neutral sentiment** class, it’s important to remember that **Gemma 3 1B-IT** is significantly more lightweight — seven times smaller — yet still delivers remarkable results.\n",
        "\n",
        "The **Gemma 3 1B-IT** model excels in classifying **negative sentiment**, achieving high accuracy and recall, and with additional fine-tuning, its performance on **neutral sentiment** could further improve. The **neutral sentiment** class, while not as strong as the 7B model, still shows promising potential and can be enhanced with targeted adjustments.\n",
        "\n",
        "The **Gemma 3 1B-IT** model offers a great trade-off between performance and computational efficiency, making it an attractive option for developers and researchers with limited resources or those needing faster deployment. While **Gemma 7B-IT** may offer a more balanced performance overall, **Gemma 3 1B-IT** is a powerful and efficient alternative, showing strong potential with fine-tuning, especially in terms of **negative sentiment classification**."
      ],
      "metadata": {
        "id": "1Jx8d_OF7xtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code organizes the results of a model's performance on a test set into a structured table and saves it to a CSV file for easy analysis."
      ],
      "metadata": {
        "id": "VTSOREz5gFA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = pd.DataFrame({'text': X_test[\"text\"],\n",
        "                           'y_true':y_true,\n",
        "                           'y_pred': y_pred},\n",
        "                         )\n",
        "evaluation.to_csv(\"test_predictions.csv\", index=False)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.182657,
          "end_time": "2023-12-17T22:21:18.077143",
          "exception": false,
          "start_time": "2023-12-17T22:21:17.894486",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.859150Z",
          "iopub.status.idle": "2025-09-25T12:40:52.859522Z",
          "shell.execute_reply": "2025-09-25T12:40:52.859360Z"
        },
        "id": "15D9nvPl7xt0"
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course. Here is a brief explanation.\n",
        "\n",
        "This code block **combines the small, trained LoRA adapter with the original base model to create a single, full-sized, standalone fine-tuned model.**\n",
        "\n",
        "It performs three key actions:\n",
        "\n",
        "1.  **`merged_model = trainer.model.merge_and_unload()`**: This is the core step. It \"bakes in\" the fine-tuning by mathematically merging the LoRA adapter's weights into the base model's weights. It then unloads the now-redundant adapter from memory, leaving you with a single, unified model.\n",
        "\n",
        "2.  **`merged_model.save_pretrained(...)`**: This command saves the newly created, full-sized merged model to a directory. Unlike saving just the adapter (which is very small), this saves the entire multi-billion parameter model, making it larger but easier to deploy.\n",
        "\n",
        "3.  **`tokenizer.save_pretrained(...)`**: This saves the tokenizer into the same directory, bundling everything needed for inference into one convenient location. Now, anyone can load this single directory to get the complete, ready-to-use fine-tuned model and its correct tokenizer."
      ],
      "metadata": {
        "id": "BwATAtaPgPCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge LoRA and base model and save\n",
        "merged_model = trainer.model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"merged4q-Gemma3-model\")\n",
        "tokenizer.save_pretrained(\"merged4q-Gemma3-model\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-25T12:40:52.860336Z",
          "iopub.status.idle": "2025-09-25T12:40:52.860774Z",
          "shell.execute_reply": "2025-09-25T12:40:52.860542Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QyfDQlF7xt0",
        "outputId": "d8c42c9d-4d80-44ee-efc1-0ea0b0ebde54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('merged4q-Gemma3-model/tokenizer_config.json',\n",
              " 'merged4q-Gemma3-model/special_tokens_map.json',\n",
              " 'merged4q-Gemma3-model/chat_template.jinja',\n",
              " 'merged4q-Gemma3-model/tokenizer.model',\n",
              " 'merged4q-Gemma3-model/added_tokens.json',\n",
              " 'merged4q-Gemma3-model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "if not DEMO:\n",
        "  repo_name = f\"lmassaron/merged-LoRA-Gemma3-1B-finsent\"\n",
        "\n",
        "  # Push the model to the Hub\n",
        "  merged_model.push_to_hub(repo_name)\n",
        "\n",
        "  # Push the tokenizer to the Hub\n",
        "  tokenizer.push_to_hub(repo_name)\n",
        "\n",
        "  print(f\"Model and tokenizer pushed to {repo_name}\")"
      ],
      "metadata": {
        "id": "zOKqVveOAp1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code **calculates and prints the total disk size of the two different model types you saved**: the small LoRA adapter and the large, fully merged model.\n",
        "\n",
        "It works in two main parts:\n",
        "\n",
        "1.  **The `get_directory_size` function:**\n",
        "    This is a helper utility. It \"walks\" through every single file within a given directory (and all its subdirectories), gets the size of each one, and adds them all together to calculate a total size in bytes.\n",
        "\n",
        "2.  **The Main Loop:**\n",
        "    The main part of the code then iterates through your two model directories (`qlora-Gemma3-model` and `merged4q-Gemma3-model`). For each one, it:\n",
        "    *   Calls the function to get its total size in bytes.\n",
        "    *   Converts this byte value into gigabytes (GB).\n",
        "    *   Prints the final size in a formatted, easy-to-read way.\n",
        "\n",
        "The purpose of this is to clearly demonstrate the practical difference in file size between saving just the efficient **LoRA adapter** versus saving the entire, standalone **merged model**."
      ],
      "metadata": {
        "id": "JYXV12odgRNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_directory_size(directory_path):\n",
        "    \"\"\"Calculates the total size of a directory and its subdirectories in bytes.\"\"\"\n",
        "    total_size = 0\n",
        "    # os.walk generates the file names in a directory tree\n",
        "    for dirpath, dirnames, filenames in os.walk(directory_path):\n",
        "        for f in filenames:\n",
        "            # os.path.join creates a full file path\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            # os.path.getsize gets the size of the file\n",
        "            # os.path.islink checks to avoid errors with symbolic links\n",
        "            if not os.path.islink(fp):\n",
        "                total_size += os.path.getsize(fp)\n",
        "    return total_size\n",
        "\n",
        "bytes_to_gb = 1024 * 1024 * 1024 # or 1e9 for Gigabytes vs. Gibibytes\n",
        "\n",
        "for directory in [\"./qlora-Gemma3-model\", \"./merged4q-Gemma3-model\"]:\n",
        "    # Get size in bytes\n",
        "    size_in_bytes = get_directory_size(directory)\n",
        "\n",
        "    # Convert to GB\n",
        "    size_in_gb = size_in_bytes / bytes_to_gb\n",
        "\n",
        "    # Print the result, formatted to 2 decimal places\n",
        "    print(f\"Directory '{directory}': {size_in_gb:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMNRLPzqCK4T",
        "outputId": "ee867cae-09e1-46cf-bb9d-5d98a666268a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory './qlora-Gemma3-model': 0.23 GB\n",
            "Directory './merged4q-Gemma3-model': 1.50 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_last_json_block(text: str) -> str | None:\n",
        "    \"\"\"Extracts the last JSON block from a string.\"\"\"\n",
        "    # Find the last closing curly brace\n",
        "    last_close_brace_index = text.rfind('}')\n",
        "\n",
        "    # If no closing brace is found, we can't find a block\n",
        "    if last_close_brace_index == -1:\n",
        "        return None\n",
        "\n",
        "    # Starting from the position of the last closing brace,\n",
        "    # find the last opening curly brace that comes before it.\n",
        "    last_open_brace_index = text.rfind('{', 0, last_close_brace_index)\n",
        "\n",
        "    # If no opening brace is found before the last closing brace, it's not a valid block\n",
        "    if last_open_brace_index == -1:\n",
        "        return None\n",
        "\n",
        "    # Return the slice of the string between the two braces\n",
        "    return text[last_open_brace_index : last_close_brace_index + 1]"
      ],
      "metadata": {
        "id": "3DyuQgBqsv-t"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def predict_with_reasoning(X_test, model, tokenizer, device=\"auto\", max_new_tokens=300, temperature=0.0):\n",
        "    \"\"\"Predict the sentiment of news headlines and extract the reasoning\"\"\"\n",
        "\n",
        "    y_pred = []  # List to store predicted sentiment strings\n",
        "    all_reasonings = [] # List to store the reasoning strings\n",
        "\n",
        "    # Iterate through each headline in X_test\n",
        "    for i in tqdm(range(len(X_test)), desc=\"Predicting Sentiments\"):\n",
        "        headline = X_test.iloc[i][\"text\"].split(\"[\")[-1].split(\"]\")[0]  # Extract headline text\n",
        "\n",
        "        # Construct the prompt to request a JSON output\n",
        "        prompt = f\"\"\"\n",
        "        Analyze the sentiment of the following headline and provide your reasoning.\n",
        "        The sentiment can be 'positive', 'negative', or 'neutral'.\n",
        "        Return the output as a JSON object with two keys: 'sentiment' and 'reasoning'.\n",
        "        In the reason briefly explain the reaasoning for the sentiment.\n",
        "\n",
        "        Headline: \"{headline}\"\n",
        "\n",
        "        JSON output:\n",
        "        \"\"\"\n",
        "\n",
        "        # Tokenize and move input to the appropriate device\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate output from the model\n",
        "        params = {\"do_sample\": True, \"temperature\": temperature} if temperature > 0 else {\"do_sample\": False}\n",
        "\n",
        "        outputs = model.generate(**input_ids,\n",
        "                                 max_new_tokens=max_new_tokens,\n",
        "                                 **params)\n",
        "\n",
        "        # Decode the generated output into text\n",
        "        result = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        if \"{\" in result and \"}\" not in result:\n",
        "          result = result + \"\\\"}\"\n",
        "\n",
        "        # Extract and parse the JSON from the result\n",
        "        try:\n",
        "            # Extract the JSON part of the string\n",
        "            json_string = extract_last_json_block(result)\n",
        "\n",
        "            # Parse the JSON string into a Python dictionary\n",
        "            parsed_json = json.loads(json_string)\n",
        "\n",
        "            # Append the sentiment to y_pred, providing a default if the key is missing\n",
        "            y_pred.append(parsed_json.get(\"sentiment\", \"none\"))\n",
        "\n",
        "            # Append the reasoning, providing a default if the key is missing\n",
        "            all_reasonings.append(parsed_json.get(\"reasoning\", \"No reasoning provided.\"))\n",
        "\n",
        "        except:\n",
        "            error_reason = \"Failed to decode JSON from the output.\"\n",
        "            y_pred.append(\"none\")\n",
        "            all_reasonings.append(result)\n",
        "\n",
        "    return y_pred, all_reasonings"
      ],
      "metadata": {
        "id": "fboyMtFCj5aJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred, explanations = predict_with_reasoning(X_test[:10], merged_model.eval(), tokenizer, device=device, max_new_tokens=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyW4U5ybkUny",
        "outputId": "586d9c1e-cec6-457e-e1dd-083c12c87fe9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting Sentiments: 100%|██████████| 10/10 [00:47<00:00,  4.79s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for pred, reason in zip(y_pred, explanations):\n",
        "    print(f\"Prediction: {pred}, Reasoning: {reason}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRAnTDJfqjoM",
        "outputId": "bc5a026d-cc3e-4c80-86b8-562a02c6508a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: positive, Reasoning: The headline expresses optimism about the soy-oats' potential to enter the UK market, suggesting a positive outlook.\n",
            "\n",
            "Prediction: positive, Reasoning: The headline indicates a significant increase in sales, suggesting positive growth.\n",
            "\n",
            "Prediction: positive, Reasoning: The headline indicates a positive agreement between M-real and Sappi, suggesting a beneficial partnership and potential future growth.\n",
            "\n",
            "Prediction: positive, Reasoning: The headline expresses positive sentiment due to the mention of 'strong customer interest' and 'feedback', suggesting that the mentioned entities are likely to improve their products\n",
            "\n",
            "Prediction: positive, Reasoning: The headline expresses positive sentiment towards Activision and Rapala, highlighting a fresh and colorful approach.\n",
            "\n",
            "Prediction: positive, Reasoning: The headline indicates a positive turning point in steel prices, suggesting increased profitability and favorable market conditions.\n",
            "\n",
            "Prediction: positive, Reasoning: The headline indicates a significant increase in the company's net profit, suggesting a positive financial performance.\n",
            "\n",
            "Prediction: positive, Reasoning: The headline indicates that Ruukki, a Finnish footballer, has signed a contract with Veidekke Entreprenor AS, a Norwegian company.\n",
            "\n",
            "Prediction: positive, Reasoning: The headline indicates a significant increase in sales, suggesting a positive financial performance.\n",
            "\n",
            "Prediction: positive, Reasoning: The headline indicates a positive development - the distribution of Shimano's products. This suggests increased sales and market expansion.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}