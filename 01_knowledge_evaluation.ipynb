{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmassaron/fine-tuning-workshop/blob/main/01_knowledge_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the output of the **NVIDIA System Management Interface (nvidia-smi)**, a command-line utility used for monitoring and managing NVIDIA GPU devices. It provides a real-time snapshot of the GPU's status and the processes utilizing it.\n",
        "\n",
        "Here's a breakdown of what each section of the output means:\n",
        "\n",
        "### Header Information\n",
        "\n",
        "The top section provides details about the installed NVIDIA driver and the CUDA version it supports.\n",
        "\n",
        "| Header | Value | Description |\n",
        "|---|---|---|\n",
        "| `NVIDIA-SMI` | `550.163.01` | The version of the nvidia-smi utility itself. |\n",
        "| `Driver Version` | `550.163.01` | The version of the installed NVIDIA display driver. |\n",
        "| `CUDA Version` | `12.4` | The highest version of CUDA that is supported by the installed driver. |\n",
        "\n",
        "### GPU Details Table\n",
        "\n",
        "This table provides detailed information about the GPU installed in the system.\n",
        "\n",
        "| Metric | Value | Description |\n",
        "|---|---|---|\n",
        "| `GPU` | `0` | The index of the GPU in the system. Since it's 0, this is the first and only GPU. |\n",
        "| `Name` | `NVIDIA GeForce RTX 3090` | The model of the graphics card. |\n",
        "| `Persistence-M` | `Off` | Persistence mode. When \"On\", the NVIDIA driver remains loaded even when no applications are using the GPU. |\n",
        "| `Bus-Id` | `00000000:01:00.0` | The PCI bus address of the GPU, which helps in identifying the physical slot it's in. |\n",
        "| `Disp.A` | `On` | Display Active. This indicates whether a display is connected to and active on this GPU. |\n",
        "| `Volatile Uncorr. ECC` | `N/A` | Information about volatile uncorrectable ECC (Error Correction Code) memory errors. \"N/A\" means it's not applicable to this GPU. |\n",
        "| `Fan` | `30%` | The current speed of the GPU's cooling fan as a percentage of its maximum speed. |\n",
        "| `Temp` | `36C` | The current temperature of the GPU in degrees Celsius. |\n",
        "| `Perf` | `P5` | The current performance state of the GPU. This ranges from P0 (maximum performance) to P12 (minimum performance). |\n",
        "| `Pwr:Usage/Cap` | `33W / 350W` | The current power consumption of the GPU in watts compared to its maximum power capacity. |\n",
        "| `Memory-Usage` | `382MiB / 24576MiB` | The amount of dedicated GPU memory currently in use out of the total available memory. |\n",
        "| `GPU-Util` | `36%` | The percentage of time the GPU's processing cores were active over a specific period. |\n",
        "| `Compute M.` | `Default` | The compute mode of the GPU. \"Default\" allows multiple processes to use the GPU for compute tasks simultaneously. |\n",
        "\n",
        "### Processes Table\n",
        "\n",
        "This section lists the processes that are currently using the GPU's resources.\n",
        "\n",
        "| Column | Description |\n",
        "|---|---|\n",
        "| `GPU` | The index of the GPU being used by the process. |\n",
        "| `GI` & `CI` | Graphics and Compute Instance IDs. These are used for Multi-Instance GPU (MIG) functionality, which is not active here (\"N/A\"). |\n",
        "| `PID` | The Process ID of the application using the GPU. |\n",
        "| `Type` | The type of context the process is using: \"G\" for Graphics, \"C\" for Compute, and \"C+G\" for both. In this case, all listed processes are using the GPU for graphics. |\n",
        "| `Process name` | The name of the process. Here we see the X.Org display server, the GNOME desktop environment's shell, and the Visual Studio Code editor. |\n",
        "| `GPU Memory Usage` | The amount of the GPU's memory being used by that specific process. |"
      ],
      "metadata": {
        "id": "nx1LnM8qdXSu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhNrI5idd_vH",
        "outputId": "66594aca-2169-4c70-bd17-0904ec9c0ec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Sep 25 00:37:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0  On |                  N/A |\n",
            "| 30%   36C    P5             33W /  350W |     382MiB /  24576MiB |     36%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A      2018      G   /usr/lib/xorg/Xorg                            102MiB |\n",
            "|    0   N/A  N/A      2164      G   /usr/bin/gnome-shell                          134MiB |\n",
            "|    0   N/A  N/A      4735      G   /usr/share/code/code                          134MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check the GPU information\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%%capture` is a \"magic command\" in IPython, which is the interactive shell that powers Jupyter notebooks. Magic commands are special commands that are not part of the Python language itself but provide extra functionality within the IPython/Jupyter environment.\n",
        "\n",
        "Specifically, `%%capture` is a **cell magic**, which means it starts with `%%` and applies to the entire code cell in which it is placed. Its primary purpose is to **prevent the output of a code cell from being displayed and to capture that output into a variable**.\n",
        "\n",
        "### Basic Usage\n",
        "\n",
        "The most basic way to use `%%capture` is to simply put it at the top of a cell. This will run the code in the cell but will discard all the output.\n",
        "\n",
        "```python\n",
        "%%capture\n",
        "print(\"This output will be hidden.\")\n",
        "```\n",
        "\n",
        "### Storing Captured Output\n",
        "\n",
        "To make `%%capture` truly useful, you can provide a variable name after it. The captured output will be stored in an object in that variable.\n",
        "\n",
        "```python\n",
        "%%capture my_output\n",
        "import sys\n",
        "\n",
        "print(\"This is standard output.\")\n",
        "print(\"This is an error message.\", file=sys.stderr)\n",
        "```\n",
        "\n",
        "After running this cell, the variable `my_output` will contain a special `CapturedIO` object. You can then access the captured output through its attributes:\n",
        "\n",
        "*   `my_output.stdout`: A string containing everything that was sent to standard output.\n",
        "*   `my_output.stderr`: A string containing everything that was sent to standard error.\n",
        "\n",
        "You can then print these variables to see the captured content:\n",
        "\n",
        "```python\n",
        "print(\"--- Standard Output ---\")\n",
        "print(my_output.stdout)\n",
        "print(\"--- Standard Error ---\")\n",
        "print(my_output.stderr)\n",
        "```"
      ],
      "metadata": {
        "id": "V93pX8f8dq6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a brief description of each package:\n",
        "\n",
        "*   **`transformers`**: Developed by Hugging Face, this is a foundational library providing a vast collection of pre-trained models (like BERT, GPT, and Llama) for a wide range of tasks in natural language processing, computer vision, and audio. It simplifies downloading and using these state-of-the-art models with a consistent API.\n",
        "\n",
        "*   **`trl`**: Standing for Transformer Reinforcement Learning, `trl` is a library designed to fine-tune models from the `transformers` library using reinforcement learning techniques. It simplifies advanced training methods like Supervised Fine-Tuning (SFT) and Proximal Policy Optimization (PPO).\n",
        "\n",
        "*   **`accelerate`**: Also from Hugging Face, `accelerate` is a library that simplifies running PyTorch training scripts across different types of hardware (like multiple GPUs or TPUs) with minimal code changes. It handles the boilerplate code for distributed training and mixed-precision, making it easier to scale up your model training.\n",
        "\n",
        "*   **`bitsandbytes`**: This is a lightweight library that provides powerful quantization methods, allowing you to run and train large language models with significantly less memory. Its key features are 8-bit and 4-bit quantization, which dramatically reduce the GPU memory footprint of a model, making it possible to work with very large models on consumer-grade hardware."
      ],
      "metadata": {
        "id": "TFRCCwgveLmw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnjgsKN_esrw"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries for model training and evaluation\n",
        "%%capture\n",
        "!pip install -U transformers trl accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbszOixGedSt",
        "outputId": "1b1326fe-82e0-47fd-e5d0-94ae33116084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PyTorch version: 2.8.0+cu128\n",
            "Using TRL version: 0.22.2\n",
            "Using bitsandbytes version: 0.47.0\n"
          ]
        }
      ],
      "source": [
        "# Import and print the versions of the installed libraries\n",
        "import torch\n",
        "import trl\n",
        "import bitsandbytes\n",
        "\n",
        "print(f\"Using PyTorch version: {torch.__version__}\")\n",
        "print(f\"Using TRL version: {trl.__version__}\")\n",
        "print(f\"Using bitsandbytes version: {bitsandbytes.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfeOmO0cfNGh",
        "outputId": "0cc618d3-1d46-4b37-a424-511dc417eee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 09-25 00:37:35 [__init__.py:216] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "# Import various libraries needed for data handling, model loading, and training\n",
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from huggingface_hub import login\n",
        "from peft import LoraConfig\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import SFTConfig, SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class acts as a **centralized place to store and manage important settings** for a machine learning script, specifically for running Google's Gemma model.\n",
        "\n",
        "The reasons for using it are:\n",
        "\n",
        "*   **Easy to Change:** You can quickly change the model size (e.g., from `\"3-1b\"` to `\"7b\"`) or token lengths in one spot without hunting through the code.\n",
        "*   **Keeps Code Clean:** It groups all the key parameters together, making the main script more readable and organized.\n",
        "*   **Avoids \"Magic Numbers\":** It gives descriptive names (`max_prompt_length`) to numbers that would otherwise be scattered throughout the code, making their purpose clear."
      ],
      "metadata": {
        "id": "HeqSZp5Gflvx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrTseGUkffTN"
      },
      "outputs": [],
      "source": [
        "# Define configuration parameters for the model and data\n",
        "class Config:\n",
        "    \"\"\"Configuration parameters\"\"\"\n",
        "\n",
        "    SIZE = \"3-1b\"\n",
        "    MODEL_NAME = f\"google/gemma-{SIZE}-it\"\n",
        "\n",
        "    max_prompt_length = 352\n",
        "    max_completion_length = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`init()`\n",
        "\n",
        "This function **sets up the entire environment** for the script to run properly. It does three main things:\n",
        "1.  **Configures Settings:** It sets several environment variables to ensure stability, control which GPU is used (`\"0\"`), and manage memory allocation.\n",
        "2.  **Handles Login:** It automatically logs into Hugging Face by looking for your access token, first in the standard environment variables and then specifically in Google Colab's secure \"secrets\" manager.\n",
        "3.  **Cleans Up:** It frees up GPU memory, runs Python's garbage collector to free up system RAM, and silences warning messages for a cleaner output.\n",
        "\n",
        "`is_bfloat16_supported()`\n",
        "\n",
        "This function is a **hardware check**. It checks if the available NVIDIA GPU has the necessary architecture (Compute Capability 8.0 or higher, like an A100 or RTX 30/40 series) to support the `bfloat16` data type, which is an efficient format for training modern deep learning models. It returns `True` if it's supported and `False` if it's not.\n",
        "\n",
        "`info_device()`\n",
        "\n",
        "This function **determines the best hardware to use** for computations. It checks if a CUDA-enabled GPU is available and selects it. If not, it falls back to using the CPU. It then prints a message to inform you which device is being used (`\"cuda\"` or `\"cpu\"`) and returns this device object so the rest of the program knows where to send the model and data."
      ],
      "metadata": {
        "id": "LxbubbXVf5Z_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_hM-990eqrr"
      },
      "outputs": [],
      "source": [
        "# Initialization script to set up the environment and Hugging Face login\n",
        "def init():\n",
        "    \"\"\"Initialization script\"\"\"\n",
        "    os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
        "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
        "\n",
        "    # It is recommended to set the HF_TOKEN as an environment variable\n",
        "    token = os.environ.get(\"HF_TOKEN\")\n",
        "    if token:\n",
        "        login(token=token)\n",
        "    else:\n",
        "      try:\n",
        "        from google.colab import userdata\n",
        "        # Retrieve your Hugging Face token from Colab's secrets manager\n",
        "        # The name 'HF_TOKEN' should match the name you used in the secrets tab\n",
        "        hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "        # Check if the token was successfully retrieved\n",
        "        if hf_token:\n",
        "            # Log in to Hugging Face using the retrieved token\n",
        "            # The `add_to_git_credential=True` argument is optional and useful if you plan to push models to the Hub\n",
        "            login(token=hf_token, add_to_git_credential=True)\n",
        "            print(\"Hugging Face login successful using Google Colab secrets!\")\n",
        "        else:\n",
        "            print(\"Error: HF_TOKEN not found in Google Colab secrets or is empty.\")\n",
        "            print(\"Please ensure you have created a secret named 'HF_TOKEN' in the 'Secrets' tab (ðŸ”‘) on the left sidebar.\")\n",
        "      except:\n",
        "        print(\"HF_TOKEN not set. You might need to log in manually.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def is_bfloat16_supported():\n",
        "    \"\"\"Checks if the current device supports bfloat16.\"\"\"\n",
        "    return torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "\n",
        "\n",
        "def info_device():\n",
        "    \"\"\"Get device for PyTorch\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW6ft5_SfOV5",
        "outputId": "36ada8c2-ec90-47f7-b3b2-6411560311e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Using dtype: torch.bfloat16\n"
          ]
        }
      ],
      "source": [
        "# Initialize the environment, get parameters, device, and data type\n",
        "init()\n",
        "params = Config()\n",
        "device = info_device()\n",
        "dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n",
        "print(f\"Using dtype: {dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OtC1THhf0VS"
      },
      "outputs": [],
      "source": [
        "# Function to load dataset from Hugging Face Hub\n",
        "def get_data(repo_id, mapping_func=None, split=\"train\"):\n",
        "    \"\"\"Upload HF dataset\"\"\"\n",
        "    data = load_dataset(repo_id, cache_dir=\"/tmp\")[split]\n",
        "    if mapping_func:\n",
        "      data = data.map(mapping_func)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line `get_data(repo_id=\"lmassaron/Sherlock_QA_test\")` does the following:\n",
        "\n",
        "1.  **Downloads a Dataset:** It connects to the Hugging Face Hub and downloads the dataset named `Sherlock_QA_test`. This is an example of expert-based QA for testing purposes.\n",
        "2.  **Selects the Training Data:** By default, it selects the `\"train\"` split from that dataset.\n",
        "3.  **Returns the Data:** It returns this training data and stores it in the `data` variable without applying any transformations (since no `mapping_func` was provided)."
      ],
      "metadata": {
        "id": "w4H7v40PggXE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ty8thGnhB1M",
        "colab": {
          "referenced_widgets": [
            "2baa134ec13740da93d6f06b98a4fd86"
          ]
        },
        "outputId": "b2743de5-49cc-40c3-88ff-c6dd9c05eb1b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2baa134ec13740da93d6f06b98a4fd86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/25 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the Sherlock QA dataset\n",
        "data = get_data(repo_id=\"lmassaron/Sherlock_QA_test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hugging Face `Dataset` object structures data like a **highly efficient, memory-mapped table** (similar to a spreadsheet or a database table).\n",
        "\n",
        "In regard of our dataset:\n",
        "\n",
        "*   `features: ['Question', 'Answer', 'Difficulty']`: These are the **columns** of the table. Every single entry in the dataset will have these three fields.\n",
        "\n",
        "*   `num_rows: 25`: This is the total number of **rows** or records in the table.\n",
        "\n",
        "So, you can think of this dataset as a table with **25 rows** and **3 columns**: `Question`, `Answer`, and `Difficulty`.\n",
        "\n",
        "The key difference from a simple list of dictionaries is that the `datasets` library uses **Apache Arrow** on the backend. This allows it to handle massive datasets that don't fit in RAM by only loading the data from your disk as you need it, making it extremely fast and memory-efficient."
      ],
      "metadata": {
        "id": "2kZlEECJg5TZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19ihvnylhGEE",
        "outputId": "83976fdf-658e-4321-b36f-35ab4c43d4c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['Question', 'Answer', 'Difficulty'],\n",
              "    num_rows: 25\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the loaded dataset information\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`AutoTokenizer` is a smart class from the Hugging Face `transformers` library. Its job is to **download the correct tokenizer for any given pre-trained model.**\n",
        "\n",
        "Think of a tokenizer as a model-specific dictionary. It converts your text into a sequence of numbers (tokens) that the model can understand and converts the model's numerical output back into human-readable text. The \"Auto\" part is the key: you just give it a model name (like `\"google/gemma-3-1b-it\"`), and it automatically figures out the right tokenizer to use, saving you from having to find the specific class yourself."
      ],
      "metadata": {
        "id": "mYz14PdLhoL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`AutoModelForCausalLM` is another smart class from Hugging Face that **downloads the pre-trained weights and architecture for a model designed for causal language modeling.**\n",
        "\n",
        "\"Causal Language Modeling\" is the task of predicting the next token in a sequence of text. This is the fundamental task for generative models like GPT and Gemma. Just like `AutoTokenizer`, the \"Auto\" part automatically selects the correct model architecture based on the name you provide.\n",
        "\n",
        "#### Parameters:\n",
        "\n",
        "*   `params.MODEL_NAME`: The identifier of the model on the Hugging Face Hub (e.g., `\"google/gemma-3-1b-it\"`). This tells the function *what* model to load.\n",
        "*   `torch_dtype=dtype`: This sets the numerical precision (e.g., `float16` or `bfloat16`) for the model's weights. Using a lower precision than the default (`float32`) significantly **reduces the model's memory footprint and speeds up computation**, which is crucial for running large models.\n",
        "*   `device_map=device`: This tells the library **where to place the model's layers** (e.g., on the GPU, specified by `device=\"cuda\"`). For very large models, this can even be used to automatically distribute layers across multiple GPUs.\n",
        "*   `use_cache=True`: This enables a key-value cache during text generation. This is a significant optimization that **dramatically speeds up the process of generating long sequences of text** by reusing previous calculations instead of recomputing them for every new word."
      ],
      "metadata": {
        "id": "Qp-YMxuBhoEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You set the `tokenizer.pad_token` to handle **batch processing**.\n",
        "\n",
        "When you send a batch of multiple sentences to the model, they all must have the same length. To achieve this, shorter sentences are \"padded\" by adding a special token to them until they match the length of the longest sentence in the batch.\n",
        "\n",
        "However, some models, especially those designed purely for text generation (like Gemma), are not trained with a specific padding token. In this case, `tokenizer.pad_token` is `None`. By setting `tokenizer.pad_token = tokenizer.eos_token` (the \"end-of-sequence\" token), we are telling the tokenizer to **use the end-of-sequence token for padding**. This is a common and safe practice because the model is already trained to understand that the `eos_token` signifies the end of meaningful content and will effectively ignore it during processing."
      ],
      "metadata": {
        "id": "G1IyTB-bhn3q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag2jIkzshLAK"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer and model from Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(params.MODEL_NAME)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    params.MODEL_NAME,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=device,\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`inputs = tokenizer.apply_chat_template(...)`\n",
        "\n",
        "This section **formats the user's question into the specific conversational structure that the chat model was trained on.** It takes a simple question and wraps it with special tokens and role identifiers (like `user` and `model`) to create a formal prompt. It then tokenizes this formatted prompt, converts it into a PyTorch tensor, and moves it to the correct device (e.g., the GPU) to be ready for the model.\n",
        "\n",
        "`outputs = model.generate(...)`\n",
        "\n",
        "This is the **core text generation step.** It feeds the prepared `inputs` into the model and instructs it to predict the next sequence of tokens. The `attention_mask` is crucial here, as it tells the model which tokens are real and which are just padding, ensuring it doesn't get confused by the padding added for batching. The `**generation_kwargs` would contain other settings that control the generation process, like the maximum length or the sampling strategy.\n",
        "\n",
        "`generated_text = tokenizer.decode(...)`\n",
        "\n",
        "This final section **translates the model's numerical output back into human-readable text.** It first slices the output tensor to isolate only the newly generated tokens (stripping away the original input prompt). It then uses the tokenizer's `decode` method to convert these token IDs back into words, while also removing any special tokens (like the end-of-sequence token) for a clean, final answer."
      ],
      "metadata": {
        "id": "OupR2iiPioeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`generated_token_ids = outputs[0, inputs.input_ids.shape[-1] :]` is for **separating the model's newly generated answer from the original prompt you gave it.**\n",
        "\n",
        "Here's the breakdown:\n",
        "\n",
        "1.  **`outputs`**: The `model.generate` function returns the **entire sequence of tokens**, which includes your original input prompt *plus* the model's generated response appended at the end.\n",
        "\n",
        "2.  **`inputs.input_ids.shape[-1]`**: This gets the **length of your original input prompt** in tokens.\n",
        "\n",
        "3.  **`[0, inputs.input_ids.shape[-1] :]`**: This is a tensor slicing operation.\n",
        "    *   `0`: Selects the first (and likely only) sequence in the batch.\n",
        "    *   `inputs.input_ids.shape[-1] :`: This tells Python to \"start slicing from the index where the input prompt ends, and go all the way to the end of the sequence.\""
      ],
      "metadata": {
        "id": "UyQhjH0li-ES"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhzjOp95hXVG",
        "outputId": "902d01c6-d58c-488c-ad8d-ba937b31fdca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Samples:   0%|          | 0/25 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:   4%|â–         | 1/25 [00:21<08:46, 21.95s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:   8%|â–Š         | 2/25 [00:53<10:36, 27.66s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:  16%|â–ˆâ–Œ        | 4/25 [00:53<03:37, 10.36s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:  24%|â–ˆâ–ˆâ–       | 6/25 [01:11<03:03,  9.65s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:  32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [01:11<01:38,  5.79s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [01:11<00:55,  3.70s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [01:29<01:10,  5.43s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [01:29<00:40,  3.66s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [01:29<00:22,  2.52s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [01:29<00:12,  1.74s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [01:47<00:27,  4.66s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [02:04<00:36,  7.36s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [02:05<00:22,  5.71s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [02:05<00:06,  3.48s/it]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Evaluating Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [02:05<00:00,  5.01s/it]\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the dataset and store results\n",
        "temperature = 0\n",
        "results_list = []\n",
        "instructions = \"\\nBriefly, just give the straight answer to the question.\"\n",
        "\n",
        "# It's good practice to set the pad_token if it's not already set.\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "for row in tqdm(data, desc=\"Evaluating Samples\"):\n",
        "  question = row['Question']\n",
        "  answer = row['Answer']\n",
        "  difficulty = row['Difficulty']\n",
        "\n",
        "  # Tokenize the input and get both input_ids and attention_mask\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": question + instructions}],\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,  # Crucial for telling the model it's its turn to speak\n",
        "            return_tensors=\"pt\",\n",
        "            return_dict=True  # Ensure the output is a dictionary\n",
        "        ).to(device)\n",
        "\n",
        "  # Prepare arguments for the generate function\n",
        "  generation_kwargs = {\n",
        "      \"pad_token_id\": tokenizer.eos_token_id,\n",
        "      \"max_new_tokens\": params.max_completion_length,\n",
        "      \"do_sample\": temperature > 0\n",
        "  }\n",
        "\n",
        "  # Only add temperature to kwargs if sampling is enabled\n",
        "  if generation_kwargs[\"do_sample\"]:\n",
        "      generation_kwargs[\"temperature\"] = temperature\n",
        "\n",
        "  # Generate a completion from the model, passing the attention_mask\n",
        "  outputs = model.generate(\n",
        "      inputs.input_ids, # Pass input_ids explicitly\n",
        "      attention_mask=inputs.attention_mask, # Pass the attention mask\n",
        "      **generation_kwargs\n",
        "      )\n",
        "\n",
        "  generated_token_ids = outputs[0, inputs.input_ids.shape[-1] :]\n",
        "  generated_text = tokenizer.decode(\n",
        "      generated_token_ids,\n",
        "      skip_special_tokens=True,\n",
        "  ).strip()\n",
        "\n",
        "  results_list.append({\n",
        "      'question': question,\n",
        "      'expected_answer': answer,\n",
        "      'generated_answer': generated_text,\n",
        "      'difficulty': difficulty\n",
        "  })\n",
        "\n",
        "results_df = pd.DataFrame(results_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkXFPkxWRztL"
      },
      "outputs": [],
      "source": [
        "# Delete the model and tokenizer to free up GPU memory\n",
        "del [model, tokenizer]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MypZobdjqjYg"
      },
      "outputs": [],
      "source": [
        "# Evaluate correctness based on keyword matching\n",
        "def evaluate_keyword(row):\n",
        "    return row['expected_answer'].lower() in row['generated_answer'].lower()\n",
        "\n",
        "results_df['is_correct_keyword'] = results_df.apply(evaluate_keyword, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we ** evaluate if the model's answers are correct by comparing their *semantic meaning*, not just their exact words.**\n",
        "\n",
        "Here's the simple breakdown of the steps:\n",
        "\n",
        "1.  **Convert to Numbers:** It uses a `SentenceTransformer` model to convert both the expected \"correct\" answers and the model's generated answers into numerical vectors (called embeddings). In these vectors, sentences with similar meanings are mathematically close to each other.\n",
        "\n",
        "2.  **Calculate Similarity:** It then calculates the \"cosine similarity\" between the vector for the expected answer and the vector for the generated answer. This results in a score from -1 to 1, where 1 means the meanings are identical.\n",
        "\n",
        "3.  **Make a Decision:** Finally, it checks if this similarity score is above a certain threshold (in this case, `0.5`). If it is, the answer is marked as `True` (correct), even if the wording isn't exactly the same."
      ],
      "metadata": {
        "id": "6haJ39UlkHT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity is a metric used to measure how similar two things are, not by their size or magnitude, but by their **orientation** or **direction**.\n",
        "\n",
        "Imagine two arrows starting from the same point.\n",
        "\n",
        "*   If the arrows point in the **exact same direction**, their cosine similarity is **1** (maximum similarity).\n",
        "*   If the arrows are **perpendicular** (pointing at a 90-degree angle to each other), they are considered unrelated, and their similarity is **0**.\n",
        "*   If they point in **opposite directions**, their similarity is **-1** (maximum dissimilarity).\n",
        "\n",
        "In text analysis, sentences are converted into these \"arrows\" (vectors) in a high-dimensional space. Cosine similarity then tells us if two sentences \"point\" in the same semantic direction, meaning they have a similar topic or meaning, regardless of the exact words used.\n",
        "\n",
        "At its core, the calculation is based on the **dot product** of two vectors divided by the product of their **magnitudes**.\n",
        "\n",
        "Here is the formula:\n",
        "\n",
        "![Cosine Similarity Formula](https://wikimedia.org/api/rest_v1/media/math/render/svg/15d11df2d48da4787ee86a4b8c14551fbf0bc96a)\n",
        "\n",
        "Where:\n",
        "*   **A â‹… B** is the **dot product** of vectors A and B.\n",
        "*   **||A||** and **||B||** are the **magnitudes** (or lengths) of vectors A and B.\n",
        "\n",
        "---\n",
        "\n",
        "### Step-by-Step Calculation with a Simple Example\n",
        "\n",
        "Let's say we want to measure the similarity between two short sentences:\n",
        "*   **Sentence A:** \"the cat sat\"\n",
        "*   **Sentence B:** \"the dog sat\"\n",
        "\n",
        "**Step 1: Convert Sentences to Vectors**\n",
        "\n",
        "First, we create a vocabulary of all unique words: `{\"the\", \"cat\", \"sat\", \"dog\"}`. Then, we count the occurrences of each word in each sentence to create our vectors.\n",
        "\n",
        "*   **Vector A** (for \"the cat sat\"): `[1, 1, 1, 0]`  (1 \"the\", 1 \"cat\", 1 \"sat\", 0 \"dog\")\n",
        "*   **Vector B** (for \"the dog sat\"): `[1, 0, 1, 1]`  (1 \"the\", 0 \"cat\", 1 \"sat\", 1 \"dog\")\n",
        "\n",
        "Now we have our two vectors, `A = [1, 1, 1, 0]` and `B = [1, 0, 1, 1]`.\n",
        "\n",
        "**Step 2: Calculate the Dot Product (A â‹… B)**\n",
        "\n",
        "The dot product is the sum of the products of the corresponding elements in the vectors.\n",
        "\n",
        "`A â‹… B = (1 * 1) + (1 * 0) + (1 * 1) + (0 * 1)`\n",
        "`A â‹… B = 1 + 0 + 1 + 0`\n",
        "`A â‹… B = 2`\n",
        "\n",
        "**Step 3: Calculate the Magnitude of Each Vector (||A|| and ||B||)**\n",
        "\n",
        "The magnitude is the square root of the sum of the squares of all the elements in the vector.\n",
        "\n",
        "*   **Magnitude of A (||A||):**\n",
        "    `||A|| = âˆš(1Â² + 1Â² + 1Â² + 0Â²)`\n",
        "    `||A|| = âˆš(1 + 1 + 1 + 0)`\n",
        "    `||A|| = âˆš3 â‰ˆ 1.732`\n",
        "\n",
        "*   **Magnitude of B (||B||):**\n",
        "    `||B|| = âˆš(1Â² + 0Â² + 1Â² + 1Â²)`\n",
        "    `||B|| = âˆš(1 + 0 + 1 + 1)`\n",
        "    `||B|| = âˆš3 â‰ˆ 1.732`\n",
        "\n",
        "**Step 4: Divide the Dot Product by the Product of the Magnitudes**\n",
        "\n",
        "Now we just plug the results from the previous steps into the main formula.\n",
        "\n",
        "`Cosine Similarity = (A â‹… B) / (||A|| * ||B||)`\n",
        "`Cosine Similarity = 2 / (âˆš3 * âˆš3)`\n",
        "`Cosine Similarity = 2 / 3 â‰ˆ 0.667`\n",
        "\n",
        "The resulting similarity score of **0.667** indicates a strong positive similarity between the two sentences, which makes sense as they share two out of three words."
      ],
      "metadata": {
        "id": "5sZe4a2LkRTm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b62iLlS1cJb"
      },
      "outputs": [],
      "source": [
        "# Evaluate correctness based on semantic similarity using Sentence-BERT\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load the Sentence-BERT model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Encode the expected and generated answers into embeddings\n",
        "expected_embeddings = model.encode(results_df['expected_answer'].tolist(), convert_to_tensor=True)\n",
        "generated_embeddings = model.encode(results_df['generated_answer'].tolist(), convert_to_tensor=True)\n",
        "\n",
        "# Calculate cosine similarity between embeddings\n",
        "cosine_scores = util.cos_sim(expected_embeddings, generated_embeddings)\n",
        "cosine_scores = np.array(cosine_scores.cpu())\n",
        "\n",
        "# Store the semantic similarity scores\n",
        "results_df['semantic_similarity'] = [cosine_scores[i][i] for i in range(len(cosine_scores))]\n",
        "\n",
        "# Determine correctness based on a similarity threshold\n",
        "similarity_threshold = 0.5\n",
        "results_df['is_correct_semantic'] = results_df['semantic_similarity'] >= similarity_threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMCEuo_TR352"
      },
      "outputs": [],
      "source": [
        "# Delete the Sentence-BERT model to free up memory\n",
        "del [model]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`meta-llama/Llama-3.2-3B-Instruct`** is a small yet powerful language model from Meta AI, designed to be highly efficient and accessible. The \"3B\" refers to its 3.21 billion parameters, making it lightweight enough to run on consumer hardware and at the edge. The \"-Instruct\" suffix means it has been specifically fine-tuned to follow instructions and engage in dialogue, making it ideal for creating chatbots and assistants.\n",
        "\n",
        "### Key Strengths:\n",
        "\n",
        "*   **Strong Multilingual Capabilities:** The model is explicitly optimized for multilingual use cases and officially supports eight languages: English, German, French, Spanish, Italian, Portuguese, Hindi, and Thai. This is a significant advantage for building global applications.\n",
        "*   **Large Context Window:** It supports a 128,000-token context window, allowing it to understand and process very long documents or conversations without losing track of the details.\n",
        "* **Strong Instruction Following:** The `-Instruct` fine-tuning means it's specifically designed to follow detailed instructions. You can provide it with a complex rubric or set of evaluation criteria (e.g., \"Score the response based on helpfulness, clarity, and factual accuracy, then provide a short rationale\"), and it will do a good job of adhering to that format.\n",
        "*   **Commercial Use:** The model is available for commercial use under the Llama 3.2 Community License, making it an attractive option for businesses and developers looking to build AI-powered products.\n",
        "\n",
        "### Weaknesses and Considerations (Where you need to be cautious)\n",
        "\n",
        "1.  **Limited Nuance and Depth:** This is the primary trade-off. For highly complex, subtle, or creative tasks (e.g., judging the quality of a poem, evaluating a complex legal argument, or assessing a sophisticated piece of code), a 3B model lacks the deep world knowledge and nuanced understanding of a much larger model (like a 70B or a GPT-4 class model). Its judgments on such topics will be more superficial.\n",
        "\n",
        "2.  **Susceptibility to Bias:** Like all LLMs, it can be prone to common evaluation biases, such as:\n",
        "    *   **Positional Bias:** Tending to prefer the first or second answer it's shown, regardless of quality.\n",
        "    *   **Verbosity Bias:** Preferring longer, more detailed answers even if they aren't more correct.\n",
        "    *   **Affirmation Bias:** Agreeing with the user's framing or the premise of the question.\n",
        "    These biases might be more pronounced in a smaller model.\n",
        "\n",
        "3.  **Fact-Checking Limitations:** A 3B model has a smaller knowledge base and is more likely to hallucinate or fail to identify factual errors in the texts it's judging. You cannot rely on it as a sole arbiter of factual accuracy.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dl0L6boElhDm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4D_3Auh7osc",
        "colab": {
          "referenced_widgets": [
            "83bff5a4334443c8909604ae5c3d42e6"
          ]
        },
        "outputId": "09d171ba-549a-48a4-82e9-0ae11bb08cd1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83bff5a4334443c8909604ae5c3d42e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the evaluation model and tokenizer (AI Judge)\n",
        "evaluation_model = \"meta-llama/Llama-3.2-3B-Instruct\" # \"alpindale/Llama-3.2-3B-Instruct\"\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(evaluation_model)\n",
        "eval_model = AutoModelForCausalLM.from_pretrained(\n",
        "    evaluation_model,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=device,\n",
        "    use_cache=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UmScYdFOvz0",
        "outputId": "a9cb3d83-3e14-4279-8ee0-15e1bb221521"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/25 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:02<00:00, 11.64it/s]\n"
          ]
        }
      ],
      "source": [
        "# Function to generate the prompt for the AI judge\n",
        "def evaluation_prompt(question, expected_answer, generated_answer):\n",
        "  prompt = f\"\"\"You are an impartial evaluator.\n",
        "Your task is to determine if the \"Generated Answer\", even if too verbose, correctly answers the \"Question\".\n",
        "The \"Expected Answer\" is provided as a reference for the correct information.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Expected Answer:\n",
        "{expected_answer}\n",
        "\n",
        "Generated Answer:\n",
        "{generated_answer}\n",
        "\n",
        "Is the \"Generated Answer\" correct? Please answer with \"Yes\" or \"No\".\n",
        "\"\"\"\n",
        "  return prompt\n",
        "\n",
        "# Evaluate generated answers using the AI judge\n",
        "ai_judge = []\n",
        "\n",
        "for i in tqdm(range(len(results_df))):\n",
        "  question = results_df.iloc[i]['question']\n",
        "  expected_answer = results_df.iloc[i]['expected_answer']\n",
        "  generated_answer = results_df.iloc[i]['generated_answer']\n",
        "  prompt = evaluation_prompt(question, expected_answer, generated_answer)\n",
        "\n",
        "  inputs = eval_tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": prompt}],\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(device)\n",
        "\n",
        "  # Generate a response from the AI judge\n",
        "  outputs = eval_model.generate(\n",
        "      inputs,\n",
        "      pad_token_id=eval_tokenizer.eos_token_id,\n",
        "      max_new_tokens=100,\n",
        "      temperature=0.1,\n",
        "      do_sample=True,\n",
        "  )\n",
        "\n",
        "  generated_token_ids = outputs[0, inputs.shape[-1] :]\n",
        "  generated_text = eval_tokenizer.decode(\n",
        "      generated_token_ids,\n",
        "      skip_special_tokens=True,\n",
        "  ).strip()\n",
        "\n",
        "  # Determine correctness based on the AI judge's response\n",
        "  if \"yes\" in generated_text.lower():\n",
        "    ai_judge.append(True)\n",
        "  else:\n",
        "    ai_judge.append(False)\n",
        "\n",
        "results_df[\"is_correct_ai_eval\"] = ai_judge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aff7ce6",
        "outputId": "d676269a-4847-47b5-ca2f-80dd0f4c9553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Keyword Matching Accuracy: 0.24\n",
            "Overall Semantic Similarity Accuracy (threshold=0.5): 0.48\n",
            "Overall AI Judge Accuracy: 0.32\n"
          ]
        }
      ],
      "source": [
        "# Calculate overall correctness metrics for each evaluation method\n",
        "overall_keyword_accuracy = results_df['is_correct_keyword'].mean()\n",
        "overall_semantic_accuracy = results_df['is_correct_semantic'].mean()\n",
        "overall_ai_judge_accuracy = results_df['is_correct_ai_eval'].mean()\n",
        "\n",
        "print(f\"Overall Keyword Matching Accuracy: {overall_keyword_accuracy:.2f}\")\n",
        "print(f\"Overall Semantic Similarity Accuracy (threshold=0.5): {overall_semantic_accuracy:.2f}\")\n",
        "print(f\"Overall AI Judge Accuracy: {overall_ai_judge_accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "bb165b21",
        "outputId": "543b9bc6-e966-4752-f51a-2ee416db1d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Keyword Matching Accuracy by Difficulty:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>difficulty</th>\n",
              "      <th>is_correct_keyword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Easy</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hard</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Medium</td>\n",
              "      <td>0.111111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  difficulty  is_correct_keyword\n",
              "0       Easy            0.416667\n",
              "1       Hard            0.000000\n",
              "2     Medium            0.111111"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Semantic Similarity Accuracy by Difficulty (threshold=0.5):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>difficulty</th>\n",
              "      <th>is_correct_semantic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Easy</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hard</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Medium</td>\n",
              "      <td>0.222222</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  difficulty  is_correct_semantic\n",
              "0       Easy             0.750000\n",
              "1       Hard             0.250000\n",
              "2     Medium             0.222222"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "AI Judge Accuracy by Difficulty:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>difficulty</th>\n",
              "      <th>is_correct_ai_eval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Easy</td>\n",
              "      <td>0.583333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hard</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Medium</td>\n",
              "      <td>0.111111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  difficulty  is_correct_ai_eval\n",
              "0       Easy            0.583333\n",
              "1       Hard            0.000000\n",
              "2     Medium            0.111111"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Analyze correctness by difficulty for each evaluation method\n",
        "difficulty_analysis_keyword = results_df.groupby('difficulty')['is_correct_keyword'].mean().reset_index()\n",
        "difficulty_analysis_semantic = results_df.groupby('difficulty')['is_correct_semantic'].mean().reset_index()\n",
        "difficulty_analysis_ai_judge = results_df.groupby('difficulty')['is_correct_ai_eval'].mean().reset_index()\n",
        "\n",
        "print(\"\\nKeyword Matching Accuracy by Difficulty:\")\n",
        "display(difficulty_analysis_keyword)\n",
        "\n",
        "print(\"\\nSemantic Similarity Accuracy by Difficulty (threshold=0.5):\")\n",
        "display(difficulty_analysis_semantic)\n",
        "\n",
        "print(\"\\nAI Judge Accuracy by Difficulty:\")\n",
        "display(difficulty_analysis_ai_judge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d8af88b9",
        "outputId": "314c1dc7-e013-40e4-a191-5cf2a2a04422"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>expected_answer</th>\n",
              "      <th>generated_answer</th>\n",
              "      <th>difficulty</th>\n",
              "      <th>is_correct_keyword</th>\n",
              "      <th>semantic_similarity</th>\n",
              "      <th>is_correct_semantic</th>\n",
              "      <th>is_correct_ai_eval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Who created the character of Sherlock Holmes?</td>\n",
              "      <td>Sir Arthur Conan Doyle</td>\n",
              "      <td>Arthur Conan Doyle</td>\n",
              "      <td>Easy</td>\n",
              "      <td>False</td>\n",
              "      <td>0.962577</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the name of Sherlock Holmes's enemy?</td>\n",
              "      <td>Professor Moriarty</td>\n",
              "      <td>Professor Moriarty</td>\n",
              "      <td>Easy</td>\n",
              "      <td>True</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Where does Sherlock Holmes live?</td>\n",
              "      <td>221b Baker Street in London</td>\n",
              "      <td>221B Baker Street.</td>\n",
              "      <td>Easy</td>\n",
              "      <td>False</td>\n",
              "      <td>0.901767</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Who is Sherlock Holmes's best friend?</td>\n",
              "      <td>Dr. John Watson</td>\n",
              "      <td>Dr. John Watson</td>\n",
              "      <td>Easy</td>\n",
              "      <td>True</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the name of Sherlock's older brother?</td>\n",
              "      <td>Mycroft Holmes</td>\n",
              "      <td>William.</td>\n",
              "      <td>Easy</td>\n",
              "      <td>False</td>\n",
              "      <td>0.264208</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Who is the landlady of 221b Baker Street?</td>\n",
              "      <td>Mrs. Hudson</td>\n",
              "      <td>Mrs. Hudson</td>\n",
              "      <td>Easy</td>\n",
              "      <td>True</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What musical instrument does Sherlock Holmes l...</td>\n",
              "      <td>The violin</td>\n",
              "      <td>The piano.</td>\n",
              "      <td>Easy</td>\n",
              "      <td>False</td>\n",
              "      <td>0.587473</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>In which Sherlock Holmes short story do we mee...</td>\n",
              "      <td>A Scandal In Bohemia</td>\n",
              "      <td>The Adventure of the Dancing Men.</td>\n",
              "      <td>Medium</td>\n",
              "      <td>False</td>\n",
              "      <td>0.200190</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Which actor plays Sherlock Holmes in the TV se...</td>\n",
              "      <td>Benedict Cumberbatch</td>\n",
              "      <td>Benedict Cumberbatch</td>\n",
              "      <td>Easy</td>\n",
              "      <td>True</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Who did Dr. Watson marry?</td>\n",
              "      <td>Mary Morstan</td>\n",
              "      <td>Dr. John Watson.</td>\n",
              "      <td>Medium</td>\n",
              "      <td>False</td>\n",
              "      <td>0.315029</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What are the street boys called who run errand...</td>\n",
              "      <td>The Baker Street Irregulars</td>\n",
              "      <td>Footmen.</td>\n",
              "      <td>Medium</td>\n",
              "      <td>False</td>\n",
              "      <td>0.190314</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Who stars as Sherlock Holmes in the 2009 film ...</td>\n",
              "      <td>Robert Downey Jr.</td>\n",
              "      <td>Robert Downey Jr.</td>\n",
              "      <td>Easy</td>\n",
              "      <td>True</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Who stars as Watson in the 2009 film Sherlock ...</td>\n",
              "      <td>Jude Law</td>\n",
              "      <td>Daniel Craig</td>\n",
              "      <td>Easy</td>\n",
              "      <td>False</td>\n",
              "      <td>0.408639</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>What was the first Sherlock Holmes story titled?</td>\n",
              "      <td>A Study In Scarlet</td>\n",
              "      <td>The Hound of the Baskervilles.</td>\n",
              "      <td>Medium</td>\n",
              "      <td>False</td>\n",
              "      <td>0.277919</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Which 2020 film features the teenage sister of...</td>\n",
              "      <td>Enola Holmes</td>\n",
              "      <td>Sherlock Holmes: A Game of Shadows</td>\n",
              "      <td>Easy</td>\n",
              "      <td>False</td>\n",
              "      <td>0.403840</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Where did Sherlock and Watson first meet?</td>\n",
              "      <td>St. Bartholomew's hospital</td>\n",
              "      <td>At Baker Street.</td>\n",
              "      <td>Medium</td>\n",
              "      <td>False</td>\n",
              "      <td>0.305989</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>When Sherlock Holmes retired, what hobby did h...</td>\n",
              "      <td>Beekeeping</td>\n",
              "      <td>He started collecting stamps.</td>\n",
              "      <td>Medium</td>\n",
              "      <td>False</td>\n",
              "      <td>0.177777</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Where does Sherlock Holmes keep his tobacco?</td>\n",
              "      <td>In a Persian slipper</td>\n",
              "      <td>In his pipe.</td>\n",
              "      <td>Hard</td>\n",
              "      <td>False</td>\n",
              "      <td>0.303259</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>What is the client's name in the short story \"...</td>\n",
              "      <td>John Openshaw</td>\n",
              "      <td>The clientâ€™s name is â€œThe Five Orange Pips.â€</td>\n",
              "      <td>Hard</td>\n",
              "      <td>False</td>\n",
              "      <td>0.165419</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>What was the title of the short story publishe...</td>\n",
              "      <td>The Final Problem</td>\n",
              "      <td>The Hound of the Baskervilles</td>\n",
              "      <td>Medium</td>\n",
              "      <td>False</td>\n",
              "      <td>0.125750</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>What was the first book released after everyon...</td>\n",
              "      <td>The Hound Of The Baskervilles</td>\n",
              "      <td>â€œThe Hound of the Baskervillesâ€ by Arthur Cona...</td>\n",
              "      <td>Medium</td>\n",
              "      <td>True</td>\n",
              "      <td>0.794517</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>What object is the Blue Carbuncle?</td>\n",
              "      <td>A priceless gemstone</td>\n",
              "      <td>A gemstone.</td>\n",
              "      <td>Medium</td>\n",
              "      <td>False</td>\n",
              "      <td>0.836267</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>What is Sherlock Holmes's most famous line?</td>\n",
              "      <td>\"Elementary, my dear Watson.\"</td>\n",
              "      <td>â€œElementary, my dear Watson.â€</td>\n",
              "      <td>Easy</td>\n",
              "      <td>False</td>\n",
              "      <td>0.993280</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>On what British TV channel is the series Sherl...</td>\n",
              "      <td>BBC One</td>\n",
              "      <td>ITV.</td>\n",
              "      <td>Hard</td>\n",
              "      <td>False</td>\n",
              "      <td>0.626558</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>In The Hound Of The Baskervilles, what village...</td>\n",
              "      <td>Grimpen</td>\n",
              "      <td>Dartmoor.</td>\n",
              "      <td>Hard</td>\n",
              "      <td>False</td>\n",
              "      <td>0.168748</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             question  \\\n",
              "0       Who created the character of Sherlock Holmes?   \n",
              "1        What is the name of Sherlock Holmes's enemy?   \n",
              "2                    Where does Sherlock Holmes live?   \n",
              "3               Who is Sherlock Holmes's best friend?   \n",
              "4       What is the name of Sherlock's older brother?   \n",
              "5           Who is the landlady of 221b Baker Street?   \n",
              "6   What musical instrument does Sherlock Holmes l...   \n",
              "7   In which Sherlock Holmes short story do we mee...   \n",
              "8   Which actor plays Sherlock Holmes in the TV se...   \n",
              "9                           Who did Dr. Watson marry?   \n",
              "10  What are the street boys called who run errand...   \n",
              "11  Who stars as Sherlock Holmes in the 2009 film ...   \n",
              "12  Who stars as Watson in the 2009 film Sherlock ...   \n",
              "13   What was the first Sherlock Holmes story titled?   \n",
              "14  Which 2020 film features the teenage sister of...   \n",
              "15          Where did Sherlock and Watson first meet?   \n",
              "16  When Sherlock Holmes retired, what hobby did h...   \n",
              "17       Where does Sherlock Holmes keep his tobacco?   \n",
              "18  What is the client's name in the short story \"...   \n",
              "19  What was the title of the short story publishe...   \n",
              "20  What was the first book released after everyon...   \n",
              "21                 What object is the Blue Carbuncle?   \n",
              "22        What is Sherlock Holmes's most famous line?   \n",
              "23  On what British TV channel is the series Sherl...   \n",
              "24  In The Hound Of The Baskervilles, what village...   \n",
              "\n",
              "                  expected_answer  \\\n",
              "0          Sir Arthur Conan Doyle   \n",
              "1              Professor Moriarty   \n",
              "2     221b Baker Street in London   \n",
              "3                 Dr. John Watson   \n",
              "4                  Mycroft Holmes   \n",
              "5                     Mrs. Hudson   \n",
              "6                      The violin   \n",
              "7            A Scandal In Bohemia   \n",
              "8            Benedict Cumberbatch   \n",
              "9                    Mary Morstan   \n",
              "10    The Baker Street Irregulars   \n",
              "11              Robert Downey Jr.   \n",
              "12                       Jude Law   \n",
              "13             A Study In Scarlet   \n",
              "14                   Enola Holmes   \n",
              "15     St. Bartholomew's hospital   \n",
              "16                     Beekeeping   \n",
              "17           In a Persian slipper   \n",
              "18                  John Openshaw   \n",
              "19              The Final Problem   \n",
              "20  The Hound Of The Baskervilles   \n",
              "21           A priceless gemstone   \n",
              "22  \"Elementary, my dear Watson.\"   \n",
              "23                        BBC One   \n",
              "24                        Grimpen   \n",
              "\n",
              "                                     generated_answer difficulty  \\\n",
              "0                                  Arthur Conan Doyle       Easy   \n",
              "1                                  Professor Moriarty       Easy   \n",
              "2                                  221B Baker Street.       Easy   \n",
              "3                                     Dr. John Watson       Easy   \n",
              "4                                            William.       Easy   \n",
              "5                                         Mrs. Hudson       Easy   \n",
              "6                                          The piano.       Easy   \n",
              "7                   The Adventure of the Dancing Men.     Medium   \n",
              "8                                Benedict Cumberbatch       Easy   \n",
              "9                                    Dr. John Watson.     Medium   \n",
              "10                                           Footmen.     Medium   \n",
              "11                                  Robert Downey Jr.       Easy   \n",
              "12                                       Daniel Craig       Easy   \n",
              "13                     The Hound of the Baskervilles.     Medium   \n",
              "14                 Sherlock Holmes: A Game of Shadows       Easy   \n",
              "15                                   At Baker Street.     Medium   \n",
              "16                      He started collecting stamps.     Medium   \n",
              "17                                       In his pipe.       Hard   \n",
              "18       The clientâ€™s name is â€œThe Five Orange Pips.â€       Hard   \n",
              "19                      The Hound of the Baskervilles     Medium   \n",
              "20  â€œThe Hound of the Baskervillesâ€ by Arthur Cona...     Medium   \n",
              "21                                        A gemstone.     Medium   \n",
              "22                      â€œElementary, my dear Watson.â€       Easy   \n",
              "23                                               ITV.       Hard   \n",
              "24                                          Dartmoor.       Hard   \n",
              "\n",
              "    is_correct_keyword  semantic_similarity  is_correct_semantic  \\\n",
              "0                False             0.962577                 True   \n",
              "1                 True             1.000000                 True   \n",
              "2                False             0.901767                 True   \n",
              "3                 True             1.000000                 True   \n",
              "4                False             0.264208                False   \n",
              "5                 True             1.000000                 True   \n",
              "6                False             0.587473                 True   \n",
              "7                False             0.200190                False   \n",
              "8                 True             1.000000                 True   \n",
              "9                False             0.315029                False   \n",
              "10               False             0.190314                False   \n",
              "11                True             1.000000                 True   \n",
              "12               False             0.408639                False   \n",
              "13               False             0.277919                False   \n",
              "14               False             0.403840                False   \n",
              "15               False             0.305989                False   \n",
              "16               False             0.177777                False   \n",
              "17               False             0.303259                False   \n",
              "18               False             0.165419                False   \n",
              "19               False             0.125750                False   \n",
              "20                True             0.794517                 True   \n",
              "21               False             0.836267                 True   \n",
              "22               False             0.993280                 True   \n",
              "23               False             0.626558                 True   \n",
              "24               False             0.168748                False   \n",
              "\n",
              "    is_correct_ai_eval  \n",
              "0                 True  \n",
              "1                 True  \n",
              "2                False  \n",
              "3                 True  \n",
              "4                False  \n",
              "5                 True  \n",
              "6                False  \n",
              "7                False  \n",
              "8                 True  \n",
              "9                False  \n",
              "10               False  \n",
              "11                True  \n",
              "12               False  \n",
              "13               False  \n",
              "14               False  \n",
              "15               False  \n",
              "16               False  \n",
              "17               False  \n",
              "18               False  \n",
              "19               False  \n",
              "20                True  \n",
              "21               False  \n",
              "22                True  \n",
              "23               False  \n",
              "24               False  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display the detailed results DataFrame\n",
        "display(results_df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv (3.12.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}